{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf-gpu': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7f98c0274b0118aac6cef4e869dba8152400cb7c799f77889ec50952b45afc8d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "I'll be trying to use a VGG16 (and some others) using Transfer learning to try and recognise the faces of my 109 mangaset!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 InputLayer False\n1 Conv2D False\n2 Conv2D False\n3 MaxPooling2D False\n4 Conv2D False\n5 Conv2D False\n6 MaxPooling2D False\n7 Conv2D False\n8 Conv2D False\n9 Conv2D False\n10 MaxPooling2D False\n11 Conv2D False\n12 Conv2D False\n13 Conv2D False\n14 MaxPooling2D False\n15 Conv2D False\n16 Conv2D False\n17 Conv2D False\n18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "###### Voor Tensorflow-GPU ########\n",
    "import tensorflow as tf \n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from keras.applications import VGG16\n",
    "rows = 100\n",
    "cols = 100\n",
    "\n",
    "model = VGG16(weights = 'imagenet', include_top = False, input_shape = (rows, cols, 3))\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(str(i) + \" \" + layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addlayer(bottom_model, num_classes):\n",
    "    \"\"\"creates the head of the model that will bw placed on top of the bottom layers\"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Dense(1024,activation='relu')(top_model)\n",
    "    top_model = Dense(1024,activation='relu')(top_model)\n",
    "    top_model = Dense(512,activation='relu')(top_model)\n",
    "    top_model = Dense(num_classes,activation='softmax')(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_4:0' shape=(None, 100, 100, 3) dtype=float32>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x1f7356dfa08>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7356dfc88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f732fa7208>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f7356e1408>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f7356ec108>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f730cb42c8>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f730977d88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f72d321948>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f72e0467c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f732e3dc08>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f732e28948>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f732e301c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f732e07448>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f732ddb788>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f732d50f88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f732d36148>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f732d4bb48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x1f731d2f408>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x1f732d41fc8>]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "222\nModel: \"model_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 100, 100, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 100, 100, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 100, 100, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 50, 50, 64)        0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 50, 50, 128)       73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 50, 50, 128)       147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 25, 25, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 25, 25, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 25, 25, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 25, 25, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 12, 12, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 12, 12, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 12, 12, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 12, 12, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 6, 6, 512)         0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 6, 6, 512)         2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 6, 6, 512)         2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 6, 6, 512)         2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n_________________________________________________________________\nglobal_average_pooling2d_4 ( (None, 512)               0         \n_________________________________________________________________\ndense_16 (Dense)             (None, 1024)              525312    \n_________________________________________________________________\ndense_17 (Dense)             (None, 1024)              1049600   \n_________________________________________________________________\ndense_18 (Dense)             (None, 512)               524800    \n_________________________________________________________________\ndense_19 (Dense)             (None, 222)               113886    \n=================================================================\nTotal params: 16,928,286\nTrainable params: 2,213,598\nNon-trainable params: 14,714,688\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, GlobalAveragePooling2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "directory = r'D:\\RESEARCH PROJECT\\Data\\manga109_faces_by_name\\train'\n",
    "files = os.listdir(directory)\n",
    "\n",
    "num_classes = len(files)\n",
    "\n",
    "print(num_classes)\n",
    "\n",
    "FC_Head = addlayer(model, num_classes)\n",
    "modelnew = Model(inputs=model.input, outputs=FC_Head)\n",
    "print(modelnew.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 9642 images belonging to 222 classes.\n",
      "Found 6639 images belonging to 222 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = (r'D:\\RESEARCH PROJECT\\Data\\manga109_faces_by_name/train')\n",
    "validation_data_dir = (r'D:\\RESEARCH PROJECT\\Data\\manga109_faces_by_name/val')\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=20, \n",
    "                                   width_shift_range=0.2, \n",
    "                                   height_shift_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_batchsize = 15\n",
    "val_batchsize = 10\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(rows, cols),\n",
    "                                                    batch_size=train_batchsize,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                    target_size=(rows, cols),\n",
    "                                                    batch_size=val_batchsize,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.3336 - accuracy: 0.0667\n",
      "Epoch 00001: val_loss improved from inf to 4.51094, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 4.3336 - accuracy: 0.0667 - val_loss: 4.5109 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6726 - accuracy: 0.1667\n",
      "Epoch 00002: val_loss improved from 4.51094 to 2.62957, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 493ms/step - loss: 3.6726 - accuracy: 0.1667 - val_loss: 2.6296 - val_accuracy: 0.4000\n",
      "Epoch 3/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6927 - accuracy: 0.4333\n",
      "Epoch 00003: val_loss improved from 2.62957 to 2.28869, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 2.6927 - accuracy: 0.4333 - val_loss: 2.2887 - val_accuracy: 0.4000\n",
      "Epoch 4/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7831 - accuracy: 0.2333\n",
      "Epoch 00004: val_loss did not improve from 2.28869\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.7831 - accuracy: 0.2333 - val_loss: 2.3223 - val_accuracy: 0.7000\n",
      "Epoch 5/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5408 - accuracy: 0.2333\n",
      "Epoch 00005: val_loss did not improve from 2.28869\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 3.5408 - accuracy: 0.2333 - val_loss: 3.2733 - val_accuracy: 0.2000\n",
      "Epoch 6/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3651 - accuracy: 0.2667\n",
      "Epoch 00006: val_loss did not improve from 2.28869\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3.3651 - accuracy: 0.2667 - val_loss: 2.9069 - val_accuracy: 0.4000\n",
      "Epoch 7/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6488 - accuracy: 0.2667\n",
      "Epoch 00007: val_loss did not improve from 2.28869\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3.6488 - accuracy: 0.2667 - val_loss: 2.3372 - val_accuracy: 0.5000\n",
      "Epoch 8/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4667 - accuracy: 0.2333\n",
      "Epoch 00008: val_loss improved from 2.28869 to 2.24954, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 3.4667 - accuracy: 0.2333 - val_loss: 2.2495 - val_accuracy: 0.4000\n",
      "Epoch 9/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2982 - accuracy: 0.1667\n",
      "Epoch 00009: val_loss did not improve from 2.24954\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.2982 - accuracy: 0.1667 - val_loss: 4.8603 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5385 - accuracy: 0.2667\n",
      "Epoch 00010: val_loss did not improve from 2.24954\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.5385 - accuracy: 0.2667 - val_loss: 4.2390 - val_accuracy: 0.3000\n",
      "Epoch 11/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.1050 - accuracy: 0.2000\n",
      "Epoch 00011: val_loss did not improve from 2.24954\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 3.8818 - accuracy: 0.2000 - val_loss: 2.4974 - val_accuracy: 0.2000\n",
      "Epoch 12/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5603 - accuracy: 0.2667\n",
      "Epoch 00012: val_loss did not improve from 2.24954\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.5603 - accuracy: 0.2667 - val_loss: 2.5982 - val_accuracy: 0.1000\n",
      "Epoch 13/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0904 - accuracy: 0.3000\n",
      "Epoch 00013: val_loss improved from 2.24954 to 1.65172, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 335ms/step - loss: 3.0904 - accuracy: 0.3000 - val_loss: 1.6517 - val_accuracy: 0.5000\n",
      "Epoch 14/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6263 - accuracy: 0.2000\n",
      "Epoch 00014: val_loss did not improve from 1.65172\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.6263 - accuracy: 0.2000 - val_loss: 2.0313 - val_accuracy: 0.6000\n",
      "Epoch 15/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0314 - accuracy: 0.4333\n",
      "Epoch 00015: val_loss did not improve from 1.65172\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 3.0314 - accuracy: 0.4333 - val_loss: 3.3272 - val_accuracy: 0.1000\n",
      "Epoch 16/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9645 - accuracy: 0.2333\n",
      "Epoch 00016: val_loss did not improve from 1.65172\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.9645 - accuracy: 0.2333 - val_loss: 2.8893 - val_accuracy: 0.2000\n",
      "Epoch 17/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9989 - accuracy: 0.4000\n",
      "Epoch 00017: val_loss did not improve from 1.65172\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.9989 - accuracy: 0.4000 - val_loss: 2.5951 - val_accuracy: 0.4000\n",
      "Epoch 18/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1337 - accuracy: 0.2667\n",
      "Epoch 00018: val_loss did not improve from 1.65172\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.1337 - accuracy: 0.2667 - val_loss: 4.4501 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9079 - accuracy: 0.2333\n",
      "Epoch 00019: val_loss did not improve from 1.65172\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 2.9079 - accuracy: 0.2333 - val_loss: 2.8738 - val_accuracy: 0.2000\n",
      "Epoch 20/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1794 - accuracy: 0.2667\n",
      "Epoch 00020: val_loss did not improve from 1.65172\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.1794 - accuracy: 0.2667 - val_loss: 1.9368 - val_accuracy: 0.6000\n",
      "Epoch 21/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6957 - accuracy: 0.3333\n",
      "Epoch 00021: val_loss improved from 1.65172 to 1.38110, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 3.6957 - accuracy: 0.3333 - val_loss: 1.3811 - val_accuracy: 0.7000\n",
      "Epoch 22/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7399 - accuracy: 0.1667\n",
      "Epoch 00022: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.7399 - accuracy: 0.1667 - val_loss: 1.6833 - val_accuracy: 0.6000\n",
      "Epoch 23/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9530 - accuracy: 0.2667\n",
      "Epoch 00023: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 2.9530 - accuracy: 0.2667 - val_loss: 1.5531 - val_accuracy: 0.8000\n",
      "Epoch 24/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5437 - accuracy: 0.2000\n",
      "Epoch 00024: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.5437 - accuracy: 0.2000 - val_loss: 2.3332 - val_accuracy: 0.4000\n",
      "Epoch 25/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3563 - accuracy: 0.1667\n",
      "Epoch 00025: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.3563 - accuracy: 0.1667 - val_loss: 2.8687 - val_accuracy: 0.1000\n",
      "Epoch 26/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7442 - accuracy: 0.1333\n",
      "Epoch 00026: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 3.7442 - accuracy: 0.1333 - val_loss: 2.9762 - val_accuracy: 0.3000\n",
      "Epoch 27/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6554 - accuracy: 0.2000    \n",
      "Epoch 00027: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.6554 - accuracy: 0.2000 - val_loss: 2.9542 - val_accuracy: 0.2000\n",
      "Epoch 28/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8377 - accuracy: 0.2667\n",
      "Epoch 00028: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.8377 - accuracy: 0.2667 - val_loss: 2.9323 - val_accuracy: 0.4000\n",
      "Epoch 29/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.8524 - accuracy: 0.2000\n",
      "Epoch 00029: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 3.4450 - accuracy: 0.2667 - val_loss: 1.7789 - val_accuracy: 0.4000\n",
      "Epoch 30/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7838 - accuracy: 0.2667\n",
      "Epoch 00030: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 2.7838 - accuracy: 0.2667 - val_loss: 1.8047 - val_accuracy: 0.5000\n",
      "Epoch 31/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0212 - accuracy: 0.1667\n",
      "Epoch 00031: val_loss did not improve from 1.38110\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.0212 - accuracy: 0.1667 - val_loss: 3.1160 - val_accuracy: 0.3000\n",
      "Epoch 32/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1130 - accuracy: 0.2333\n",
      "Epoch 00032: val_loss improved from 1.38110 to 1.31271, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 371ms/step - loss: 3.1130 - accuracy: 0.2333 - val_loss: 1.3127 - val_accuracy: 0.7000\n",
      "Epoch 33/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8600 - accuracy: 0.3333\n",
      "Epoch 00033: val_loss did not improve from 1.31271\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 2.8600 - accuracy: 0.3333 - val_loss: 5.8434 - val_accuracy: 0.1000\n",
      "Epoch 34/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4391 - accuracy: 0.2333\n",
      "Epoch 00034: val_loss did not improve from 1.31271\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.4391 - accuracy: 0.2333 - val_loss: 4.0706 - val_accuracy: 0.1000\n",
      "Epoch 35/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2467 - accuracy: 0.2667\n",
      "Epoch 00035: val_loss did not improve from 1.31271\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.2467 - accuracy: 0.2667 - val_loss: 3.7256 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1449 - accuracy: 0.2667\n",
      "Epoch 00036: val_loss did not improve from 1.31271\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 2.9660 - accuracy: 0.3333 - val_loss: 3.9521 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3125 - accuracy: 0.1667\n",
      "Epoch 00037: val_loss did not improve from 1.31271\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.3125 - accuracy: 0.1667 - val_loss: 4.3917 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5347 - accuracy: 0.1667\n",
      "Epoch 00038: val_loss improved from 1.31271 to 1.17418, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 479ms/step - loss: 3.5347 - accuracy: 0.1667 - val_loss: 1.1742 - val_accuracy: 0.7000\n",
      "Epoch 39/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9399 - accuracy: 0.1000\n",
      "Epoch 00039: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.9399 - accuracy: 0.1000 - val_loss: 5.2163 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6274 - accuracy: 0.4000\n",
      "Epoch 00040: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 2.6274 - accuracy: 0.4000 - val_loss: 3.8297 - val_accuracy: 0.2000\n",
      "Epoch 41/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9691 - accuracy: 0.2667\n",
      "Epoch 00041: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.9691 - accuracy: 0.2667 - val_loss: 3.9879 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9635 - accuracy: 0.3333\n",
      "Epoch 00042: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.9635 - accuracy: 0.3333 - val_loss: 4.7610 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2469 - accuracy: 0.2000\n",
      "Epoch 00043: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.2469 - accuracy: 0.2000 - val_loss: 3.5321 - val_accuracy: 0.1000\n",
      "Epoch 44/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1331 - accuracy: 0.3333\n",
      "Epoch 00044: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.1331 - accuracy: 0.3333 - val_loss: 6.3074 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9287 - accuracy: 0.3000\n",
      "Epoch 00045: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.9287 - accuracy: 0.3000 - val_loss: 4.4425 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0816 - accuracy: 0.2333\n",
      "Epoch 00046: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 4.0816 - accuracy: 0.2333 - val_loss: 6.0027 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3111 - accuracy: 0.2333\n",
      "Epoch 00047: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.3111 - accuracy: 0.2333 - val_loss: 3.4876 - val_accuracy: 0.3000\n",
      "Epoch 48/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5273 - accuracy: 0.2667\n",
      "Epoch 00048: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 3.5273 - accuracy: 0.2667 - val_loss: 3.9429 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7386 - accuracy: 0.2000\n",
      "Epoch 00049: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.7386 - accuracy: 0.2000 - val_loss: 4.1904 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9356 - accuracy: 0.3333\n",
      "Epoch 00050: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 2.9356 - accuracy: 0.3333 - val_loss: 4.6761 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8626 - accuracy: 0.1000\n",
      "Epoch 00051: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.8626 - accuracy: 0.1000 - val_loss: 4.0941 - val_accuracy: 0.2000\n",
      "Epoch 52/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2659 - accuracy: 0.2333\n",
      "Epoch 00052: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.2659 - accuracy: 0.2333 - val_loss: 3.9817 - val_accuracy: 0.2000\n",
      "Epoch 53/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4801 - accuracy: 0.2333\n",
      "Epoch 00053: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.4801 - accuracy: 0.2333 - val_loss: 1.7150 - val_accuracy: 0.5000\n",
      "Epoch 54/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8102 - accuracy: 0.2000\n",
      "Epoch 00054: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.8102 - accuracy: 0.2000 - val_loss: 2.3303 - val_accuracy: 0.4000\n",
      "Epoch 55/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7720 - accuracy: 0.2000\n",
      "Epoch 00055: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.7720 - accuracy: 0.2000 - val_loss: 4.0034 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0987 - accuracy: 0.2000\n",
      "Epoch 00056: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 4.0987 - accuracy: 0.2000 - val_loss: 2.7648 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0272 - accuracy: 0.3333\n",
      "Epoch 00057: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.0272 - accuracy: 0.3333 - val_loss: 4.2044 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0773 - accuracy: 0.2667\n",
      "Epoch 00058: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.0773 - accuracy: 0.2667 - val_loss: 4.0704 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6423 - accuracy: 0.3000\n",
      "Epoch 00059: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.6423 - accuracy: 0.3000 - val_loss: 4.6315 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6739 - accuracy: 0.2000\n",
      "Epoch 00060: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.6739 - accuracy: 0.2000 - val_loss: 5.3510 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7142 - accuracy: 0.2333\n",
      "Epoch 00061: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.7142 - accuracy: 0.2333 - val_loss: 6.6203 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0451 - accuracy: 0.3333\n",
      "Epoch 00062: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.0451 - accuracy: 0.3333 - val_loss: 3.6590 - val_accuracy: 0.1000\n",
      "Epoch 63/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9125 - accuracy: 0.2667\n",
      "Epoch 00063: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.9125 - accuracy: 0.2667 - val_loss: 4.2900 - val_accuracy: 0.1000\n",
      "Epoch 64/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3242 - accuracy: 0.3333\n",
      "Epoch 00064: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.3242 - accuracy: 0.3333 - val_loss: 4.5545 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4236 - accuracy: 0.1667\n",
      "Epoch 00065: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.4236 - accuracy: 0.1667 - val_loss: 4.8791 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6711 - accuracy: 0.1333\n",
      "Epoch 00066: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.6711 - accuracy: 0.1333 - val_loss: 5.8154 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7147 - accuracy: 0.1667\n",
      "Epoch 00067: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.7147 - accuracy: 0.1667 - val_loss: 5.2689 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8840 - accuracy: 0.3667\n",
      "Epoch 00068: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.8840 - accuracy: 0.3667 - val_loss: 1.1915 - val_accuracy: 0.8000\n",
      "Epoch 69/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7455 - accuracy: 0.1333\n",
      "Epoch 00069: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.7455 - accuracy: 0.1333 - val_loss: 3.3473 - val_accuracy: 0.2000\n",
      "Epoch 70/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7083 - accuracy: 0.2333\n",
      "Epoch 00070: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.7083 - accuracy: 0.2333 - val_loss: 3.6435 - val_accuracy: 0.2000\n",
      "Epoch 71/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6182 - accuracy: 0.1667\n",
      "Epoch 00071: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.6182 - accuracy: 0.1667 - val_loss: 3.0840 - val_accuracy: 0.2000\n",
      "Epoch 72/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7981 - accuracy: 0.1000\n",
      "Epoch 00072: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 3.7981 - accuracy: 0.1000 - val_loss: 4.9776 - val_accuracy: 0.1000\n",
      "Epoch 73/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2165 - accuracy: 0.2667\n",
      "Epoch 00073: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.2165 - accuracy: 0.2667 - val_loss: 4.3026 - val_accuracy: 0.1000\n",
      "Epoch 74/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5771 - accuracy: 0.2333\n",
      "Epoch 00074: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.5771 - accuracy: 0.2333 - val_loss: 3.6028 - val_accuracy: 0.1000\n",
      "Epoch 75/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6448 - accuracy: 0.2000\n",
      "Epoch 00075: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.6448 - accuracy: 0.2000 - val_loss: 3.6050 - val_accuracy: 0.2000\n",
      "Epoch 76/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3034 - accuracy: 0.2000\n",
      "Epoch 00076: val_loss did not improve from 1.17418\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.3034 - accuracy: 0.2000 - val_loss: 2.8901 - val_accuracy: 0.3000\n",
      "Epoch 77/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8478 - accuracy: 0.2000\n",
      "Epoch 00077: val_loss improved from 1.17418 to 0.60842, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 2.8478 - accuracy: 0.2000 - val_loss: 0.6084 - val_accuracy: 0.8000\n",
      "Epoch 78/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5201 - accuracy: 0.2333\n",
      "Epoch 00078: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.5201 - accuracy: 0.2333 - val_loss: 4.4061 - val_accuracy: 0.1000\n",
      "Epoch 79/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9451 - accuracy: 0.1667\n",
      "Epoch 00079: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.9451 - accuracy: 0.1667 - val_loss: 2.7596 - val_accuracy: 0.2000\n",
      "Epoch 80/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0812 - accuracy: 0.3667\n",
      "Epoch 00080: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.0812 - accuracy: 0.3667 - val_loss: 1.3449 - val_accuracy: 0.6000\n",
      "Epoch 81/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0891 - accuracy: 0.2333\n",
      "Epoch 00081: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0891 - accuracy: 0.2333 - val_loss: 3.1213 - val_accuracy: 0.4000\n",
      "Epoch 82/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4479 - accuracy: 0.3667\n",
      "Epoch 00082: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 3.4479 - accuracy: 0.3667 - val_loss: 5.1050 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0447 - accuracy: 0.2333\n",
      "Epoch 00083: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.0447 - accuracy: 0.2333 - val_loss: 5.6602 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2901 - accuracy: 0.1000    \n",
      "Epoch 00084: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 4.2901 - accuracy: 0.1000 - val_loss: 1.5857 - val_accuracy: 0.7000\n",
      "Epoch 85/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6756 - accuracy: 0.2333\n",
      "Epoch 00085: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.6756 - accuracy: 0.2333 - val_loss: 3.1464 - val_accuracy: 0.2000\n",
      "Epoch 86/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8906 - accuracy: 0.1333\n",
      "Epoch 00086: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.8906 - accuracy: 0.1333 - val_loss: 3.4119 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9874 - accuracy: 0.3000\n",
      "Epoch 00087: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 2.9874 - accuracy: 0.3000 - val_loss: 3.7949 - val_accuracy: 0.1000\n",
      "Epoch 88/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0135 - accuracy: 0.3667\n",
      "Epoch 00088: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.0135 - accuracy: 0.3667 - val_loss: 4.2493 - val_accuracy: 0.1000\n",
      "Epoch 89/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2602 - accuracy: 0.3333\n",
      "Epoch 00089: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.2602 - accuracy: 0.3333 - val_loss: 4.9577 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6721 - accuracy: 0.2667\n",
      "Epoch 00090: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.6721 - accuracy: 0.2667 - val_loss: 4.1376 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1855 - accuracy: 0.2333\n",
      "Epoch 00091: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.1855 - accuracy: 0.2333 - val_loss: 3.4722 - val_accuracy: 0.4000\n",
      "Epoch 92/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9081 - accuracy: 0.2000\n",
      "Epoch 00092: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.9081 - accuracy: 0.2000 - val_loss: 2.4507 - val_accuracy: 0.5000\n",
      "Epoch 93/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3413 - accuracy: 0.2667\n",
      "Epoch 00093: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.3413 - accuracy: 0.2667 - val_loss: 4.5639 - val_accuracy: 0.1000\n",
      "Epoch 94/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.6197 - accuracy: 0.0667\n",
      "Epoch 00094: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 3.9557 - accuracy: 0.1667 - val_loss: 6.7274 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3090 - accuracy: 0.2667\n",
      "Epoch 00095: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 3.3090 - accuracy: 0.2667 - val_loss: 5.3585 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7527 - accuracy: 0.1000\n",
      "Epoch 00096: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.7527 - accuracy: 0.1000 - val_loss: 4.7276 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2098 - accuracy: 0.2667\n",
      "Epoch 00097: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.2098 - accuracy: 0.2667 - val_loss: 6.4094 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.1473 - accuracy: 0.1333\n",
      "Epoch 00098: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 3.8910 - accuracy: 0.1000 - val_loss: 1.7816 - val_accuracy: 0.4000\n",
      "Epoch 99/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3579 - accuracy: 0.1333\n",
      "Epoch 00099: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 3.3579 - accuracy: 0.1333 - val_loss: 1.2881 - val_accuracy: 0.8000\n",
      "Epoch 100/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8421 - accuracy: 0.1333\n",
      "Epoch 00100: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.8421 - accuracy: 0.1333 - val_loss: 2.5809 - val_accuracy: 0.5000\n",
      "Epoch 101/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5149 - accuracy: 0.3000\n",
      "Epoch 00101: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.5149 - accuracy: 0.3000 - val_loss: 2.0328 - val_accuracy: 0.5000\n",
      "Epoch 102/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9856 - accuracy: 0.3333\n",
      "Epoch 00102: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.9856 - accuracy: 0.3333 - val_loss: 2.7217 - val_accuracy: 0.5000\n",
      "Epoch 103/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0814 - accuracy: 0.3000\n",
      "Epoch 00103: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.0814 - accuracy: 0.3000 - val_loss: 1.5364 - val_accuracy: 0.7000\n",
      "Epoch 104/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0428 - accuracy: 0.2333\n",
      "Epoch 00104: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 4.0428 - accuracy: 0.2333 - val_loss: 5.5408 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5934 - accuracy: 0.2000\n",
      "Epoch 00105: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.5934 - accuracy: 0.2000 - val_loss: 3.9804 - val_accuracy: 0.1000\n",
      "Epoch 106/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4255 - accuracy: 0.2000\n",
      "Epoch 00106: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.4255 - accuracy: 0.2000 - val_loss: 3.6419 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8294 - accuracy: 0.2667\n",
      "Epoch 00107: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.8294 - accuracy: 0.2667 - val_loss: 3.8060 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3539 - accuracy: 0.1667\n",
      "Epoch 00108: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.3539 - accuracy: 0.1667 - val_loss: 2.7108 - val_accuracy: 0.4000\n",
      "Epoch 109/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8740 - accuracy: 0.3000\n",
      "Epoch 00109: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 2.8740 - accuracy: 0.3000 - val_loss: 3.3691 - val_accuracy: 0.2000\n",
      "Epoch 110/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9317 - accuracy: 0.2333\n",
      "Epoch 00110: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.9317 - accuracy: 0.2333 - val_loss: 3.9747 - val_accuracy: 0.1000\n",
      "Epoch 111/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8577 - accuracy: 0.4000\n",
      "Epoch 00111: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.8577 - accuracy: 0.4000 - val_loss: 6.2457 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6261 - accuracy: 0.3000\n",
      "Epoch 00112: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.6261 - accuracy: 0.3000 - val_loss: 4.8567 - val_accuracy: 0.1000\n",
      "Epoch 113/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2613 - accuracy: 0.3000\n",
      "Epoch 00113: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.2613 - accuracy: 0.3000 - val_loss: 6.3227 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7359 - accuracy: 0.1667\n",
      "Epoch 00114: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.7359 - accuracy: 0.1667 - val_loss: 1.9442 - val_accuracy: 0.5000\n",
      "Epoch 115/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5638 - accuracy: 0.2667\n",
      "Epoch 00115: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.5638 - accuracy: 0.2667 - val_loss: 1.0395 - val_accuracy: 0.7000\n",
      "Epoch 116/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1264 - accuracy: 0.3333\n",
      "Epoch 00116: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.1264 - accuracy: 0.3333 - val_loss: 5.4321 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8011 - accuracy: 0.2333\n",
      "Epoch 00117: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.8011 - accuracy: 0.2333 - val_loss: 2.3464 - val_accuracy: 0.5000\n",
      "Epoch 118/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8268 - accuracy: 0.3667\n",
      "Epoch 00118: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 2.8268 - accuracy: 0.3667 - val_loss: 4.4840 - val_accuracy: 0.3000\n",
      "Epoch 119/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2304 - accuracy: 0.2333\n",
      "Epoch 00119: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.2304 - accuracy: 0.2333 - val_loss: 3.4119 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3008 - accuracy: 0.2333\n",
      "Epoch 00120: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.3008 - accuracy: 0.2333 - val_loss: 6.3065 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0413 - accuracy: 0.2667\n",
      "Epoch 00121: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.0413 - accuracy: 0.2667 - val_loss: 5.5302 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2055 - accuracy: 0.3000\n",
      "Epoch 00122: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.2055 - accuracy: 0.3000 - val_loss: 5.1329 - val_accuracy: 0.1000\n",
      "Epoch 123/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0434 - accuracy: 0.3000\n",
      "Epoch 00123: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 3.0434 - accuracy: 0.3000 - val_loss: 2.5665 - val_accuracy: 0.4000\n",
      "Epoch 124/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4927 - accuracy: 0.1000\n",
      "Epoch 00124: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.4927 - accuracy: 0.1000 - val_loss: 4.7737 - val_accuracy: 0.1000\n",
      "Epoch 125/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9071 - accuracy: 0.1000\n",
      "Epoch 00125: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.9071 - accuracy: 0.1000 - val_loss: 5.7082 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6834 - accuracy: 0.2333\n",
      "Epoch 00126: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.6834 - accuracy: 0.2333 - val_loss: 2.6216 - val_accuracy: 0.4000\n",
      "Epoch 127/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0236 - accuracy: 0.1667\n",
      "Epoch 00127: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.0236 - accuracy: 0.1667 - val_loss: 2.1245 - val_accuracy: 0.4000\n",
      "Epoch 128/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3248 - accuracy: 0.1667\n",
      "Epoch 00128: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.3248 - accuracy: 0.1667 - val_loss: 3.4776 - val_accuracy: 0.1000\n",
      "Epoch 129/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5090 - accuracy: 0.2333\n",
      "Epoch 00129: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.5090 - accuracy: 0.2333 - val_loss: 3.4485 - val_accuracy: 0.1000\n",
      "Epoch 130/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7301 - accuracy: 0.1333\n",
      "Epoch 00130: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.7301 - accuracy: 0.1333 - val_loss: 3.5192 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.3711 - accuracy: 0.1667\n",
      "Epoch 00131: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.3711 - accuracy: 0.1667 - val_loss: 3.1140 - val_accuracy: 0.3000\n",
      "Epoch 132/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2356 - accuracy: 0.1000\n",
      "Epoch 00132: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 4.2356 - accuracy: 0.1000 - val_loss: 2.9215 - val_accuracy: 0.6000\n",
      "Epoch 133/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7529 - accuracy: 0.1667\n",
      "Epoch 00133: val_loss did not improve from 0.60842\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.7529 - accuracy: 0.1667 - val_loss: 4.3507 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6580 - accuracy: 0.2333\n",
      "Epoch 00134: val_loss improved from 0.60842 to 0.57346, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 347ms/step - loss: 3.6580 - accuracy: 0.2333 - val_loss: 0.5735 - val_accuracy: 0.9000\n",
      "Epoch 135/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1508 - accuracy: 0.2000\n",
      "Epoch 00135: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.1508 - accuracy: 0.2000 - val_loss: 2.5371 - val_accuracy: 0.4000\n",
      "Epoch 136/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4754 - accuracy: 0.1333\n",
      "Epoch 00136: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.4754 - accuracy: 0.1333 - val_loss: 1.7527 - val_accuracy: 0.7000\n",
      "Epoch 137/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.8497 - accuracy: 0.3333\n",
      "Epoch 00137: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 3.2942 - accuracy: 0.2667 - val_loss: 0.7889 - val_accuracy: 0.9000\n",
      "Epoch 138/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5495 - accuracy: 0.2000\n",
      "Epoch 00138: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 3.5495 - accuracy: 0.2000 - val_loss: 2.3858 - val_accuracy: 0.6000\n",
      "Epoch 139/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.7526 - accuracy: 0.2000\n",
      "Epoch 00139: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 3.7243 - accuracy: 0.2333 - val_loss: 4.5161 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9116 - accuracy: 0.1667\n",
      "Epoch 00140: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.9116 - accuracy: 0.1667 - val_loss: 2.9879 - val_accuracy: 0.4000\n",
      "Epoch 141/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0542 - accuracy: 0.1333\n",
      "Epoch 00141: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 4.0542 - accuracy: 0.1333 - val_loss: 2.3218 - val_accuracy: 0.4000\n",
      "Epoch 142/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7418 - accuracy: 0.2667\n",
      "Epoch 00142: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.7418 - accuracy: 0.2667 - val_loss: 2.1587 - val_accuracy: 0.3000\n",
      "Epoch 143/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2612 - accuracy: 0.3000\n",
      "Epoch 00143: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 3.2612 - accuracy: 0.3000 - val_loss: 4.6578 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6914 - accuracy: 0.1333\n",
      "Epoch 00144: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.6914 - accuracy: 0.1333 - val_loss: 2.7900 - val_accuracy: 0.2000\n",
      "Epoch 145/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1588 - accuracy: 0.2333\n",
      "Epoch 00145: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 3.1588 - accuracy: 0.2333 - val_loss: 1.6363 - val_accuracy: 0.6000\n",
      "Epoch 146/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8109 - accuracy: 0.1000\n",
      "Epoch 00146: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 3.8109 - accuracy: 0.1000 - val_loss: 2.1427 - val_accuracy: 0.5000\n",
      "Epoch 147/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3234 - accuracy: 0.1667\n",
      "Epoch 00147: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.3234 - accuracy: 0.1667 - val_loss: 5.7767 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0938 - accuracy: 0.2000\n",
      "Epoch 00148: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 3.0938 - accuracy: 0.2000 - val_loss: 2.8081 - val_accuracy: 0.3000\n",
      "Epoch 149/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9615 - accuracy: 0.3000\n",
      "Epoch 00149: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 2.9615 - accuracy: 0.3000 - val_loss: 3.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1109 - accuracy: 0.2000\n",
      "Epoch 00150: val_loss did not improve from 0.57346\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 4.1109 - accuracy: 0.2000 - val_loss: 2.2018 - val_accuracy: 0.5000\n",
      "Epoch 151/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2586 - accuracy: 0.3333\n",
      "Epoch 00151: val_loss improved from 0.57346 to 0.33214, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 374ms/step - loss: 3.2586 - accuracy: 0.3333 - val_loss: 0.3321 - val_accuracy: 1.0000\n",
      "Epoch 152/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1644 - accuracy: 0.1000\n",
      "Epoch 00152: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 4.1644 - accuracy: 0.1000 - val_loss: 1.8507 - val_accuracy: 0.4000\n",
      "Epoch 153/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8042 - accuracy: 0.2000\n",
      "Epoch 00153: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.8042 - accuracy: 0.2000 - val_loss: 6.2520 - val_accuracy: 0.0000e+00\n",
      "Epoch 154/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3052 - accuracy: 0.3000\n",
      "Epoch 00154: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.3052 - accuracy: 0.3000 - val_loss: 3.8302 - val_accuracy: 0.1000\n",
      "Epoch 155/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1359 - accuracy: 0.2333\n",
      "Epoch 00155: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.1359 - accuracy: 0.2333 - val_loss: 5.4385 - val_accuracy: 0.0000e+00\n",
      "Epoch 156/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.4095 - accuracy: 0.2000\n",
      "Epoch 00156: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3.4335 - accuracy: 0.2000 - val_loss: 2.4229 - val_accuracy: 0.4000\n",
      "Epoch 157/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1102 - accuracy: 0.1000\n",
      "Epoch 00157: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 4.1102 - accuracy: 0.1000 - val_loss: 2.9047 - val_accuracy: 0.2000\n",
      "Epoch 158/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3704 - accuracy: 0.2333\n",
      "Epoch 00158: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.3704 - accuracy: 0.2333 - val_loss: 3.4647 - val_accuracy: 0.2000\n",
      "Epoch 159/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9162 - accuracy: 0.1667\n",
      "Epoch 00159: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.9162 - accuracy: 0.1667 - val_loss: 2.7362 - val_accuracy: 0.2000\n",
      "Epoch 160/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.7083 - accuracy: 0.3333\n",
      "Epoch 00160: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 3.5685 - accuracy: 0.2000 - val_loss: 5.2125 - val_accuracy: 0.0000e+00\n",
      "Epoch 161/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9941 - accuracy: 0.1000\n",
      "Epoch 00161: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.9941 - accuracy: 0.1000 - val_loss: 5.9096 - val_accuracy: 0.0000e+00\n",
      "Epoch 162/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5174 - accuracy: 0.2000\n",
      "Epoch 00162: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.5174 - accuracy: 0.2000 - val_loss: 3.8776 - val_accuracy: 0.3000\n",
      "Epoch 163/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7424 - accuracy: 0.1333\n",
      "Epoch 00163: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.7424 - accuracy: 0.1333 - val_loss: 3.9359 - val_accuracy: 0.5000\n",
      "Epoch 164/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4143 - accuracy: 0.2667\n",
      "Epoch 00164: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.4143 - accuracy: 0.2667 - val_loss: 2.6684 - val_accuracy: 0.2000\n",
      "Epoch 165/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.4713 - accuracy: 0.1667\n",
      "Epoch 00165: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 4.4713 - accuracy: 0.1667 - val_loss: 3.2956 - val_accuracy: 0.1000\n",
      "Epoch 166/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4975 - accuracy: 0.2333\n",
      "Epoch 00166: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.4975 - accuracy: 0.2333 - val_loss: 3.7416 - val_accuracy: 0.1000\n",
      "Epoch 167/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0548 - accuracy: 0.2667\n",
      "Epoch 00167: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 3.0548 - accuracy: 0.2667 - val_loss: 6.5571 - val_accuracy: 0.0000e+00\n",
      "Epoch 168/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4548 - accuracy: 0.1667\n",
      "Epoch 00168: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.4548 - accuracy: 0.1667 - val_loss: 3.6236 - val_accuracy: 0.2000\n",
      "Epoch 169/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0271 - accuracy: 0.1333\n",
      "Epoch 00169: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 3.0271 - accuracy: 0.1333 - val_loss: 3.8759 - val_accuracy: 0.0000e+00\n",
      "Epoch 170/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6507 - accuracy: 0.2000\n",
      "Epoch 00170: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.6507 - accuracy: 0.2000 - val_loss: 5.9798 - val_accuracy: 0.0000e+00\n",
      "Epoch 171/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8533 - accuracy: 0.1333\n",
      "Epoch 00171: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.8533 - accuracy: 0.1333 - val_loss: 2.3616 - val_accuracy: 0.4000\n",
      "Epoch 172/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6952 - accuracy: 0.2667\n",
      "Epoch 00172: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 2.6952 - accuracy: 0.2667 - val_loss: 2.4488 - val_accuracy: 0.7000\n",
      "Epoch 173/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3300 - accuracy: 0.1333\n",
      "Epoch 00173: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.3300 - accuracy: 0.1333 - val_loss: 2.1544 - val_accuracy: 0.2000\n",
      "Epoch 174/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6526 - accuracy: 0.1667\n",
      "Epoch 00174: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.6526 - accuracy: 0.1667 - val_loss: 6.7475 - val_accuracy: 0.0000e+00\n",
      "Epoch 175/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2678 - accuracy: 0.1000\n",
      "Epoch 00175: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 4.2678 - accuracy: 0.1000 - val_loss: 4.9874 - val_accuracy: 0.0000e+00\n",
      "Epoch 176/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5425 - accuracy: 0.1333\n",
      "Epoch 00176: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.5425 - accuracy: 0.1333 - val_loss: 3.5404 - val_accuracy: 0.1000\n",
      "Epoch 177/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4953 - accuracy: 0.2667\n",
      "Epoch 00177: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.4953 - accuracy: 0.2667 - val_loss: 2.7764 - val_accuracy: 0.2000\n",
      "Epoch 178/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2101 - accuracy: 0.2667\n",
      "Epoch 00178: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.2101 - accuracy: 0.2667 - val_loss: 2.8226 - val_accuracy: 0.2000\n",
      "Epoch 179/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6570 - accuracy: 0.1667\n",
      "Epoch 00179: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.6570 - accuracy: 0.1667 - val_loss: 4.1906 - val_accuracy: 0.1000\n",
      "Epoch 180/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9038 - accuracy: 0.2333\n",
      "Epoch 00180: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.9038 - accuracy: 0.2333 - val_loss: 4.1835 - val_accuracy: 0.2000\n",
      "Epoch 181/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7196 - accuracy: 0.2333\n",
      "Epoch 00181: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.7196 - accuracy: 0.2333 - val_loss: 2.2363 - val_accuracy: 0.5000\n",
      "Epoch 182/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3925 - accuracy: 0.2333\n",
      "Epoch 00182: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 3.3925 - accuracy: 0.2333 - val_loss: 1.4542 - val_accuracy: 0.7000\n",
      "Epoch 183/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0603 - accuracy: 0.1667\n",
      "Epoch 00183: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.0603 - accuracy: 0.1667 - val_loss: 5.1529 - val_accuracy: 0.0000e+00\n",
      "Epoch 184/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8436 - accuracy: 0.2333\n",
      "Epoch 00184: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.8436 - accuracy: 0.2333 - val_loss: 4.5199 - val_accuracy: 0.1000\n",
      "Epoch 185/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5272 - accuracy: 0.2000\n",
      "Epoch 00185: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.5272 - accuracy: 0.2000 - val_loss: 4.8641 - val_accuracy: 0.1000\n",
      "Epoch 186/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7182 - accuracy: 0.2000\n",
      "Epoch 00186: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.7182 - accuracy: 0.2000 - val_loss: 5.6662 - val_accuracy: 0.0000e+00\n",
      "Epoch 187/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2296 - accuracy: 0.2333\n",
      "Epoch 00187: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 3.2296 - accuracy: 0.2333 - val_loss: 7.7229 - val_accuracy: 0.0000e+00\n",
      "Epoch 188/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0657 - accuracy: 0.2667\n",
      "Epoch 00188: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.0657 - accuracy: 0.2667 - val_loss: 5.9322 - val_accuracy: 0.0000e+00\n",
      "Epoch 189/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.5072 - accuracy: 0.0333\n",
      "Epoch 00189: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 4.5072 - accuracy: 0.0333 - val_loss: 3.4071 - val_accuracy: 0.2000\n",
      "Epoch 190/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6081 - accuracy: 0.1333\n",
      "Epoch 00190: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.6081 - accuracy: 0.1333 - val_loss: 3.8326 - val_accuracy: 0.1000\n",
      "Epoch 191/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9152 - accuracy: 0.3667\n",
      "Epoch 00191: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.9152 - accuracy: 0.3667 - val_loss: 1.0255 - val_accuracy: 0.7000\n",
      "Epoch 192/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5795 - accuracy: 0.3000\n",
      "Epoch 00192: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.5795 - accuracy: 0.3000 - val_loss: 1.5138 - val_accuracy: 0.6000\n",
      "Epoch 193/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7763 - accuracy: 0.1333\n",
      "Epoch 00193: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.7763 - accuracy: 0.1333 - val_loss: 1.6427 - val_accuracy: 0.7000\n",
      "Epoch 194/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5437 - accuracy: 0.1667\n",
      "Epoch 00194: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.5437 - accuracy: 0.1667 - val_loss: 5.0884 - val_accuracy: 0.0000e+00\n",
      "Epoch 195/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5725 - accuracy: 0.2000\n",
      "Epoch 00195: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.5725 - accuracy: 0.2000 - val_loss: 5.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 196/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7525 - accuracy: 0.2000\n",
      "Epoch 00196: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.7525 - accuracy: 0.2000 - val_loss: 3.5750 - val_accuracy: 0.3000\n",
      "Epoch 197/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4111 - accuracy: 0.3000\n",
      "Epoch 00197: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.4111 - accuracy: 0.3000 - val_loss: 2.7828 - val_accuracy: 0.4000\n",
      "Epoch 198/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6771 - accuracy: 0.3667\n",
      "Epoch 00198: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.6771 - accuracy: 0.3667 - val_loss: 1.8807 - val_accuracy: 0.7000\n",
      "Epoch 199/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1612 - accuracy: 0.1667\n",
      "Epoch 00199: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.1612 - accuracy: 0.1667 - val_loss: 3.6043 - val_accuracy: 0.0000e+00\n",
      "Epoch 200/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6917 - accuracy: 0.1667\n",
      "Epoch 00200: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.6917 - accuracy: 0.1667 - val_loss: 2.4833 - val_accuracy: 0.4000\n",
      "Epoch 201/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6880 - accuracy: 0.2000\n",
      "Epoch 00201: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.6880 - accuracy: 0.2000 - val_loss: 3.2050 - val_accuracy: 0.2000\n",
      "Epoch 202/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5538 - accuracy: 0.2667\n",
      "Epoch 00202: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 3.5538 - accuracy: 0.2667 - val_loss: 2.5795 - val_accuracy: 0.5000\n",
      "Epoch 203/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2147 - accuracy: 0.2667\n",
      "Epoch 00203: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.2147 - accuracy: 0.2667 - val_loss: 2.9769 - val_accuracy: 0.0000e+00\n",
      "Epoch 204/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6614 - accuracy: 0.2333\n",
      "Epoch 00204: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.6614 - accuracy: 0.2333 - val_loss: 3.1368 - val_accuracy: 0.2000\n",
      "Epoch 205/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3702 - accuracy: 0.2333\n",
      "Epoch 00205: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.3702 - accuracy: 0.2333 - val_loss: 4.7697 - val_accuracy: 0.1000\n",
      "Epoch 206/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9903 - accuracy: 0.3000\n",
      "Epoch 00206: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.9903 - accuracy: 0.3000 - val_loss: 2.1940 - val_accuracy: 0.5000\n",
      "Epoch 207/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6754 - accuracy: 0.1333\n",
      "Epoch 00207: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.6754 - accuracy: 0.1333 - val_loss: 3.7585 - val_accuracy: 0.2000\n",
      "Epoch 208/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5767 - accuracy: 0.1000\n",
      "Epoch 00208: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.5767 - accuracy: 0.1000 - val_loss: 3.4269 - val_accuracy: 0.0000e+00\n",
      "Epoch 209/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1001 - accuracy: 0.3000\n",
      "Epoch 00209: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 3.1001 - accuracy: 0.3000 - val_loss: 0.9639 - val_accuracy: 0.8000\n",
      "Epoch 210/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4960 - accuracy: 0.1667\n",
      "Epoch 00210: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.4960 - accuracy: 0.1667 - val_loss: 1.4587 - val_accuracy: 0.7000\n",
      "Epoch 211/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1100 - accuracy: 0.3333\n",
      "Epoch 00211: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.1100 - accuracy: 0.3333 - val_loss: 1.4425 - val_accuracy: 0.7000\n",
      "Epoch 212/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7855 - accuracy: 0.2333\n",
      "Epoch 00212: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.7855 - accuracy: 0.2333 - val_loss: 2.4307 - val_accuracy: 0.3000\n",
      "Epoch 213/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5540 - accuracy: 0.2000\n",
      "Epoch 00213: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.5540 - accuracy: 0.2000 - val_loss: 3.0751 - val_accuracy: 0.0000e+00\n",
      "Epoch 214/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7497 - accuracy: 0.2667\n",
      "Epoch 00214: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.7497 - accuracy: 0.2667 - val_loss: 1.9698 - val_accuracy: 0.1000\n",
      "Epoch 215/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1233 - accuracy: 0.0667\n",
      "Epoch 00215: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 4.1233 - accuracy: 0.0667 - val_loss: 2.0212 - val_accuracy: 0.1000\n",
      "Epoch 216/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1470 - accuracy: 0.3333\n",
      "Epoch 00216: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.1470 - accuracy: 0.3333 - val_loss: 2.4487 - val_accuracy: 0.4000\n",
      "Epoch 217/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8296 - accuracy: 0.2667\n",
      "Epoch 00217: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.8296 - accuracy: 0.2667 - val_loss: 2.3856 - val_accuracy: 0.7000\n",
      "Epoch 218/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8407 - accuracy: 0.2333\n",
      "Epoch 00218: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.8407 - accuracy: 0.2333 - val_loss: 2.4809 - val_accuracy: 0.7000\n",
      "Epoch 219/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4093 - accuracy: 0.3000\n",
      "Epoch 00219: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 3.4093 - accuracy: 0.3000 - val_loss: 2.6791 - val_accuracy: 0.2000\n",
      "Epoch 220/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1424 - accuracy: 0.2333\n",
      "Epoch 00220: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 3.1424 - accuracy: 0.2333 - val_loss: 1.9566 - val_accuracy: 0.6000\n",
      "Epoch 221/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9604 - accuracy: 0.3333\n",
      "Epoch 00221: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.9604 - accuracy: 0.3333 - val_loss: 2.3688 - val_accuracy: 0.4000\n",
      "Epoch 222/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2138 - accuracy: 0.2333\n",
      "Epoch 00222: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.2138 - accuracy: 0.2333 - val_loss: 1.5069 - val_accuracy: 0.7000\n",
      "Epoch 223/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0028 - accuracy: 0.2333\n",
      "Epoch 00223: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 3.0028 - accuracy: 0.2333 - val_loss: 0.7662 - val_accuracy: 0.8000\n",
      "Epoch 224/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.7567 - accuracy: 0.3333\n",
      "Epoch 00224: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 3.4697 - accuracy: 0.2667 - val_loss: 0.9522 - val_accuracy: 0.8000\n",
      "Epoch 225/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5467 - accuracy: 0.3333\n",
      "Epoch 00225: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.5467 - accuracy: 0.3333 - val_loss: 4.5865 - val_accuracy: 0.0000e+00\n",
      "Epoch 226/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5732 - accuracy: 0.1333\n",
      "Epoch 00226: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 3.5732 - accuracy: 0.1333 - val_loss: 3.1975 - val_accuracy: 0.1000\n",
      "Epoch 227/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2670 - accuracy: 0.3000\n",
      "Epoch 00227: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.2670 - accuracy: 0.3000 - val_loss: 2.1603 - val_accuracy: 0.6000\n",
      "Epoch 228/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4410 - accuracy: 0.2667\n",
      "Epoch 00228: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.4410 - accuracy: 0.2667 - val_loss: 2.3928 - val_accuracy: 0.3000\n",
      "Epoch 229/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2972 - accuracy: 0.1667\n",
      "Epoch 00229: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 4.2972 - accuracy: 0.1667 - val_loss: 2.6398 - val_accuracy: 0.5000\n",
      "Epoch 230/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1947 - accuracy: 0.2333\n",
      "Epoch 00230: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.1947 - accuracy: 0.2333 - val_loss: 5.0926 - val_accuracy: 0.0000e+00\n",
      "Epoch 231/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4554 - accuracy: 0.1852\n",
      "Epoch 00231: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.4554 - accuracy: 0.1852 - val_loss: 3.2186 - val_accuracy: 0.2000\n",
      "Epoch 232/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9168 - accuracy: 0.4000\n",
      "Epoch 00232: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 2.9168 - accuracy: 0.4000 - val_loss: 1.5523 - val_accuracy: 0.6000\n",
      "Epoch 233/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2831 - accuracy: 0.1667\n",
      "Epoch 00233: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.2831 - accuracy: 0.1667 - val_loss: 4.4755 - val_accuracy: 0.0000e+00\n",
      "Epoch 234/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2831 - accuracy: 0.2333\n",
      "Epoch 00234: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 3.2831 - accuracy: 0.2333 - val_loss: 5.2315 - val_accuracy: 0.0000e+00\n",
      "Epoch 235/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6312 - accuracy: 0.2000\n",
      "Epoch 00235: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.6312 - accuracy: 0.2000 - val_loss: 2.9261 - val_accuracy: 0.5000\n",
      "Epoch 236/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0523 - accuracy: 0.2333\n",
      "Epoch 00236: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.0523 - accuracy: 0.2333 - val_loss: 4.3207 - val_accuracy: 0.4000\n",
      "Epoch 237/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2458 - accuracy: 0.2000\n",
      "Epoch 00237: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.2458 - accuracy: 0.2000 - val_loss: 3.0037 - val_accuracy: 0.3000\n",
      "Epoch 238/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1465 - accuracy: 0.3333\n",
      "Epoch 00238: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.1465 - accuracy: 0.3333 - val_loss: 2.6744 - val_accuracy: 0.4000\n",
      "Epoch 239/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.5974 - accuracy: 0.4000\n",
      "Epoch 00239: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.5539 - accuracy: 0.2667 - val_loss: 3.4124 - val_accuracy: 0.2000\n",
      "Epoch 240/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5663 - accuracy: 0.2667\n",
      "Epoch 00240: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.5663 - accuracy: 0.2667 - val_loss: 1.8156 - val_accuracy: 0.4000\n",
      "Epoch 241/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6248 - accuracy: 0.2333\n",
      "Epoch 00241: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.6248 - accuracy: 0.2333 - val_loss: 3.5264 - val_accuracy: 0.3000\n",
      "Epoch 242/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5653 - accuracy: 0.2333\n",
      "Epoch 00242: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.5653 - accuracy: 0.2333 - val_loss: 5.6078 - val_accuracy: 0.3000\n",
      "Epoch 243/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7285 - accuracy: 0.4000\n",
      "Epoch 00243: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.7285 - accuracy: 0.4000 - val_loss: 1.8847 - val_accuracy: 0.5000\n",
      "Epoch 244/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1033 - accuracy: 0.1333\n",
      "Epoch 00244: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.1033 - accuracy: 0.1333 - val_loss: 2.0684 - val_accuracy: 0.5000\n",
      "Epoch 245/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5177 - accuracy: 0.1667    \n",
      "Epoch 00245: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 3.5177 - accuracy: 0.1667 - val_loss: 1.2858 - val_accuracy: 0.7000\n",
      "Epoch 246/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0496 - accuracy: 0.2667\n",
      "Epoch 00246: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.0496 - accuracy: 0.2667 - val_loss: 3.4231 - val_accuracy: 0.4000\n",
      "Epoch 247/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2514 - accuracy: 0.2000\n",
      "Epoch 00247: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 3.2514 - accuracy: 0.2000 - val_loss: 4.1559 - val_accuracy: 0.1000\n",
      "Epoch 248/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8601 - accuracy: 0.4000\n",
      "Epoch 00248: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 2.8601 - accuracy: 0.4000 - val_loss: 3.4436 - val_accuracy: 0.1000\n",
      "Epoch 249/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1688 - accuracy: 0.2000\n",
      "Epoch 00249: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 3.1688 - accuracy: 0.2000 - val_loss: 2.7742 - val_accuracy: 0.2000\n",
      "Epoch 250/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.7381 - accuracy: 0.4000\n",
      "Epoch 00250: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 3.2949 - accuracy: 0.3667 - val_loss: 3.8841 - val_accuracy: 0.0000e+00\n",
      "Epoch 251/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.3489 - accuracy: 0.4000\n",
      "Epoch 00251: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 3.0112 - accuracy: 0.3667 - val_loss: 1.8436 - val_accuracy: 0.5000\n",
      "Epoch 252/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5244 - accuracy: 0.2667\n",
      "Epoch 00252: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.5244 - accuracy: 0.2667 - val_loss: 1.8328 - val_accuracy: 0.5000\n",
      "Epoch 253/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2251 - accuracy: 0.2333\n",
      "Epoch 00253: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.2251 - accuracy: 0.2333 - val_loss: 4.0797 - val_accuracy: 0.1000\n",
      "Epoch 254/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3433 - accuracy: 0.2667\n",
      "Epoch 00254: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.3433 - accuracy: 0.2667 - val_loss: 4.5930 - val_accuracy: 0.1000\n",
      "Epoch 255/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8179 - accuracy: 0.3000\n",
      "Epoch 00255: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.8179 - accuracy: 0.3000 - val_loss: 3.0773 - val_accuracy: 0.1000\n",
      "Epoch 256/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3538 - accuracy: 0.2667\n",
      "Epoch 00256: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.3538 - accuracy: 0.2667 - val_loss: 2.9609 - val_accuracy: 0.1000\n",
      "Epoch 257/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3201 - accuracy: 0.2000    \n",
      "Epoch 00257: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.3201 - accuracy: 0.2000 - val_loss: 2.4880 - val_accuracy: 0.2000\n",
      "Epoch 258/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9037 - accuracy: 0.2667\n",
      "Epoch 00258: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.9037 - accuracy: 0.2667 - val_loss: 5.5987 - val_accuracy: 0.0000e+00\n",
      "Epoch 259/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0511 - accuracy: 0.2333\n",
      "Epoch 00259: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.0511 - accuracy: 0.2333 - val_loss: 2.0592 - val_accuracy: 0.6000\n",
      "Epoch 260/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8702 - accuracy: 0.3000\n",
      "Epoch 00260: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.8702 - accuracy: 0.3000 - val_loss: 2.3103 - val_accuracy: 0.3000\n",
      "Epoch 261/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3966 - accuracy: 0.1667\n",
      "Epoch 00261: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.3966 - accuracy: 0.1667 - val_loss: 0.5103 - val_accuracy: 0.9000\n",
      "Epoch 262/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3772 - accuracy: 0.2333\n",
      "Epoch 00262: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.3772 - accuracy: 0.2333 - val_loss: 2.1121 - val_accuracy: 0.0000e+00\n",
      "Epoch 263/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7181 - accuracy: 0.3667\n",
      "Epoch 00263: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 2.7181 - accuracy: 0.3667 - val_loss: 2.1407 - val_accuracy: 0.3000\n",
      "Epoch 264/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5463 - accuracy: 0.2667\n",
      "Epoch 00264: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.5463 - accuracy: 0.2667 - val_loss: 2.7014 - val_accuracy: 0.0000e+00\n",
      "Epoch 265/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4271 - accuracy: 0.1667\n",
      "Epoch 00265: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.4271 - accuracy: 0.1667 - val_loss: 4.2110 - val_accuracy: 0.0000e+00\n",
      "Epoch 266/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2123 - accuracy: 0.3000\n",
      "Epoch 00266: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.2123 - accuracy: 0.3000 - val_loss: 3.9195 - val_accuracy: 0.1000\n",
      "Epoch 267/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0454 - accuracy: 0.3333\n",
      "Epoch 00267: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.0454 - accuracy: 0.3333 - val_loss: 3.1026 - val_accuracy: 0.3000\n",
      "Epoch 268/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9151 - accuracy: 0.2333\n",
      "Epoch 00268: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 2.9151 - accuracy: 0.2333 - val_loss: 3.0781 - val_accuracy: 0.2000\n",
      "Epoch 269/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5872 - accuracy: 0.2333\n",
      "Epoch 00269: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.5872 - accuracy: 0.2333 - val_loss: 3.1562 - val_accuracy: 0.3000\n",
      "Epoch 270/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9509 - accuracy: 0.2667\n",
      "Epoch 00270: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.9509 - accuracy: 0.2667 - val_loss: 3.0560 - val_accuracy: 0.1000\n",
      "Epoch 271/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3480 - accuracy: 0.1667\n",
      "Epoch 00271: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.3480 - accuracy: 0.1667 - val_loss: 2.3988 - val_accuracy: 0.3000\n",
      "Epoch 272/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0914 - accuracy: 0.2667\n",
      "Epoch 00272: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 3.0914 - accuracy: 0.2667 - val_loss: 2.8002 - val_accuracy: 0.2000\n",
      "Epoch 273/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6332 - accuracy: 0.3000\n",
      "Epoch 00273: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.6332 - accuracy: 0.3000 - val_loss: 3.2294 - val_accuracy: 0.2000\n",
      "Epoch 274/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9436 - accuracy: 0.3333\n",
      "Epoch 00274: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.9436 - accuracy: 0.3333 - val_loss: 1.9824 - val_accuracy: 0.4000\n",
      "Epoch 275/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8069 - accuracy: 0.3333\n",
      "Epoch 00275: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.8069 - accuracy: 0.3333 - val_loss: 2.0536 - val_accuracy: 0.5000\n",
      "Epoch 276/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0215 - accuracy: 0.3000\n",
      "Epoch 00276: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.0215 - accuracy: 0.3000 - val_loss: 4.1122 - val_accuracy: 0.0000e+00\n",
      "Epoch 277/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6165 - accuracy: 0.3000\n",
      "Epoch 00277: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.6165 - accuracy: 0.3000 - val_loss: 6.0045 - val_accuracy: 0.0000e+00\n",
      "Epoch 278/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6205 - accuracy: 0.3000\n",
      "Epoch 00278: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.6205 - accuracy: 0.3000 - val_loss: 9.1328 - val_accuracy: 0.0000e+00\n",
      "Epoch 279/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9950 - accuracy: 0.3000\n",
      "Epoch 00279: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.9950 - accuracy: 0.3000 - val_loss: 3.9483 - val_accuracy: 0.0000e+00\n",
      "Epoch 280/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1343 - accuracy: 0.2667\n",
      "Epoch 00280: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.1343 - accuracy: 0.2667 - val_loss: 3.7706 - val_accuracy: 0.1000\n",
      "Epoch 281/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7573 - accuracy: 0.1667\n",
      "Epoch 00281: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.7573 - accuracy: 0.1667 - val_loss: 4.7552 - val_accuracy: 0.2000\n",
      "Epoch 282/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2602 - accuracy: 0.2333\n",
      "Epoch 00282: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.2602 - accuracy: 0.2333 - val_loss: 1.6333 - val_accuracy: 0.5000\n",
      "Epoch 283/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0262 - accuracy: 0.1667\n",
      "Epoch 00283: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0262 - accuracy: 0.1667 - val_loss: 1.4669 - val_accuracy: 0.5000\n",
      "Epoch 284/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7915 - accuracy: 0.2333\n",
      "Epoch 00284: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.7915 - accuracy: 0.2333 - val_loss: 1.5913 - val_accuracy: 0.6000\n",
      "Epoch 285/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0134 - accuracy: 0.3000\n",
      "Epoch 00285: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.0134 - accuracy: 0.3000 - val_loss: 2.5351 - val_accuracy: 0.5000\n",
      "Epoch 286/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8311 - accuracy: 0.2667\n",
      "Epoch 00286: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.8311 - accuracy: 0.2667 - val_loss: 0.9435 - val_accuracy: 0.6000\n",
      "Epoch 287/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3935 - accuracy: 0.2667\n",
      "Epoch 00287: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.3935 - accuracy: 0.2667 - val_loss: 4.3556 - val_accuracy: 0.0000e+00\n",
      "Epoch 288/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2266 - accuracy: 0.0667    \n",
      "Epoch 00288: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 4.2266 - accuracy: 0.0667 - val_loss: 1.8607 - val_accuracy: 0.5000\n",
      "Epoch 289/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4987 - accuracy: 0.1667\n",
      "Epoch 00289: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.4987 - accuracy: 0.1667 - val_loss: 1.7367 - val_accuracy: 0.6000\n",
      "Epoch 290/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9492 - accuracy: 0.3000\n",
      "Epoch 00290: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 2.9492 - accuracy: 0.3000 - val_loss: 3.1609 - val_accuracy: 0.4000\n",
      "Epoch 291/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8419 - accuracy: 0.3000\n",
      "Epoch 00291: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.8419 - accuracy: 0.3000 - val_loss: 2.0286 - val_accuracy: 0.0000e+00\n",
      "Epoch 292/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0307 - accuracy: 0.1667\n",
      "Epoch 00292: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 4.0307 - accuracy: 0.1667 - val_loss: 2.4411 - val_accuracy: 0.1000\n",
      "Epoch 293/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3743 - accuracy: 0.2333\n",
      "Epoch 00293: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.3743 - accuracy: 0.2333 - val_loss: 4.6201 - val_accuracy: 0.0000e+00\n",
      "Epoch 294/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3041 - accuracy: 0.3000\n",
      "Epoch 00294: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.3041 - accuracy: 0.3000 - val_loss: 3.1436 - val_accuracy: 0.2000\n",
      "Epoch 295/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8068 - accuracy: 0.2000\n",
      "Epoch 00295: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.8068 - accuracy: 0.2000 - val_loss: 4.3083 - val_accuracy: 0.0000e+00\n",
      "Epoch 296/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8393 - accuracy: 0.1667\n",
      "Epoch 00296: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.8393 - accuracy: 0.1667 - val_loss: 3.3905 - val_accuracy: 0.3000\n",
      "Epoch 297/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5181 - accuracy: 0.2000\n",
      "Epoch 00297: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.5181 - accuracy: 0.2000 - val_loss: 3.7837 - val_accuracy: 0.0000e+00\n",
      "Epoch 298/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8935 - accuracy: 0.3000\n",
      "Epoch 00298: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.8935 - accuracy: 0.3000 - val_loss: 1.9570 - val_accuracy: 0.4000\n",
      "Epoch 299/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7543 - accuracy: 0.2000\n",
      "Epoch 00299: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.7543 - accuracy: 0.2000 - val_loss: 3.5150 - val_accuracy: 0.3000\n",
      "Epoch 300/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8535 - accuracy: 0.4000\n",
      "Epoch 00300: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.8535 - accuracy: 0.4000 - val_loss: 3.2584 - val_accuracy: 0.2000\n",
      "Epoch 301/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3582 - accuracy: 0.4000\n",
      "Epoch 00301: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.3582 - accuracy: 0.4000 - val_loss: 4.6511 - val_accuracy: 0.1000\n",
      "Epoch 302/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3859 - accuracy: 0.1667\n",
      "Epoch 00302: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.3859 - accuracy: 0.1667 - val_loss: 1.6449 - val_accuracy: 0.7000\n",
      "Epoch 303/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6811 - accuracy: 0.1667\n",
      "Epoch 00303: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.6811 - accuracy: 0.1667 - val_loss: 4.3012 - val_accuracy: 0.2000\n",
      "Epoch 304/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0274 - accuracy: 0.3000\n",
      "Epoch 00304: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.0274 - accuracy: 0.3000 - val_loss: 1.9831 - val_accuracy: 0.6000\n",
      "Epoch 305/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2424 - accuracy: 0.3667\n",
      "Epoch 00305: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.2424 - accuracy: 0.3667 - val_loss: 2.9068 - val_accuracy: 0.4000\n",
      "Epoch 306/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3952 - accuracy: 0.1667\n",
      "Epoch 00306: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.3952 - accuracy: 0.1667 - val_loss: 1.6446 - val_accuracy: 0.5000\n",
      "Epoch 307/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2884 - accuracy: 0.2000\n",
      "Epoch 00307: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.2884 - accuracy: 0.2000 - val_loss: 2.9212 - val_accuracy: 0.8000\n",
      "Epoch 308/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1680 - accuracy: 0.3000\n",
      "Epoch 00308: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.1680 - accuracy: 0.3000 - val_loss: 1.9911 - val_accuracy: 0.7000\n",
      "Epoch 309/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7996 - accuracy: 0.3000\n",
      "Epoch 00309: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.7996 - accuracy: 0.3000 - val_loss: 4.6041 - val_accuracy: 0.1000\n",
      "Epoch 310/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9086 - accuracy: 0.2000\n",
      "Epoch 00310: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.9086 - accuracy: 0.2000 - val_loss: 5.1743 - val_accuracy: 0.0000e+00\n",
      "Epoch 311/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0417 - accuracy: 0.2667\n",
      "Epoch 00311: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.0417 - accuracy: 0.2667 - val_loss: 4.3129 - val_accuracy: 0.1000\n",
      "Epoch 312/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1459 - accuracy: 0.3333\n",
      "Epoch 00312: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.1459 - accuracy: 0.3333 - val_loss: 4.3507 - val_accuracy: 0.1000\n",
      "Epoch 313/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0361 - accuracy: 0.1667\n",
      "Epoch 00313: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 4.0361 - accuracy: 0.1667 - val_loss: 6.0616 - val_accuracy: 0.0000e+00\n",
      "Epoch 314/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2962 - accuracy: 0.2667\n",
      "Epoch 00314: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.2962 - accuracy: 0.2667 - val_loss: 1.7117 - val_accuracy: 0.7000\n",
      "Epoch 315/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0014 - accuracy: 0.3000\n",
      "Epoch 00315: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.0014 - accuracy: 0.3000 - val_loss: 0.8722 - val_accuracy: 0.7000\n",
      "Epoch 316/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5002 - accuracy: 0.1667\n",
      "Epoch 00316: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.5002 - accuracy: 0.1667 - val_loss: 4.2239 - val_accuracy: 0.0000e+00\n",
      "Epoch 317/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1521 - accuracy: 0.2667\n",
      "Epoch 00317: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.1521 - accuracy: 0.2667 - val_loss: 3.2683 - val_accuracy: 0.0000e+00\n",
      "Epoch 318/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2281 - accuracy: 0.2667\n",
      "Epoch 00318: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.2281 - accuracy: 0.2667 - val_loss: 3.1581 - val_accuracy: 0.1000\n",
      "Epoch 319/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8217 - accuracy: 0.2000\n",
      "Epoch 00319: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.8217 - accuracy: 0.2000 - val_loss: 3.1643 - val_accuracy: 0.1000\n",
      "Epoch 320/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2478 - accuracy: 0.3000\n",
      "Epoch 00320: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.2478 - accuracy: 0.3000 - val_loss: 2.7115 - val_accuracy: 0.2000\n",
      "Epoch 321/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0276 - accuracy: 0.1667\n",
      "Epoch 00321: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 4.0276 - accuracy: 0.1667 - val_loss: 2.3651 - val_accuracy: 0.4000\n",
      "Epoch 322/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0544 - accuracy: 0.3000\n",
      "Epoch 00322: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.0544 - accuracy: 0.3000 - val_loss: 1.2142 - val_accuracy: 0.6000\n",
      "Epoch 323/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6805 - accuracy: 0.2667\n",
      "Epoch 00323: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.6805 - accuracy: 0.2667 - val_loss: 2.4728 - val_accuracy: 0.6000\n",
      "Epoch 324/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2700 - accuracy: 0.3000\n",
      "Epoch 00324: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.2700 - accuracy: 0.3000 - val_loss: 1.2551 - val_accuracy: 0.6000\n",
      "Epoch 325/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.2971 - accuracy: 0.4000\n",
      "Epoch 00325: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.2971 - accuracy: 0.4000 - val_loss: 2.4023 - val_accuracy: 0.4000\n",
      "Epoch 326/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0279 - accuracy: 0.3333\n",
      "Epoch 00326: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.0279 - accuracy: 0.3333 - val_loss: 1.4374 - val_accuracy: 0.5000\n",
      "Epoch 327/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3157 - accuracy: 0.3000\n",
      "Epoch 00327: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 3.3157 - accuracy: 0.3000 - val_loss: 1.8277 - val_accuracy: 0.5000\n",
      "Epoch 328/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0056 - accuracy: 0.3667\n",
      "Epoch 00328: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.0056 - accuracy: 0.3667 - val_loss: 2.0992 - val_accuracy: 0.6000\n",
      "Epoch 329/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1082 - accuracy: 0.2667\n",
      "Epoch 00329: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.1082 - accuracy: 0.2667 - val_loss: 1.7685 - val_accuracy: 0.5000\n",
      "Epoch 330/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3282 - accuracy: 0.2333\n",
      "Epoch 00330: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.3282 - accuracy: 0.2333 - val_loss: 3.1903 - val_accuracy: 0.2000\n",
      "Epoch 331/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0221 - accuracy: 0.2667\n",
      "Epoch 00331: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0221 - accuracy: 0.2667 - val_loss: 2.7667 - val_accuracy: 0.4000\n",
      "Epoch 332/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5034 - accuracy: 0.1667\n",
      "Epoch 00332: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.5034 - accuracy: 0.1667 - val_loss: 3.3624 - val_accuracy: 0.3000\n",
      "Epoch 333/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6908 - accuracy: 0.3000\n",
      "Epoch 00333: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.6908 - accuracy: 0.3000 - val_loss: 4.7453 - val_accuracy: 0.2000\n",
      "Epoch 334/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6295 - accuracy: 0.2333\n",
      "Epoch 00334: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.6295 - accuracy: 0.2333 - val_loss: 2.0190 - val_accuracy: 0.3000\n",
      "Epoch 335/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6747 - accuracy: 0.1333\n",
      "Epoch 00335: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.6747 - accuracy: 0.1333 - val_loss: 2.0161 - val_accuracy: 0.6000\n",
      "Epoch 336/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2455 - accuracy: 0.3333\n",
      "Epoch 00336: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.2455 - accuracy: 0.3333 - val_loss: 2.7080 - val_accuracy: 0.7000\n",
      "Epoch 337/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7316 - accuracy: 0.3667\n",
      "Epoch 00337: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.7316 - accuracy: 0.3667 - val_loss: 1.8678 - val_accuracy: 0.6000\n",
      "Epoch 338/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4843 - accuracy: 0.3333\n",
      "Epoch 00338: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.4843 - accuracy: 0.3333 - val_loss: 2.5835 - val_accuracy: 0.4000\n",
      "Epoch 339/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5842 - accuracy: 0.3000\n",
      "Epoch 00339: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.5842 - accuracy: 0.3000 - val_loss: 2.7978 - val_accuracy: 0.3000\n",
      "Epoch 340/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2129 - accuracy: 0.2667\n",
      "Epoch 00340: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.2129 - accuracy: 0.2667 - val_loss: 1.1002 - val_accuracy: 0.8000\n",
      "Epoch 341/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5661 - accuracy: 0.0667\n",
      "Epoch 00341: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.5661 - accuracy: 0.0667 - val_loss: 4.1996 - val_accuracy: 0.0000e+00\n",
      "Epoch 342/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0764 - accuracy: 0.2667\n",
      "Epoch 00342: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.0764 - accuracy: 0.2667 - val_loss: 3.7394 - val_accuracy: 0.2000\n",
      "Epoch 343/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0429 - accuracy: 0.2667\n",
      "Epoch 00343: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.0429 - accuracy: 0.2667 - val_loss: 2.4197 - val_accuracy: 0.3000\n",
      "Epoch 344/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8341 - accuracy: 0.4000\n",
      "Epoch 00344: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.8341 - accuracy: 0.4000 - val_loss: 2.5667 - val_accuracy: 0.2000\n",
      "Epoch 345/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7305 - accuracy: 0.1333\n",
      "Epoch 00345: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.7305 - accuracy: 0.1333 - val_loss: 1.7345 - val_accuracy: 0.5000\n",
      "Epoch 346/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9710 - accuracy: 0.2333\n",
      "Epoch 00346: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.9710 - accuracy: 0.2333 - val_loss: 2.0923 - val_accuracy: 0.5000\n",
      "Epoch 347/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7902 - accuracy: 0.1000\n",
      "Epoch 00347: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.7902 - accuracy: 0.1000 - val_loss: 4.7116 - val_accuracy: 0.1000\n",
      "Epoch 348/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7775 - accuracy: 0.2667\n",
      "Epoch 00348: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.7775 - accuracy: 0.2667 - val_loss: 2.9871 - val_accuracy: 0.2000\n",
      "Epoch 349/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0280 - accuracy: 0.2000\n",
      "Epoch 00349: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.0280 - accuracy: 0.2000 - val_loss: 1.8933 - val_accuracy: 0.4000\n",
      "Epoch 350/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0729 - accuracy: 0.2667\n",
      "Epoch 00350: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.0729 - accuracy: 0.2667 - val_loss: 4.5672 - val_accuracy: 0.0000e+00\n",
      "Epoch 351/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6842 - accuracy: 0.2000\n",
      "Epoch 00351: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.6842 - accuracy: 0.2000 - val_loss: 2.5990 - val_accuracy: 0.3000\n",
      "Epoch 352/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1725 - accuracy: 0.2000\n",
      "Epoch 00352: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.1725 - accuracy: 0.2000 - val_loss: 2.8198 - val_accuracy: 0.0000e+00\n",
      "Epoch 353/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7497 - accuracy: 0.4000\n",
      "Epoch 00353: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.7497 - accuracy: 0.4000 - val_loss: 2.1982 - val_accuracy: 0.7000\n",
      "Epoch 354/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5150 - accuracy: 0.1667\n",
      "Epoch 00354: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.5150 - accuracy: 0.1667 - val_loss: 1.6074 - val_accuracy: 0.8000\n",
      "Epoch 355/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9243 - accuracy: 0.1333\n",
      "Epoch 00355: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.9243 - accuracy: 0.1333 - val_loss: 1.0379 - val_accuracy: 0.9000\n",
      "Epoch 356/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5758 - accuracy: 0.3667\n",
      "Epoch 00356: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.5758 - accuracy: 0.3667 - val_loss: 1.9457 - val_accuracy: 0.6000\n",
      "Epoch 357/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4795 - accuracy: 0.2000\n",
      "Epoch 00357: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.4795 - accuracy: 0.2000 - val_loss: 2.3591 - val_accuracy: 0.3000\n",
      "Epoch 358/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4196 - accuracy: 0.2333\n",
      "Epoch 00358: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.4196 - accuracy: 0.2333 - val_loss: 3.0036 - val_accuracy: 0.3000\n",
      "Epoch 359/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5958 - accuracy: 0.2333\n",
      "Epoch 00359: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 175ms/step - loss: 3.5958 - accuracy: 0.2333 - val_loss: 2.0642 - val_accuracy: 0.3000\n",
      "Epoch 360/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2599 - accuracy: 0.2000\n",
      "Epoch 00360: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 3.2599 - accuracy: 0.2000 - val_loss: 3.2538 - val_accuracy: 0.3000\n",
      "Epoch 361/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7005 - accuracy: 0.3000\n",
      "Epoch 00361: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.7005 - accuracy: 0.3000 - val_loss: 3.2165 - val_accuracy: 0.2000\n",
      "Epoch 362/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6102 - accuracy: 0.4667\n",
      "Epoch 00362: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.6102 - accuracy: 0.4667 - val_loss: 2.7635 - val_accuracy: 0.3000\n",
      "Epoch 363/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2307 - accuracy: 0.2000\n",
      "Epoch 00363: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.2307 - accuracy: 0.2000 - val_loss: 3.3951 - val_accuracy: 0.1000\n",
      "Epoch 364/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2936 - accuracy: 0.2000\n",
      "Epoch 00364: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.2936 - accuracy: 0.2000 - val_loss: 1.7164 - val_accuracy: 0.5000\n",
      "Epoch 365/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0483 - accuracy: 0.2333\n",
      "Epoch 00365: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.0483 - accuracy: 0.2333 - val_loss: 5.2721 - val_accuracy: 0.1000\n",
      "Epoch 366/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2517 - accuracy: 0.2000\n",
      "Epoch 00366: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.2517 - accuracy: 0.2000 - val_loss: 4.3922 - val_accuracy: 0.2000\n",
      "Epoch 367/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5520 - accuracy: 0.1667\n",
      "Epoch 00367: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.5520 - accuracy: 0.1667 - val_loss: 3.2216 - val_accuracy: 0.1000\n",
      "Epoch 368/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1067 - accuracy: 0.2333\n",
      "Epoch 00368: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.1067 - accuracy: 0.2333 - val_loss: 3.7309 - val_accuracy: 0.3000\n",
      "Epoch 369/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8671 - accuracy: 0.1333\n",
      "Epoch 00369: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.8671 - accuracy: 0.1333 - val_loss: 3.5823 - val_accuracy: 0.2000\n",
      "Epoch 370/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3833 - accuracy: 0.2667\n",
      "Epoch 00370: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.3833 - accuracy: 0.2667 - val_loss: 1.8819 - val_accuracy: 0.5000\n",
      "Epoch 371/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9407 - accuracy: 0.2667\n",
      "Epoch 00371: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.9407 - accuracy: 0.2667 - val_loss: 4.1065 - val_accuracy: 0.0000e+00\n",
      "Epoch 372/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4649 - accuracy: 0.2667\n",
      "Epoch 00372: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.4649 - accuracy: 0.2667 - val_loss: 3.4289 - val_accuracy: 0.2000\n",
      "Epoch 373/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5157 - accuracy: 0.2333\n",
      "Epoch 00373: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.5157 - accuracy: 0.2333 - val_loss: 2.5699 - val_accuracy: 0.2000\n",
      "Epoch 374/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9641 - accuracy: 0.3000\n",
      "Epoch 00374: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.9641 - accuracy: 0.3000 - val_loss: 6.4479 - val_accuracy: 0.0000e+00\n",
      "Epoch 375/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9112 - accuracy: 0.4000\n",
      "Epoch 00375: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 2.9112 - accuracy: 0.4000 - val_loss: 2.5905 - val_accuracy: 0.6000\n",
      "Epoch 376/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6021 - accuracy: 0.3667\n",
      "Epoch 00376: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.6021 - accuracy: 0.3667 - val_loss: 5.9510 - val_accuracy: 0.0000e+00\n",
      "Epoch 377/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0958 - accuracy: 0.2000\n",
      "Epoch 00377: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.0958 - accuracy: 0.2000 - val_loss: 6.0768 - val_accuracy: 0.0000e+00\n",
      "Epoch 378/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1761 - accuracy: 0.3333\n",
      "Epoch 00378: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.1761 - accuracy: 0.3333 - val_loss: 5.9286 - val_accuracy: 0.0000e+00\n",
      "Epoch 379/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4340 - accuracy: 0.5000\n",
      "Epoch 00379: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.4340 - accuracy: 0.5000 - val_loss: 4.5009 - val_accuracy: 0.1000\n",
      "Epoch 380/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6003 - accuracy: 0.3000\n",
      "Epoch 00380: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.6003 - accuracy: 0.3000 - val_loss: 4.1246 - val_accuracy: 0.3000\n",
      "Epoch 381/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0861 - accuracy: 0.0667\n",
      "Epoch 00381: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 4.0861 - accuracy: 0.0667 - val_loss: 3.7686 - val_accuracy: 0.0000e+00\n",
      "Epoch 382/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3743 - accuracy: 0.3000\n",
      "Epoch 00382: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.3743 - accuracy: 0.3000 - val_loss: 4.2736 - val_accuracy: 0.1000\n",
      "Epoch 383/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8650 - accuracy: 0.1667\n",
      "Epoch 00383: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.8650 - accuracy: 0.1667 - val_loss: 3.6626 - val_accuracy: 0.3000\n",
      "Epoch 384/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3503 - accuracy: 0.1667\n",
      "Epoch 00384: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.3503 - accuracy: 0.1667 - val_loss: 3.6909 - val_accuracy: 0.4000\n",
      "Epoch 385/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4905 - accuracy: 0.2000\n",
      "Epoch 00385: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.4905 - accuracy: 0.2000 - val_loss: 1.6725 - val_accuracy: 0.5000\n",
      "Epoch 386/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4408 - accuracy: 0.2333\n",
      "Epoch 00386: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.4408 - accuracy: 0.2333 - val_loss: 2.1429 - val_accuracy: 0.5000\n",
      "Epoch 387/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5639 - accuracy: 0.1333\n",
      "Epoch 00387: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.5639 - accuracy: 0.1333 - val_loss: 2.6700 - val_accuracy: 0.4000\n",
      "Epoch 388/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1669 - accuracy: 0.3333\n",
      "Epoch 00388: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.1669 - accuracy: 0.3333 - val_loss: 2.4652 - val_accuracy: 0.0000e+00\n",
      "Epoch 389/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0894 - accuracy: 0.1667\n",
      "Epoch 00389: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 4.0894 - accuracy: 0.1667 - val_loss: 3.9270 - val_accuracy: 0.0000e+00\n",
      "Epoch 390/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6556 - accuracy: 0.2000\n",
      "Epoch 00390: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.6556 - accuracy: 0.2000 - val_loss: 4.3202 - val_accuracy: 0.0000e+00\n",
      "Epoch 391/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0960 - accuracy: 0.1667\n",
      "Epoch 00391: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 4.0960 - accuracy: 0.1667 - val_loss: 3.4247 - val_accuracy: 0.0000e+00\n",
      "Epoch 392/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6120 - accuracy: 0.2333\n",
      "Epoch 00392: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.6120 - accuracy: 0.2333 - val_loss: 4.8204 - val_accuracy: 0.0000e+00\n",
      "Epoch 393/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5836 - accuracy: 0.2000\n",
      "Epoch 00393: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.5836 - accuracy: 0.2000 - val_loss: 6.5529 - val_accuracy: 0.0000e+00\n",
      "Epoch 394/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9672 - accuracy: 0.4000\n",
      "Epoch 00394: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 2.9672 - accuracy: 0.4000 - val_loss: 3.8620 - val_accuracy: 0.1000\n",
      "Epoch 395/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5910 - accuracy: 0.1333\n",
      "Epoch 00395: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.5910 - accuracy: 0.1333 - val_loss: 3.1935 - val_accuracy: 0.3000\n",
      "Epoch 396/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0880 - accuracy: 0.3000\n",
      "Epoch 00396: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.0880 - accuracy: 0.3000 - val_loss: 4.3340 - val_accuracy: 0.1000\n",
      "Epoch 397/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2733 - accuracy: 0.2333\n",
      "Epoch 00397: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.2733 - accuracy: 0.2333 - val_loss: 3.9815 - val_accuracy: 0.0000e+00\n",
      "Epoch 398/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8666 - accuracy: 0.3000\n",
      "Epoch 00398: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.8666 - accuracy: 0.3000 - val_loss: 5.5906 - val_accuracy: 0.0000e+00\n",
      "Epoch 399/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9651 - accuracy: 0.2000\n",
      "Epoch 00399: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.9651 - accuracy: 0.2000 - val_loss: 5.1013 - val_accuracy: 0.0000e+00\n",
      "Epoch 400/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1751 - accuracy: 0.4333\n",
      "Epoch 00400: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.1751 - accuracy: 0.4333 - val_loss: 1.3133 - val_accuracy: 0.6000\n",
      "Epoch 401/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7213 - accuracy: 0.3333\n",
      "Epoch 00401: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.7213 - accuracy: 0.3333 - val_loss: 2.9496 - val_accuracy: 0.3000\n",
      "Epoch 402/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1075 - accuracy: 0.2000\n",
      "Epoch 00402: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 4.1075 - accuracy: 0.2000 - val_loss: 3.0913 - val_accuracy: 0.4000\n",
      "Epoch 403/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3018 - accuracy: 0.2000\n",
      "Epoch 00403: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.3018 - accuracy: 0.2000 - val_loss: 3.5536 - val_accuracy: 0.1000\n",
      "Epoch 404/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1687 - accuracy: 0.2333\n",
      "Epoch 00404: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.1687 - accuracy: 0.2333 - val_loss: 5.3364 - val_accuracy: 0.0000e+00\n",
      "Epoch 405/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3958 - accuracy: 0.2333\n",
      "Epoch 00405: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.3958 - accuracy: 0.2333 - val_loss: 4.2667 - val_accuracy: 0.2000\n",
      "Epoch 406/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6294 - accuracy: 0.2000\n",
      "Epoch 00406: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.6294 - accuracy: 0.2000 - val_loss: 2.9833 - val_accuracy: 0.4000\n",
      "Epoch 407/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2020 - accuracy: 0.3333\n",
      "Epoch 00407: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.2020 - accuracy: 0.3333 - val_loss: 3.4748 - val_accuracy: 0.2000\n",
      "Epoch 408/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6261 - accuracy: 0.2333\n",
      "Epoch 00408: val_loss did not improve from 0.33214\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.6261 - accuracy: 0.2333 - val_loss: 3.0222 - val_accuracy: 0.3000\n",
      "Epoch 409/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7368 - accuracy: 0.2333\n",
      "Epoch 00409: val_loss improved from 0.33214 to 0.01684, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 3.7368 - accuracy: 0.2333 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
      "Epoch 410/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8413 - accuracy: 0.1333\n",
      "Epoch 00410: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.8413 - accuracy: 0.1333 - val_loss: 3.3894 - val_accuracy: 0.1000\n",
      "Epoch 411/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0426 - accuracy: 0.2333\n",
      "Epoch 00411: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.0426 - accuracy: 0.2333 - val_loss: 1.8384 - val_accuracy: 0.3000\n",
      "Epoch 412/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2908 - accuracy: 0.2000\n",
      "Epoch 00412: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.2908 - accuracy: 0.2000 - val_loss: 0.7889 - val_accuracy: 0.9000\n",
      "Epoch 413/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8477 - accuracy: 0.3000\n",
      "Epoch 00413: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.8477 - accuracy: 0.3000 - val_loss: 2.0213 - val_accuracy: 0.6000\n",
      "Epoch 414/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5040 - accuracy: 0.2000\n",
      "Epoch 00414: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.5040 - accuracy: 0.2000 - val_loss: 7.0368 - val_accuracy: 0.0000e+00\n",
      "Epoch 415/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7959 - accuracy: 0.1667\n",
      "Epoch 00415: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.7959 - accuracy: 0.1667 - val_loss: 5.8323 - val_accuracy: 0.0000e+00\n",
      "Epoch 416/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4705 - accuracy: 0.2000\n",
      "Epoch 00416: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.4705 - accuracy: 0.2000 - val_loss: 2.3067 - val_accuracy: 0.5000\n",
      "Epoch 417/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8457 - accuracy: 0.3333\n",
      "Epoch 00417: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.8457 - accuracy: 0.3333 - val_loss: 3.7151 - val_accuracy: 0.1000\n",
      "Epoch 418/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1940 - accuracy: 0.3333\n",
      "Epoch 00418: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.1940 - accuracy: 0.3333 - val_loss: 3.4119 - val_accuracy: 0.3000\n",
      "Epoch 419/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9502 - accuracy: 0.1333\n",
      "Epoch 00419: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.9502 - accuracy: 0.1333 - val_loss: 3.6192 - val_accuracy: 0.1000\n",
      "Epoch 420/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3179 - accuracy: 0.2667\n",
      "Epoch 00420: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.3179 - accuracy: 0.2667 - val_loss: 4.8481 - val_accuracy: 0.0000e+00\n",
      "Epoch 421/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9350 - accuracy: 0.2000\n",
      "Epoch 00421: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.9350 - accuracy: 0.2000 - val_loss: 4.8122 - val_accuracy: 0.1000\n",
      "Epoch 422/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4790 - accuracy: 0.2667\n",
      "Epoch 00422: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 3.4790 - accuracy: 0.2667 - val_loss: 2.7069 - val_accuracy: 0.4000\n",
      "Epoch 423/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8348 - accuracy: 0.3667\n",
      "Epoch 00423: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.8348 - accuracy: 0.3667 - val_loss: 3.8330 - val_accuracy: 0.3000\n",
      "Epoch 424/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2001 - accuracy: 0.2333\n",
      "Epoch 00424: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.2001 - accuracy: 0.2333 - val_loss: 2.5820 - val_accuracy: 0.5000\n",
      "Epoch 425/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4056 - accuracy: 0.3000\n",
      "Epoch 00425: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.4056 - accuracy: 0.3000 - val_loss: 4.7213 - val_accuracy: 0.0000e+00\n",
      "Epoch 426/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7996 - accuracy: 0.3333\n",
      "Epoch 00426: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.7996 - accuracy: 0.3333 - val_loss: 6.6251 - val_accuracy: 0.0000e+00\n",
      "Epoch 427/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0849 - accuracy: 0.2333\n",
      "Epoch 00427: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 4.0849 - accuracy: 0.2333 - val_loss: 5.1027 - val_accuracy: 0.0000e+00\n",
      "Epoch 428/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9985 - accuracy: 0.2667\n",
      "Epoch 00428: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.9985 - accuracy: 0.2667 - val_loss: 4.2147 - val_accuracy: 0.0000e+00\n",
      "Epoch 429/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4660 - accuracy: 0.3000\n",
      "Epoch 00429: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.4660 - accuracy: 0.3000 - val_loss: 7.8854 - val_accuracy: 0.0000e+00\n",
      "Epoch 430/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6886 - accuracy: 0.3333\n",
      "Epoch 00430: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.6886 - accuracy: 0.3333 - val_loss: 3.6890 - val_accuracy: 0.2000\n",
      "Epoch 431/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5034 - accuracy: 0.4000\n",
      "Epoch 00431: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.5034 - accuracy: 0.4000 - val_loss: 3.5734 - val_accuracy: 0.4000\n",
      "Epoch 432/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2374 - accuracy: 0.2000\n",
      "Epoch 00432: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.2374 - accuracy: 0.2000 - val_loss: 3.0996 - val_accuracy: 0.3000\n",
      "Epoch 433/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7640 - accuracy: 0.2333\n",
      "Epoch 00433: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.7640 - accuracy: 0.2333 - val_loss: 2.7648 - val_accuracy: 0.3000\n",
      "Epoch 434/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1557 - accuracy: 0.1000\n",
      "Epoch 00434: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 4.1557 - accuracy: 0.1000 - val_loss: 3.0163 - val_accuracy: 0.3000\n",
      "Epoch 435/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8417 - accuracy: 0.2000\n",
      "Epoch 00435: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.8417 - accuracy: 0.2000 - val_loss: 1.9561 - val_accuracy: 0.6000\n",
      "Epoch 436/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2780 - accuracy: 0.2667\n",
      "Epoch 00436: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.2780 - accuracy: 0.2667 - val_loss: 6.1742 - val_accuracy: 0.0000e+00\n",
      "Epoch 437/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8361 - accuracy: 0.1333\n",
      "Epoch 00437: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.8361 - accuracy: 0.1333 - val_loss: 4.1821 - val_accuracy: 0.0000e+00\n",
      "Epoch 438/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5039 - accuracy: 0.1333\n",
      "Epoch 00438: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.5039 - accuracy: 0.1333 - val_loss: 3.8379 - val_accuracy: 0.1000\n",
      "Epoch 439/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6642 - accuracy: 0.3000\n",
      "Epoch 00439: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.6642 - accuracy: 0.3000 - val_loss: 4.7895 - val_accuracy: 0.0000e+00\n",
      "Epoch 440/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4642 - accuracy: 0.2667\n",
      "Epoch 00440: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.4642 - accuracy: 0.2667 - val_loss: 2.6971 - val_accuracy: 0.3000\n",
      "Epoch 441/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6275 - accuracy: 0.2000\n",
      "Epoch 00441: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.6275 - accuracy: 0.2000 - val_loss: 3.4867 - val_accuracy: 0.2000\n",
      "Epoch 442/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4109 - accuracy: 0.3000\n",
      "Epoch 00442: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.4109 - accuracy: 0.3000 - val_loss: 3.9667 - val_accuracy: 0.3000\n",
      "Epoch 443/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8070 - accuracy: 0.1667\n",
      "Epoch 00443: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.8070 - accuracy: 0.1667 - val_loss: 7.4034 - val_accuracy: 0.0000e+00\n",
      "Epoch 444/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5577 - accuracy: 0.2667\n",
      "Epoch 00444: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.5577 - accuracy: 0.2667 - val_loss: 5.6609 - val_accuracy: 0.1000\n",
      "Epoch 445/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2108 - accuracy: 0.2667\n",
      "Epoch 00445: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.2108 - accuracy: 0.2667 - val_loss: 6.2468 - val_accuracy: 0.0000e+00\n",
      "Epoch 446/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5893 - accuracy: 0.1667\n",
      "Epoch 00446: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.5893 - accuracy: 0.1667 - val_loss: 3.7032 - val_accuracy: 0.3000\n",
      "Epoch 447/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1111 - accuracy: 0.3333\n",
      "Epoch 00447: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.1111 - accuracy: 0.3333 - val_loss: 1.9167 - val_accuracy: 0.8000\n",
      "Epoch 448/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7268 - accuracy: 0.3000\n",
      "Epoch 00448: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.7268 - accuracy: 0.3000 - val_loss: 3.9124 - val_accuracy: 0.2000\n",
      "Epoch 449/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7545 - accuracy: 0.1667\n",
      "Epoch 00449: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.7545 - accuracy: 0.1667 - val_loss: 2.1192 - val_accuracy: 0.5000\n",
      "Epoch 450/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7686 - accuracy: 0.3000\n",
      "Epoch 00450: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.7686 - accuracy: 0.3000 - val_loss: 4.1783 - val_accuracy: 0.5000\n",
      "Epoch 451/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2779 - accuracy: 0.2333\n",
      "Epoch 00451: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.2779 - accuracy: 0.2333 - val_loss: 4.6646 - val_accuracy: 0.0000e+00\n",
      "Epoch 452/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7595 - accuracy: 0.2000\n",
      "Epoch 00452: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.7595 - accuracy: 0.2000 - val_loss: 6.4154 - val_accuracy: 0.0000e+00\n",
      "Epoch 453/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3223 - accuracy: 0.1333\n",
      "Epoch 00453: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.3223 - accuracy: 0.1333 - val_loss: 3.9717 - val_accuracy: 0.0000e+00\n",
      "Epoch 454/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0423 - accuracy: 0.1667\n",
      "Epoch 00454: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.0423 - accuracy: 0.1667 - val_loss: 4.1803 - val_accuracy: 0.1000\n",
      "Epoch 455/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2233 - accuracy: 0.2667\n",
      "Epoch 00455: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.2233 - accuracy: 0.2667 - val_loss: 2.0647 - val_accuracy: 0.6000\n",
      "Epoch 456/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1095 - accuracy: 0.4000\n",
      "Epoch 00456: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.1095 - accuracy: 0.4000 - val_loss: 3.2234 - val_accuracy: 0.2000\n",
      "Epoch 457/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5868 - accuracy: 0.3000\n",
      "Epoch 00457: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.5868 - accuracy: 0.3000 - val_loss: 3.1936 - val_accuracy: 0.1000\n",
      "Epoch 458/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5003 - accuracy: 0.2000\n",
      "Epoch 00458: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.5003 - accuracy: 0.2000 - val_loss: 3.4628 - val_accuracy: 0.2000\n",
      "Epoch 459/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6922 - accuracy: 0.1333\n",
      "Epoch 00459: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.6922 - accuracy: 0.1333 - val_loss: 3.1038 - val_accuracy: 0.0000e+00\n",
      "Epoch 460/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3335 - accuracy: 0.2333\n",
      "Epoch 00460: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.3335 - accuracy: 0.2333 - val_loss: 3.7960 - val_accuracy: 0.0000e+00\n",
      "Epoch 461/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1024 - accuracy: 0.3000\n",
      "Epoch 00461: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.1024 - accuracy: 0.3000 - val_loss: 2.6185 - val_accuracy: 0.2000\n",
      "Epoch 462/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4649 - accuracy: 0.1333\n",
      "Epoch 00462: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.4649 - accuracy: 0.1333 - val_loss: 4.3303 - val_accuracy: 0.0000e+00\n",
      "Epoch 463/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5680 - accuracy: 0.2667\n",
      "Epoch 00463: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.5680 - accuracy: 0.2667 - val_loss: 3.1996 - val_accuracy: 0.4000\n",
      "Epoch 464/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5635 - accuracy: 0.1333\n",
      "Epoch 00464: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.5635 - accuracy: 0.1333 - val_loss: 3.1815 - val_accuracy: 0.4000\n",
      "Epoch 465/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6840 - accuracy: 0.3667\n",
      "Epoch 00465: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.6840 - accuracy: 0.3667 - val_loss: 5.3639 - val_accuracy: 0.0000e+00\n",
      "Epoch 466/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9457 - accuracy: 0.3333\n",
      "Epoch 00466: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.9457 - accuracy: 0.3333 - val_loss: 0.1228 - val_accuracy: 1.0000\n",
      "Epoch 467/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7848 - accuracy: 0.1667\n",
      "Epoch 00467: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 3.7848 - accuracy: 0.1667 - val_loss: 2.9124 - val_accuracy: 0.5000\n",
      "Epoch 468/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1246 - accuracy: 0.3000\n",
      "Epoch 00468: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.1246 - accuracy: 0.3000 - val_loss: 1.9667 - val_accuracy: 0.6000\n",
      "Epoch 469/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7873 - accuracy: 0.4000\n",
      "Epoch 00469: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.7873 - accuracy: 0.4000 - val_loss: 1.0468 - val_accuracy: 0.8000\n",
      "Epoch 470/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3703 - accuracy: 0.2333\n",
      "Epoch 00470: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.3703 - accuracy: 0.2333 - val_loss: 1.9276 - val_accuracy: 0.7000\n",
      "Epoch 471/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1810 - accuracy: 0.3000\n",
      "Epoch 00471: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.1810 - accuracy: 0.3000 - val_loss: 3.8601 - val_accuracy: 0.0000e+00\n",
      "Epoch 472/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5444 - accuracy: 0.3000\n",
      "Epoch 00472: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.5444 - accuracy: 0.3000 - val_loss: 3.6247 - val_accuracy: 0.3000\n",
      "Epoch 473/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1001 - accuracy: 0.3000\n",
      "Epoch 00473: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.1001 - accuracy: 0.3000 - val_loss: 2.6349 - val_accuracy: 0.3000\n",
      "Epoch 474/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7337 - accuracy: 0.2333\n",
      "Epoch 00474: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.7337 - accuracy: 0.2333 - val_loss: 3.4051 - val_accuracy: 0.0000e+00\n",
      "Epoch 475/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6973 - accuracy: 0.1000\n",
      "Epoch 00475: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.6973 - accuracy: 0.1000 - val_loss: 4.0904 - val_accuracy: 0.0000e+00\n",
      "Epoch 476/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2236 - accuracy: 0.3000\n",
      "Epoch 00476: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.2236 - accuracy: 0.3000 - val_loss: 2.4071 - val_accuracy: 0.3000\n",
      "Epoch 477/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8391 - accuracy: 0.3667\n",
      "Epoch 00477: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 2.8391 - accuracy: 0.3667 - val_loss: 1.2153 - val_accuracy: 0.7000\n",
      "Epoch 478/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.3229 - accuracy: 0.1667\n",
      "Epoch 00478: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 4.3229 - accuracy: 0.1667 - val_loss: 2.2615 - val_accuracy: 0.5000\n",
      "Epoch 479/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4635 - accuracy: 0.2667\n",
      "Epoch 00479: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.4635 - accuracy: 0.2667 - val_loss: 5.4420 - val_accuracy: 0.0000e+00\n",
      "Epoch 480/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4530 - accuracy: 0.2333\n",
      "Epoch 00480: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.4530 - accuracy: 0.2333 - val_loss: 3.2710 - val_accuracy: 0.2000\n",
      "Epoch 481/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5130 - accuracy: 0.2333\n",
      "Epoch 00481: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.5130 - accuracy: 0.2333 - val_loss: 2.8880 - val_accuracy: 0.2000\n",
      "Epoch 482/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4444 - accuracy: 0.2667\n",
      "Epoch 00482: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.4444 - accuracy: 0.2667 - val_loss: 1.7199 - val_accuracy: 0.5000\n",
      "Epoch 483/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7377 - accuracy: 0.1333\n",
      "Epoch 00483: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.7377 - accuracy: 0.1333 - val_loss: 0.6923 - val_accuracy: 0.8000\n",
      "Epoch 484/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9260 - accuracy: 0.2000\n",
      "Epoch 00484: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.9260 - accuracy: 0.2000 - val_loss: 1.7357 - val_accuracy: 0.5000\n",
      "Epoch 485/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8110 - accuracy: 0.1333\n",
      "Epoch 00485: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.8110 - accuracy: 0.1333 - val_loss: 6.5447 - val_accuracy: 0.0000e+00\n",
      "Epoch 486/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6159 - accuracy: 0.2333\n",
      "Epoch 00486: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.6159 - accuracy: 0.2333 - val_loss: 4.2851 - val_accuracy: 0.0000e+00\n",
      "Epoch 487/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5838 - accuracy: 0.2667\n",
      "Epoch 00487: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.5838 - accuracy: 0.2667 - val_loss: 4.9424 - val_accuracy: 0.0000e+00\n",
      "Epoch 488/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9189 - accuracy: 0.2333\n",
      "Epoch 00488: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 2.9189 - accuracy: 0.2333 - val_loss: 2.3304 - val_accuracy: 0.4000\n",
      "Epoch 489/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6231 - accuracy: 0.1667\n",
      "Epoch 00489: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.6231 - accuracy: 0.1667 - val_loss: 2.2102 - val_accuracy: 0.3000\n",
      "Epoch 490/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9338 - accuracy: 0.1667\n",
      "Epoch 00490: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.9338 - accuracy: 0.1667 - val_loss: 2.5827 - val_accuracy: 0.2000\n",
      "Epoch 491/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3781 - accuracy: 0.3667\n",
      "Epoch 00491: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.3781 - accuracy: 0.3667 - val_loss: 2.9334 - val_accuracy: 0.0000e+00\n",
      "Epoch 492/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7803 - accuracy: 0.3000\n",
      "Epoch 00492: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.7803 - accuracy: 0.3000 - val_loss: 5.2220 - val_accuracy: 0.0000e+00\n",
      "Epoch 493/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4140 - accuracy: 0.2333\n",
      "Epoch 00493: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.4140 - accuracy: 0.2333 - val_loss: 6.7773 - val_accuracy: 0.0000e+00\n",
      "Epoch 494/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7449 - accuracy: 0.1667\n",
      "Epoch 00494: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.7449 - accuracy: 0.1667 - val_loss: 3.4393 - val_accuracy: 0.3000\n",
      "Epoch 495/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6267 - accuracy: 0.3333\n",
      "Epoch 00495: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.6267 - accuracy: 0.3333 - val_loss: 2.5698 - val_accuracy: 0.5000\n",
      "Epoch 496/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9734 - accuracy: 0.1333\n",
      "Epoch 00496: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.9734 - accuracy: 0.1333 - val_loss: 2.6943 - val_accuracy: 0.4000\n",
      "Epoch 497/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2761 - accuracy: 0.4000\n",
      "Epoch 00497: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.2761 - accuracy: 0.4000 - val_loss: 4.0926 - val_accuracy: 0.0000e+00\n",
      "Epoch 498/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2919 - accuracy: 0.1667\n",
      "Epoch 00498: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 4.2919 - accuracy: 0.1667 - val_loss: 3.9498 - val_accuracy: 0.0000e+00\n",
      "Epoch 499/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2051 - accuracy: 0.2000\n",
      "Epoch 00499: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.2051 - accuracy: 0.2000 - val_loss: 5.8262 - val_accuracy: 0.0000e+00\n",
      "Epoch 500/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8369 - accuracy: 0.3000\n",
      "Epoch 00500: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.8369 - accuracy: 0.3000 - val_loss: 3.9189 - val_accuracy: 0.1000\n",
      "Epoch 501/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0439 - accuracy: 0.2333\n",
      "Epoch 00501: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.0439 - accuracy: 0.2333 - val_loss: 3.1473 - val_accuracy: 0.1000\n",
      "Epoch 502/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4905 - accuracy: 0.3333\n",
      "Epoch 00502: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 2.4905 - accuracy: 0.3333 - val_loss: 5.2701 - val_accuracy: 0.1000\n",
      "Epoch 503/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5764 - accuracy: 0.1667\n",
      "Epoch 00503: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.5764 - accuracy: 0.1667 - val_loss: 3.7769 - val_accuracy: 0.1000\n",
      "Epoch 504/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4613 - accuracy: 0.2000\n",
      "Epoch 00504: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.4613 - accuracy: 0.2000 - val_loss: 2.8642 - val_accuracy: 0.3000\n",
      "Epoch 505/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2139 - accuracy: 0.2000\n",
      "Epoch 00505: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 3.2139 - accuracy: 0.2000 - val_loss: 3.1284 - val_accuracy: 0.2000\n",
      "Epoch 506/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3766 - accuracy: 0.2667\n",
      "Epoch 00506: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.3766 - accuracy: 0.2667 - val_loss: 5.2869 - val_accuracy: 0.0000e+00\n",
      "Epoch 507/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6477 - accuracy: 0.2333\n",
      "Epoch 00507: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 3.6477 - accuracy: 0.2333 - val_loss: 4.3688 - val_accuracy: 0.0000e+00\n",
      "Epoch 508/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8840 - accuracy: 0.3667\n",
      "Epoch 00508: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 2.8840 - accuracy: 0.3667 - val_loss: 4.0728 - val_accuracy: 0.1000\n",
      "Epoch 509/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0613 - accuracy: 0.2333\n",
      "Epoch 00509: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.0613 - accuracy: 0.2333 - val_loss: 2.4215 - val_accuracy: 0.4000\n",
      "Epoch 510/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5476 - accuracy: 0.2000\n",
      "Epoch 00510: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 3.5476 - accuracy: 0.2000 - val_loss: 2.0696 - val_accuracy: 0.3000\n",
      "Epoch 511/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6008 - accuracy: 0.2000\n",
      "Epoch 00511: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.6008 - accuracy: 0.2000 - val_loss: 3.1746 - val_accuracy: 0.2000\n",
      "Epoch 512/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2928 - accuracy: 0.2667\n",
      "Epoch 00512: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.2928 - accuracy: 0.2667 - val_loss: 5.0559 - val_accuracy: 0.0000e+00\n",
      "Epoch 513/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6621 - accuracy: 0.2000\n",
      "Epoch 00513: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.6621 - accuracy: 0.2000 - val_loss: 2.1217 - val_accuracy: 0.7000\n",
      "Epoch 514/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8655 - accuracy: 0.2333\n",
      "Epoch 00514: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.8655 - accuracy: 0.2333 - val_loss: 2.7638 - val_accuracy: 0.4000\n",
      "Epoch 515/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7205 - accuracy: 0.1333\n",
      "Epoch 00515: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.7205 - accuracy: 0.1333 - val_loss: 4.9315 - val_accuracy: 0.0000e+00\n",
      "Epoch 516/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6137 - accuracy: 0.1667\n",
      "Epoch 00516: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.6137 - accuracy: 0.1667 - val_loss: 5.0656 - val_accuracy: 0.1000\n",
      "Epoch 517/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6019 - accuracy: 0.3000\n",
      "Epoch 00517: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.6019 - accuracy: 0.3000 - val_loss: 3.1575 - val_accuracy: 0.3000\n",
      "Epoch 518/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9971 - accuracy: 0.2000\n",
      "Epoch 00518: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.9971 - accuracy: 0.2000 - val_loss: 5.5131 - val_accuracy: 0.0000e+00\n",
      "Epoch 519/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7362 - accuracy: 0.1667\n",
      "Epoch 00519: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.7362 - accuracy: 0.1667 - val_loss: 6.4313 - val_accuracy: 0.0000e+00\n",
      "Epoch 520/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3011 - accuracy: 0.2667\n",
      "Epoch 00520: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.3011 - accuracy: 0.2667 - val_loss: 5.2314 - val_accuracy: 0.0000e+00\n",
      "Epoch 521/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6504 - accuracy: 0.2667\n",
      "Epoch 00521: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.6504 - accuracy: 0.2667 - val_loss: 3.8700 - val_accuracy: 0.0000e+00\n",
      "Epoch 522/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7073 - accuracy: 0.2667\n",
      "Epoch 00522: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.7073 - accuracy: 0.2667 - val_loss: 4.0766 - val_accuracy: 0.0000e+00\n",
      "Epoch 523/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6879 - accuracy: 0.3667\n",
      "Epoch 00523: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.6879 - accuracy: 0.3667 - val_loss: 1.2533 - val_accuracy: 0.7000\n",
      "Epoch 524/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7767 - accuracy: 0.2333\n",
      "Epoch 00524: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.7767 - accuracy: 0.2333 - val_loss: 1.8689 - val_accuracy: 0.4000\n",
      "Epoch 525/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4596 - accuracy: 0.2333\n",
      "Epoch 00525: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.4596 - accuracy: 0.2333 - val_loss: 1.6317 - val_accuracy: 0.8000\n",
      "Epoch 526/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3867 - accuracy: 0.2333\n",
      "Epoch 00526: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.3867 - accuracy: 0.2333 - val_loss: 3.4655 - val_accuracy: 0.0000e+00\n",
      "Epoch 527/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5134 - accuracy: 0.2333\n",
      "Epoch 00527: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 3.5134 - accuracy: 0.2333 - val_loss: 4.3955 - val_accuracy: 0.1000\n",
      "Epoch 528/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1855 - accuracy: 0.3333\n",
      "Epoch 00528: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 3.1855 - accuracy: 0.3333 - val_loss: 3.7329 - val_accuracy: 0.1000\n",
      "Epoch 529/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8837 - accuracy: 0.1667\n",
      "Epoch 00529: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.8837 - accuracy: 0.1667 - val_loss: 3.4984 - val_accuracy: 0.1000\n",
      "Epoch 530/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9735 - accuracy: 0.1333\n",
      "Epoch 00530: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.9735 - accuracy: 0.1333 - val_loss: 1.5215 - val_accuracy: 0.7000\n",
      "Epoch 531/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4317 - accuracy: 0.2000\n",
      "Epoch 00531: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.4317 - accuracy: 0.2000 - val_loss: 2.6053 - val_accuracy: 0.4000\n",
      "Epoch 532/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9736 - accuracy: 0.3000\n",
      "Epoch 00532: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.9736 - accuracy: 0.3000 - val_loss: 2.2498 - val_accuracy: 0.3000\n",
      "Epoch 533/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0068 - accuracy: 0.1667\n",
      "Epoch 00533: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 4.0068 - accuracy: 0.1667 - val_loss: 1.9863 - val_accuracy: 0.6000\n",
      "Epoch 534/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9311 - accuracy: 0.1000\n",
      "Epoch 00534: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.9311 - accuracy: 0.1000 - val_loss: 3.5499 - val_accuracy: 0.1000\n",
      "Epoch 535/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6632 - accuracy: 0.2667\n",
      "Epoch 00535: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 3.6632 - accuracy: 0.2667 - val_loss: 3.9440 - val_accuracy: 0.4000\n",
      "Epoch 536/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6701 - accuracy: 0.3333\n",
      "Epoch 00536: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 2.6701 - accuracy: 0.3333 - val_loss: 3.1075 - val_accuracy: 0.3000\n",
      "Epoch 537/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7520 - accuracy: 0.1333\n",
      "Epoch 00537: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.7520 - accuracy: 0.1333 - val_loss: 4.9519 - val_accuracy: 0.2000\n",
      "Epoch 538/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2759 - accuracy: 0.1333\n",
      "Epoch 00538: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 4.2759 - accuracy: 0.1333 - val_loss: 3.4882 - val_accuracy: 0.2000\n",
      "Epoch 539/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4994 - accuracy: 0.1333\n",
      "Epoch 00539: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.4994 - accuracy: 0.1333 - val_loss: 3.3856 - val_accuracy: 0.3000\n",
      "Epoch 540/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6759 - accuracy: 0.2000\n",
      "Epoch 00540: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.6759 - accuracy: 0.2000 - val_loss: 3.4626 - val_accuracy: 0.1000\n",
      "Epoch 541/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1714 - accuracy: 0.2667\n",
      "Epoch 00541: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.1714 - accuracy: 0.2667 - val_loss: 2.1466 - val_accuracy: 0.5000\n",
      "Epoch 542/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5345 - accuracy: 0.4333\n",
      "Epoch 00542: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.5345 - accuracy: 0.4333 - val_loss: 1.7816 - val_accuracy: 0.6000\n",
      "Epoch 543/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3831 - accuracy: 0.2333\n",
      "Epoch 00543: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 3.3831 - accuracy: 0.2333 - val_loss: 1.9188 - val_accuracy: 0.5000\n",
      "Epoch 544/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0689 - accuracy: 0.2667\n",
      "Epoch 00544: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.0689 - accuracy: 0.2667 - val_loss: 2.5401 - val_accuracy: 0.3000\n",
      "Epoch 545/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2013 - accuracy: 0.2667\n",
      "Epoch 00545: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.2013 - accuracy: 0.2667 - val_loss: 1.9915 - val_accuracy: 0.3000\n",
      "Epoch 546/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8606 - accuracy: 0.2333\n",
      "Epoch 00546: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 2.8606 - accuracy: 0.2333 - val_loss: 1.4770 - val_accuracy: 0.7000\n",
      "Epoch 547/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3736 - accuracy: 0.1333\n",
      "Epoch 00547: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.3736 - accuracy: 0.1333 - val_loss: 1.7162 - val_accuracy: 0.7000\n",
      "Epoch 548/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6784 - accuracy: 0.2000\n",
      "Epoch 00548: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.6784 - accuracy: 0.2000 - val_loss: 2.8615 - val_accuracy: 0.3000\n",
      "Epoch 549/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5706 - accuracy: 0.2333\n",
      "Epoch 00549: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 3.5706 - accuracy: 0.2333 - val_loss: 2.9215 - val_accuracy: 0.4000\n",
      "Epoch 550/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0116 - accuracy: 0.3000\n",
      "Epoch 00550: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.0116 - accuracy: 0.3000 - val_loss: 2.7510 - val_accuracy: 0.2000\n",
      "Epoch 551/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3320 - accuracy: 0.2000\n",
      "Epoch 00551: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.3320 - accuracy: 0.2000 - val_loss: 3.6032 - val_accuracy: 0.1000\n",
      "Epoch 552/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5740 - accuracy: 0.0741\n",
      "Epoch 00552: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.5740 - accuracy: 0.0741 - val_loss: 2.5240 - val_accuracy: 0.2000\n",
      "Epoch 553/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4842 - accuracy: 0.3000\n",
      "Epoch 00553: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 3.4842 - accuracy: 0.3000 - val_loss: 2.8794 - val_accuracy: 0.4000\n",
      "Epoch 554/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5839 - accuracy: 0.2333\n",
      "Epoch 00554: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.5839 - accuracy: 0.2333 - val_loss: 1.2503 - val_accuracy: 0.8000\n",
      "Epoch 555/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8402 - accuracy: 0.4000\n",
      "Epoch 00555: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.8402 - accuracy: 0.4000 - val_loss: 1.2872 - val_accuracy: 0.7000\n",
      "Epoch 556/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0030 - accuracy: 0.4333\n",
      "Epoch 00556: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.0030 - accuracy: 0.4333 - val_loss: 2.4699 - val_accuracy: 0.6000\n",
      "Epoch 557/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8467 - accuracy: 0.3333\n",
      "Epoch 00557: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 2.8467 - accuracy: 0.3333 - val_loss: 5.0357 - val_accuracy: 0.0000e+00\n",
      "Epoch 558/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5438 - accuracy: 0.2000\n",
      "Epoch 00558: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.5438 - accuracy: 0.2000 - val_loss: 3.8853 - val_accuracy: 0.1000\n",
      "Epoch 559/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4778 - accuracy: 0.2000\n",
      "Epoch 00559: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.4778 - accuracy: 0.2000 - val_loss: 2.4012 - val_accuracy: 0.5000\n",
      "Epoch 560/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1592 - accuracy: 0.2000\n",
      "Epoch 00560: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.1592 - accuracy: 0.2000 - val_loss: 2.9421 - val_accuracy: 0.2000\n",
      "Epoch 561/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3667 - accuracy: 0.3000\n",
      "Epoch 00561: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.3667 - accuracy: 0.3000 - val_loss: 2.7665 - val_accuracy: 0.7000\n",
      "Epoch 562/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0011 - accuracy: 0.3000\n",
      "Epoch 00562: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.0011 - accuracy: 0.3000 - val_loss: 5.4067 - val_accuracy: 0.0000e+00\n",
      "Epoch 563/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0791 - accuracy: 0.3333\n",
      "Epoch 00563: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.0791 - accuracy: 0.3333 - val_loss: 4.3025 - val_accuracy: 0.1000\n",
      "Epoch 564/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9620 - accuracy: 0.1667\n",
      "Epoch 00564: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.9620 - accuracy: 0.1667 - val_loss: 1.5321 - val_accuracy: 0.6000\n",
      "Epoch 565/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0594 - accuracy: 0.2667\n",
      "Epoch 00565: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.0594 - accuracy: 0.2667 - val_loss: 4.1318 - val_accuracy: 0.0000e+00\n",
      "Epoch 566/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2403 - accuracy: 0.1333\n",
      "Epoch 00566: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.2403 - accuracy: 0.1333 - val_loss: 4.5931 - val_accuracy: 0.0000e+00\n",
      "Epoch 567/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9904 - accuracy: 0.2333\n",
      "Epoch 00567: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.9904 - accuracy: 0.2333 - val_loss: 3.2710 - val_accuracy: 0.2000\n",
      "Epoch 568/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8294 - accuracy: 0.3333\n",
      "Epoch 00568: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.8294 - accuracy: 0.3333 - val_loss: 4.3679 - val_accuracy: 0.2000\n",
      "Epoch 569/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7007 - accuracy: 0.1333\n",
      "Epoch 00569: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.7007 - accuracy: 0.1333 - val_loss: 1.5862 - val_accuracy: 0.6000\n",
      "Epoch 570/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6332 - accuracy: 0.2333\n",
      "Epoch 00570: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.6332 - accuracy: 0.2333 - val_loss: 1.9766 - val_accuracy: 0.6000\n",
      "Epoch 571/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6451 - accuracy: 0.3667\n",
      "Epoch 00571: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.6451 - accuracy: 0.3667 - val_loss: 2.7694 - val_accuracy: 0.4000\n",
      "Epoch 572/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0071 - accuracy: 0.2667\n",
      "Epoch 00572: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.0071 - accuracy: 0.2667 - val_loss: 1.7205 - val_accuracy: 0.4000\n",
      "Epoch 573/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0269 - accuracy: 0.2333\n",
      "Epoch 00573: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.0269 - accuracy: 0.2333 - val_loss: 2.6363 - val_accuracy: 0.5000\n",
      "Epoch 574/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3693 - accuracy: 0.3000\n",
      "Epoch 00574: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 2.3693 - accuracy: 0.3000 - val_loss: 4.1715 - val_accuracy: 0.2000\n",
      "Epoch 575/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9744 - accuracy: 0.4000\n",
      "Epoch 00575: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 2.9744 - accuracy: 0.4000 - val_loss: 2.2228 - val_accuracy: 0.5000\n",
      "Epoch 576/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3014 - accuracy: 0.2000\n",
      "Epoch 00576: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.3014 - accuracy: 0.2000 - val_loss: 2.5977 - val_accuracy: 0.3000\n",
      "Epoch 577/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8008 - accuracy: 0.2000\n",
      "Epoch 00577: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.8008 - accuracy: 0.2000 - val_loss: 2.9410 - val_accuracy: 0.5000\n",
      "Epoch 578/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1617 - accuracy: 0.3333\n",
      "Epoch 00578: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.1617 - accuracy: 0.3333 - val_loss: 3.0075 - val_accuracy: 0.3000\n",
      "Epoch 579/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6458 - accuracy: 0.1333\n",
      "Epoch 00579: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.6458 - accuracy: 0.1333 - val_loss: 2.7117 - val_accuracy: 0.1000\n",
      "Epoch 580/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4346 - accuracy: 0.2667\n",
      "Epoch 00580: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.4346 - accuracy: 0.2667 - val_loss: 3.2476 - val_accuracy: 0.2000\n",
      "Epoch 581/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0398 - accuracy: 0.2667\n",
      "Epoch 00581: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.0398 - accuracy: 0.2667 - val_loss: 3.2800 - val_accuracy: 0.1000\n",
      "Epoch 582/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0169 - accuracy: 0.3333\n",
      "Epoch 00582: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.0169 - accuracy: 0.3333 - val_loss: 3.9123 - val_accuracy: 0.1000\n",
      "Epoch 583/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8676 - accuracy: 0.1000\n",
      "Epoch 00583: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.8676 - accuracy: 0.1000 - val_loss: 2.5711 - val_accuracy: 0.5000\n",
      "Epoch 584/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3770 - accuracy: 0.3667\n",
      "Epoch 00584: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.3770 - accuracy: 0.3667 - val_loss: 2.6690 - val_accuracy: 0.3000\n",
      "Epoch 585/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9167 - accuracy: 0.2333\n",
      "Epoch 00585: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.9167 - accuracy: 0.2333 - val_loss: 4.4717 - val_accuracy: 0.1000\n",
      "Epoch 586/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8925 - accuracy: 0.3000\n",
      "Epoch 00586: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.8925 - accuracy: 0.3000 - val_loss: 5.6809 - val_accuracy: 0.1000\n",
      "Epoch 587/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0093 - accuracy: 0.3667\n",
      "Epoch 00587: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 3.0093 - accuracy: 0.3667 - val_loss: 2.7345 - val_accuracy: 0.4000\n",
      "Epoch 588/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0891 - accuracy: 0.2667\n",
      "Epoch 00588: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 212ms/step - loss: 3.0891 - accuracy: 0.2667 - val_loss: 2.8714 - val_accuracy: 0.3000\n",
      "Epoch 589/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8005 - accuracy: 0.3667\n",
      "Epoch 00589: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 163ms/step - loss: 2.8005 - accuracy: 0.3667 - val_loss: 0.8517 - val_accuracy: 0.9000\n",
      "Epoch 590/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6401 - accuracy: 0.2667\n",
      "Epoch 00590: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 240ms/step - loss: 3.6401 - accuracy: 0.2667 - val_loss: 3.4975 - val_accuracy: 0.1000\n",
      "Epoch 591/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1490 - accuracy: 0.3333\n",
      "Epoch 00591: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 157ms/step - loss: 3.1490 - accuracy: 0.3333 - val_loss: 1.8610 - val_accuracy: 0.7000\n",
      "Epoch 592/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5030 - accuracy: 0.1667\n",
      "Epoch 00592: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.5030 - accuracy: 0.1667 - val_loss: 1.3292 - val_accuracy: 0.6000\n",
      "Epoch 593/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5058 - accuracy: 0.4000\n",
      "Epoch 00593: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 167ms/step - loss: 2.5058 - accuracy: 0.4000 - val_loss: 0.0423 - val_accuracy: 1.0000\n",
      "Epoch 594/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9532 - accuracy: 0.3000\n",
      "Epoch 00594: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 2.9532 - accuracy: 0.3000 - val_loss: 2.3872 - val_accuracy: 0.1000\n",
      "Epoch 595/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.4891 - accuracy: 0.0667\n",
      "Epoch 00595: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 4.4891 - accuracy: 0.0667 - val_loss: 2.1471 - val_accuracy: 0.0000e+00\n",
      "Epoch 596/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3704 - accuracy: 0.2000\n",
      "Epoch 00596: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.3704 - accuracy: 0.2000 - val_loss: 2.3118 - val_accuracy: 0.0000e+00\n",
      "Epoch 597/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3111 - accuracy: 0.2667\n",
      "Epoch 00597: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.3111 - accuracy: 0.2667 - val_loss: 4.0944 - val_accuracy: 0.0000e+00\n",
      "Epoch 598/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8821 - accuracy: 0.3667\n",
      "Epoch 00598: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 2.8821 - accuracy: 0.3667 - val_loss: 3.3796 - val_accuracy: 0.3000\n",
      "Epoch 599/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9480 - accuracy: 0.2333\n",
      "Epoch 00599: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 196ms/step - loss: 2.9480 - accuracy: 0.2333 - val_loss: 2.1117 - val_accuracy: 0.5000\n",
      "Epoch 600/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7289 - accuracy: 0.1333\n",
      "Epoch 00600: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 3.7289 - accuracy: 0.1333 - val_loss: 2.5162 - val_accuracy: 0.6000\n",
      "Epoch 601/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4014 - accuracy: 0.2000\n",
      "Epoch 00601: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 3.4014 - accuracy: 0.2000 - val_loss: 2.4310 - val_accuracy: 0.5000\n",
      "Epoch 602/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9850 - accuracy: 0.3000\n",
      "Epoch 00602: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 2.9850 - accuracy: 0.3000 - val_loss: 2.8598 - val_accuracy: 0.2000\n",
      "Epoch 603/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0245 - accuracy: 0.2667\n",
      "Epoch 00603: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.0245 - accuracy: 0.2667 - val_loss: 2.0184 - val_accuracy: 0.2000\n",
      "Epoch 604/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7819 - accuracy: 0.2000\n",
      "Epoch 00604: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 3.7819 - accuracy: 0.2000 - val_loss: 3.4014 - val_accuracy: 0.0000e+00\n",
      "Epoch 605/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2243 - accuracy: 0.3000\n",
      "Epoch 00605: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.2243 - accuracy: 0.3000 - val_loss: 2.9281 - val_accuracy: 0.3000\n",
      "Epoch 606/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2411 - accuracy: 0.2333\n",
      "Epoch 00606: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.2411 - accuracy: 0.2333 - val_loss: 2.3236 - val_accuracy: 0.5000\n",
      "Epoch 607/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7990 - accuracy: 0.2333\n",
      "Epoch 00607: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.7990 - accuracy: 0.2333 - val_loss: 2.3514 - val_accuracy: 0.6000\n",
      "Epoch 608/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1898 - accuracy: 0.2000\n",
      "Epoch 00608: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.1898 - accuracy: 0.2000 - val_loss: 2.4263 - val_accuracy: 0.3000\n",
      "Epoch 609/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2338 - accuracy: 0.2667\n",
      "Epoch 00609: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.2338 - accuracy: 0.2667 - val_loss: 4.2125 - val_accuracy: 0.0000e+00\n",
      "Epoch 610/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0168 - accuracy: 0.3000\n",
      "Epoch 00610: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 3.0168 - accuracy: 0.3000 - val_loss: 8.4194 - val_accuracy: 0.0000e+00\n",
      "Epoch 611/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9942 - accuracy: 0.2667\n",
      "Epoch 00611: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.9942 - accuracy: 0.2667 - val_loss: 4.5226 - val_accuracy: 0.0000e+00\n",
      "Epoch 612/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8398 - accuracy: 0.2000\n",
      "Epoch 00612: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.8398 - accuracy: 0.2000 - val_loss: 3.6034 - val_accuracy: 0.1000\n",
      "Epoch 613/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3518 - accuracy: 0.2667\n",
      "Epoch 00613: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.3518 - accuracy: 0.2667 - val_loss: 4.6065 - val_accuracy: 0.0000e+00\n",
      "Epoch 614/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0138 - accuracy: 0.3333\n",
      "Epoch 00614: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.0138 - accuracy: 0.3333 - val_loss: 2.1512 - val_accuracy: 0.4000\n",
      "Epoch 615/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4576 - accuracy: 0.1667\n",
      "Epoch 00615: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.4576 - accuracy: 0.1667 - val_loss: 1.9842 - val_accuracy: 0.6000\n",
      "Epoch 616/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7307 - accuracy: 0.3000\n",
      "Epoch 00616: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.7307 - accuracy: 0.3000 - val_loss: 1.3294 - val_accuracy: 0.5000\n",
      "Epoch 617/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3112 - accuracy: 0.2667\n",
      "Epoch 00617: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.3112 - accuracy: 0.2667 - val_loss: 2.2536 - val_accuracy: 0.4000\n",
      "Epoch 618/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2652 - accuracy: 0.2000\n",
      "Epoch 00618: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.2652 - accuracy: 0.2000 - val_loss: 0.8839 - val_accuracy: 0.8000\n",
      "Epoch 619/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7534 - accuracy: 0.3000\n",
      "Epoch 00619: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.7534 - accuracy: 0.3000 - val_loss: 4.5504 - val_accuracy: 0.0000e+00\n",
      "Epoch 620/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9119 - accuracy: 0.2333\n",
      "Epoch 00620: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.9119 - accuracy: 0.2333 - val_loss: 2.6426 - val_accuracy: 0.3000\n",
      "Epoch 621/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9534 - accuracy: 0.3000\n",
      "Epoch 00621: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 2.9534 - accuracy: 0.3000 - val_loss: 2.2625 - val_accuracy: 0.0000e+00\n",
      "Epoch 622/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1152 - accuracy: 0.3000\n",
      "Epoch 00622: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.1152 - accuracy: 0.3000 - val_loss: 3.6752 - val_accuracy: 0.1000\n",
      "Epoch 623/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9762 - accuracy: 0.1667\n",
      "Epoch 00623: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.9762 - accuracy: 0.1667 - val_loss: 0.8846 - val_accuracy: 0.8000\n",
      "Epoch 624/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1210 - accuracy: 0.3000\n",
      "Epoch 00624: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.1210 - accuracy: 0.3000 - val_loss: 0.8963 - val_accuracy: 0.8000\n",
      "Epoch 625/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3725 - accuracy: 0.3000\n",
      "Epoch 00625: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.3725 - accuracy: 0.3000 - val_loss: 4.9422 - val_accuracy: 0.0000e+00\n",
      "Epoch 626/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0195 - accuracy: 0.2667\n",
      "Epoch 00626: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.0195 - accuracy: 0.2667 - val_loss: 3.2053 - val_accuracy: 0.3000\n",
      "Epoch 627/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4891 - accuracy: 0.2000\n",
      "Epoch 00627: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.4891 - accuracy: 0.2000 - val_loss: 3.9120 - val_accuracy: 0.0000e+00\n",
      "Epoch 628/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5807 - accuracy: 0.4000\n",
      "Epoch 00628: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 2.5807 - accuracy: 0.4000 - val_loss: 4.0019 - val_accuracy: 0.0000e+00\n",
      "Epoch 629/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2712 - accuracy: 0.2333\n",
      "Epoch 00629: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.2712 - accuracy: 0.2333 - val_loss: 3.7274 - val_accuracy: 0.1000\n",
      "Epoch 630/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0890 - accuracy: 0.2667\n",
      "Epoch 00630: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 3.0890 - accuracy: 0.2667 - val_loss: 1.3259 - val_accuracy: 0.6000\n",
      "Epoch 631/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0438 - accuracy: 0.1667\n",
      "Epoch 00631: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0438 - accuracy: 0.1667 - val_loss: 3.9049 - val_accuracy: 0.3000\n",
      "Epoch 632/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5216 - accuracy: 0.2333\n",
      "Epoch 00632: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.5216 - accuracy: 0.2333 - val_loss: 2.8720 - val_accuracy: 0.1000\n",
      "Epoch 633/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0936 - accuracy: 0.3000\n",
      "Epoch 00633: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.0936 - accuracy: 0.3000 - val_loss: 4.0260 - val_accuracy: 0.6000\n",
      "Epoch 634/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5247 - accuracy: 0.2667\n",
      "Epoch 00634: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.5247 - accuracy: 0.2667 - val_loss: 2.7360 - val_accuracy: 0.5000\n",
      "Epoch 635/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4899 - accuracy: 0.3667\n",
      "Epoch 00635: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.4899 - accuracy: 0.3667 - val_loss: 4.7428 - val_accuracy: 0.2000\n",
      "Epoch 636/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3744 - accuracy: 0.3333\n",
      "Epoch 00636: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.3744 - accuracy: 0.3333 - val_loss: 2.5245 - val_accuracy: 0.3000\n",
      "Epoch 637/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2797 - accuracy: 0.2000    \n",
      "Epoch 00637: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.2797 - accuracy: 0.2000 - val_loss: 4.7818 - val_accuracy: 0.3000\n",
      "Epoch 638/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3065 - accuracy: 0.2667\n",
      "Epoch 00638: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.3065 - accuracy: 0.2667 - val_loss: 2.7714 - val_accuracy: 0.3000\n",
      "Epoch 639/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5242 - accuracy: 0.2667\n",
      "Epoch 00639: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.5242 - accuracy: 0.2667 - val_loss: 4.6994 - val_accuracy: 0.3000\n",
      "Epoch 640/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3880 - accuracy: 0.2333\n",
      "Epoch 00640: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.3880 - accuracy: 0.2333 - val_loss: 1.5943 - val_accuracy: 0.5000\n",
      "Epoch 641/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8091 - accuracy: 0.1667    \n",
      "Epoch 00641: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.8091 - accuracy: 0.1667 - val_loss: 4.2656 - val_accuracy: 0.1000\n",
      "Epoch 642/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1923 - accuracy: 0.2000\n",
      "Epoch 00642: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.1923 - accuracy: 0.2000 - val_loss: 4.6136 - val_accuracy: 0.0000e+00\n",
      "Epoch 643/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9227 - accuracy: 0.3667\n",
      "Epoch 00643: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 2.9227 - accuracy: 0.3667 - val_loss: 3.4600 - val_accuracy: 0.1000\n",
      "Epoch 644/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7591 - accuracy: 0.2000\n",
      "Epoch 00644: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.7591 - accuracy: 0.2000 - val_loss: 3.8243 - val_accuracy: 0.1000\n",
      "Epoch 645/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1117 - accuracy: 0.2000\n",
      "Epoch 00645: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.1117 - accuracy: 0.2000 - val_loss: 6.8693 - val_accuracy: 0.0000e+00\n",
      "Epoch 646/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3474 - accuracy: 0.3000\n",
      "Epoch 00646: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.3474 - accuracy: 0.3000 - val_loss: 1.7008 - val_accuracy: 0.7000\n",
      "Epoch 647/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9211 - accuracy: 0.1333\n",
      "Epoch 00647: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 3.9211 - accuracy: 0.1333 - val_loss: 1.1698 - val_accuracy: 0.6000\n",
      "Epoch 648/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2144 - accuracy: 0.2667\n",
      "Epoch 00648: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 3.2144 - accuracy: 0.2667 - val_loss: 3.6640 - val_accuracy: 0.0000e+00\n",
      "Epoch 649/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4034 - accuracy: 0.2333\n",
      "Epoch 00649: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.4034 - accuracy: 0.2333 - val_loss: 3.0710 - val_accuracy: 0.1000\n",
      "Epoch 650/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0908 - accuracy: 0.3333\n",
      "Epoch 00650: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.0908 - accuracy: 0.3333 - val_loss: 3.7600 - val_accuracy: 0.2000\n",
      "Epoch 651/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8190 - accuracy: 0.2667\n",
      "Epoch 00651: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.8190 - accuracy: 0.2667 - val_loss: 4.0183 - val_accuracy: 0.0000e+00\n",
      "Epoch 652/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1467 - accuracy: 0.4000\n",
      "Epoch 00652: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.1467 - accuracy: 0.4000 - val_loss: 2.0802 - val_accuracy: 0.3000\n",
      "Epoch 653/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2339 - accuracy: 0.2667\n",
      "Epoch 00653: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.2339 - accuracy: 0.2667 - val_loss: 3.5878 - val_accuracy: 0.3000\n",
      "Epoch 654/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0114 - accuracy: 0.3667\n",
      "Epoch 00654: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.0114 - accuracy: 0.3667 - val_loss: 1.4634 - val_accuracy: 0.5000\n",
      "Epoch 655/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3642 - accuracy: 0.3333\n",
      "Epoch 00655: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.3642 - accuracy: 0.3333 - val_loss: 2.8060 - val_accuracy: 0.3000\n",
      "Epoch 656/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2385 - accuracy: 0.2000\n",
      "Epoch 00656: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 3.2385 - accuracy: 0.2000 - val_loss: 1.7321 - val_accuracy: 0.5000\n",
      "Epoch 657/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7602 - accuracy: 0.3667\n",
      "Epoch 00657: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.7602 - accuracy: 0.3667 - val_loss: 3.0499 - val_accuracy: 0.4000\n",
      "Epoch 658/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5619 - accuracy: 0.2000\n",
      "Epoch 00658: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.5619 - accuracy: 0.2000 - val_loss: 2.2316 - val_accuracy: 0.5000\n",
      "Epoch 659/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4452 - accuracy: 0.2000\n",
      "Epoch 00659: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.4452 - accuracy: 0.2000 - val_loss: 1.6246 - val_accuracy: 0.6000\n",
      "Epoch 660/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0262 - accuracy: 0.3667\n",
      "Epoch 00660: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.0262 - accuracy: 0.3667 - val_loss: 1.2315 - val_accuracy: 0.7000\n",
      "Epoch 661/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7303 - accuracy: 0.1667\n",
      "Epoch 00661: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.7303 - accuracy: 0.1667 - val_loss: 2.9739 - val_accuracy: 0.3000\n",
      "Epoch 662/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0048 - accuracy: 0.3000\n",
      "Epoch 00662: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.0048 - accuracy: 0.3000 - val_loss: 3.3771 - val_accuracy: 0.1000\n",
      "Epoch 663/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1851 - accuracy: 0.2000\n",
      "Epoch 00663: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.1851 - accuracy: 0.2000 - val_loss: 1.9192 - val_accuracy: 0.5000\n",
      "Epoch 664/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5584 - accuracy: 0.2667\n",
      "Epoch 00664: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.5584 - accuracy: 0.2667 - val_loss: 2.6126 - val_accuracy: 0.3000\n",
      "Epoch 665/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6072 - accuracy: 0.1667\n",
      "Epoch 00665: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.6072 - accuracy: 0.1667 - val_loss: 2.6565 - val_accuracy: 0.4000\n",
      "Epoch 666/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6636 - accuracy: 0.1333\n",
      "Epoch 00666: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.6636 - accuracy: 0.1333 - val_loss: 2.0961 - val_accuracy: 0.4000\n",
      "Epoch 667/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4046 - accuracy: 0.2667\n",
      "Epoch 00667: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 3.4046 - accuracy: 0.2667 - val_loss: 3.1081 - val_accuracy: 0.2000\n",
      "Epoch 668/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0290 - accuracy: 0.2667\n",
      "Epoch 00668: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.0290 - accuracy: 0.2667 - val_loss: 2.2630 - val_accuracy: 0.4000\n",
      "Epoch 669/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5759 - accuracy: 0.3000\n",
      "Epoch 00669: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.5759 - accuracy: 0.3000 - val_loss: 2.1768 - val_accuracy: 0.6000\n",
      "Epoch 670/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1069 - accuracy: 0.4000\n",
      "Epoch 00670: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 3.1069 - accuracy: 0.4000 - val_loss: 3.9864 - val_accuracy: 0.3000\n",
      "Epoch 671/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9798 - accuracy: 0.3333\n",
      "Epoch 00671: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.9798 - accuracy: 0.3333 - val_loss: 2.1039 - val_accuracy: 0.5000\n",
      "Epoch 672/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3921 - accuracy: 0.5333\n",
      "Epoch 00672: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.3921 - accuracy: 0.5333 - val_loss: 1.3519 - val_accuracy: 0.6000\n",
      "Epoch 673/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4847 - accuracy: 0.2000\n",
      "Epoch 00673: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.4847 - accuracy: 0.2000 - val_loss: 5.0961 - val_accuracy: 0.1000\n",
      "Epoch 674/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9630 - accuracy: 0.1667\n",
      "Epoch 00674: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.9630 - accuracy: 0.1667 - val_loss: 2.9798 - val_accuracy: 0.4000\n",
      "Epoch 675/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2644 - accuracy: 0.3000\n",
      "Epoch 00675: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.2644 - accuracy: 0.3000 - val_loss: 2.9324 - val_accuracy: 0.1000\n",
      "Epoch 676/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6061 - accuracy: 0.2000\n",
      "Epoch 00676: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.6061 - accuracy: 0.2000 - val_loss: 2.5442 - val_accuracy: 0.0000e+00\n",
      "Epoch 677/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8642 - accuracy: 0.2667\n",
      "Epoch 00677: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 2.8642 - accuracy: 0.2667 - val_loss: 2.1098 - val_accuracy: 0.4000\n",
      "Epoch 678/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6412 - accuracy: 0.1000\n",
      "Epoch 00678: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.6412 - accuracy: 0.1000 - val_loss: 2.6015 - val_accuracy: 0.3000\n",
      "Epoch 679/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1742 - accuracy: 0.2000\n",
      "Epoch 00679: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.1742 - accuracy: 0.2000 - val_loss: 4.0901 - val_accuracy: 0.0000e+00\n",
      "Epoch 680/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1175 - accuracy: 0.2667\n",
      "Epoch 00680: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.1175 - accuracy: 0.2667 - val_loss: 3.0855 - val_accuracy: 0.1000\n",
      "Epoch 681/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7402 - accuracy: 0.2000\n",
      "Epoch 00681: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.7402 - accuracy: 0.2000 - val_loss: 2.4215 - val_accuracy: 0.3000\n",
      "Epoch 682/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3993 - accuracy: 0.2333\n",
      "Epoch 00682: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.3993 - accuracy: 0.2333 - val_loss: 5.3824 - val_accuracy: 0.0000e+00\n",
      "Epoch 683/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0060 - accuracy: 0.3333\n",
      "Epoch 00683: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.0060 - accuracy: 0.3333 - val_loss: 3.1408 - val_accuracy: 0.0000e+00\n",
      "Epoch 684/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5843 - accuracy: 0.5000\n",
      "Epoch 00684: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 2.5843 - accuracy: 0.5000 - val_loss: 2.4557 - val_accuracy: 0.0000e+00\n",
      "Epoch 685/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7080 - accuracy: 0.2333\n",
      "Epoch 00685: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.7080 - accuracy: 0.2333 - val_loss: 1.6022 - val_accuracy: 0.8000\n",
      "Epoch 686/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1184 - accuracy: 0.3000\n",
      "Epoch 00686: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.1184 - accuracy: 0.3000 - val_loss: 2.3018 - val_accuracy: 0.4000\n",
      "Epoch 687/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9664 - accuracy: 0.1667\n",
      "Epoch 00687: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 3.9664 - accuracy: 0.1667 - val_loss: 1.5053 - val_accuracy: 0.8000\n",
      "Epoch 688/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7335 - accuracy: 0.1333\n",
      "Epoch 00688: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.7335 - accuracy: 0.1333 - val_loss: 2.1782 - val_accuracy: 0.4000\n",
      "Epoch 689/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3367 - accuracy: 0.2000\n",
      "Epoch 00689: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.3367 - accuracy: 0.2000 - val_loss: 2.4192 - val_accuracy: 0.2000\n",
      "Epoch 690/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1045 - accuracy: 0.2000\n",
      "Epoch 00690: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.1045 - accuracy: 0.2000 - val_loss: 1.4482 - val_accuracy: 0.7000\n",
      "Epoch 691/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2276 - accuracy: 0.3667\n",
      "Epoch 00691: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.2276 - accuracy: 0.3667 - val_loss: 1.6850 - val_accuracy: 0.4000\n",
      "Epoch 692/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9358 - accuracy: 0.2333\n",
      "Epoch 00692: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.9358 - accuracy: 0.2333 - val_loss: 3.5431 - val_accuracy: 0.2000\n",
      "Epoch 693/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3108 - accuracy: 0.2000\n",
      "Epoch 00693: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.3108 - accuracy: 0.2000 - val_loss: 3.1791 - val_accuracy: 0.4000\n",
      "Epoch 694/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3216 - accuracy: 0.3000\n",
      "Epoch 00694: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 3.3216 - accuracy: 0.3000 - val_loss: 2.7013 - val_accuracy: 0.3000\n",
      "Epoch 695/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9098 - accuracy: 0.4333\n",
      "Epoch 00695: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.9098 - accuracy: 0.4333 - val_loss: 3.2836 - val_accuracy: 0.1000\n",
      "Epoch 696/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1517 - accuracy: 0.3333\n",
      "Epoch 00696: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.1517 - accuracy: 0.3333 - val_loss: 1.9530 - val_accuracy: 0.5000\n",
      "Epoch 697/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4602 - accuracy: 0.2667\n",
      "Epoch 00697: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.4602 - accuracy: 0.2667 - val_loss: 5.6640 - val_accuracy: 0.1000\n",
      "Epoch 698/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9372 - accuracy: 0.4667\n",
      "Epoch 00698: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.9372 - accuracy: 0.4667 - val_loss: 4.6407 - val_accuracy: 0.2000\n",
      "Epoch 699/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9994 - accuracy: 0.3333\n",
      "Epoch 00699: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.9994 - accuracy: 0.3333 - val_loss: 3.1986 - val_accuracy: 0.0000e+00\n",
      "Epoch 700/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0319 - accuracy: 0.2333\n",
      "Epoch 00700: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.0319 - accuracy: 0.2333 - val_loss: 2.7375 - val_accuracy: 0.3000\n",
      "Epoch 701/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6672 - accuracy: 0.2333\n",
      "Epoch 00701: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.6672 - accuracy: 0.2333 - val_loss: 3.7762 - val_accuracy: 0.1000\n",
      "Epoch 702/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9392 - accuracy: 0.2333\n",
      "Epoch 00702: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 2.9392 - accuracy: 0.2333 - val_loss: 1.7840 - val_accuracy: 0.5000\n",
      "Epoch 703/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0611 - accuracy: 0.4000\n",
      "Epoch 00703: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.0611 - accuracy: 0.4000 - val_loss: 3.7250 - val_accuracy: 0.1000\n",
      "Epoch 704/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1791 - accuracy: 0.3000\n",
      "Epoch 00704: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.1791 - accuracy: 0.3000 - val_loss: 4.3576 - val_accuracy: 0.1000\n",
      "Epoch 705/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8702 - accuracy: 0.2000\n",
      "Epoch 00705: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.8702 - accuracy: 0.2000 - val_loss: 3.7646 - val_accuracy: 0.0000e+00\n",
      "Epoch 706/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4733 - accuracy: 0.2667\n",
      "Epoch 00706: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.4733 - accuracy: 0.2667 - val_loss: 5.8311 - val_accuracy: 0.0000e+00\n",
      "Epoch 707/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3990 - accuracy: 0.2333\n",
      "Epoch 00707: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.3990 - accuracy: 0.2333 - val_loss: 3.2908 - val_accuracy: 0.1000\n",
      "Epoch 708/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8856 - accuracy: 0.3000\n",
      "Epoch 00708: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.8856 - accuracy: 0.3000 - val_loss: 5.5576 - val_accuracy: 0.0000e+00\n",
      "Epoch 709/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0862 - accuracy: 0.2667\n",
      "Epoch 00709: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 3.0862 - accuracy: 0.2667 - val_loss: 4.9768 - val_accuracy: 0.0000e+00\n",
      "Epoch 710/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5080 - accuracy: 0.3667\n",
      "Epoch 00710: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.5080 - accuracy: 0.3667 - val_loss: 5.8530 - val_accuracy: 0.0000e+00\n",
      "Epoch 711/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1313 - accuracy: 0.1000\n",
      "Epoch 00711: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 4.1313 - accuracy: 0.1000 - val_loss: 3.7264 - val_accuracy: 0.3000\n",
      "Epoch 712/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1801 - accuracy: 0.2000\n",
      "Epoch 00712: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.1801 - accuracy: 0.2000 - val_loss: 3.6037 - val_accuracy: 0.0000e+00\n",
      "Epoch 713/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6455 - accuracy: 0.3333\n",
      "Epoch 00713: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.6455 - accuracy: 0.3333 - val_loss: 3.9126 - val_accuracy: 0.1000\n",
      "Epoch 714/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3544 - accuracy: 0.2000\n",
      "Epoch 00714: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.3544 - accuracy: 0.2000 - val_loss: 4.5120 - val_accuracy: 0.0000e+00\n",
      "Epoch 715/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4701 - accuracy: 0.5000\n",
      "Epoch 00715: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.4701 - accuracy: 0.5000 - val_loss: 4.3011 - val_accuracy: 0.1000\n",
      "Epoch 716/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0756 - accuracy: 0.2333\n",
      "Epoch 00716: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.0756 - accuracy: 0.2333 - val_loss: 3.4188 - val_accuracy: 0.0000e+00\n",
      "Epoch 717/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8808 - accuracy: 0.0333    \n",
      "Epoch 00717: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.8808 - accuracy: 0.0333 - val_loss: 1.4153 - val_accuracy: 0.5000\n",
      "Epoch 718/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0146 - accuracy: 0.3333\n",
      "Epoch 00718: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.0146 - accuracy: 0.3333 - val_loss: 1.4676 - val_accuracy: 0.6000\n",
      "Epoch 719/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8136 - accuracy: 0.4000\n",
      "Epoch 00719: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.8136 - accuracy: 0.4000 - val_loss: 4.0961 - val_accuracy: 0.0000e+00\n",
      "Epoch 720/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1214 - accuracy: 0.2667\n",
      "Epoch 00720: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.1214 - accuracy: 0.2667 - val_loss: 2.8889 - val_accuracy: 0.0000e+00\n",
      "Epoch 721/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9273 - accuracy: 0.0667\n",
      "Epoch 00721: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.9273 - accuracy: 0.0667 - val_loss: 3.9193 - val_accuracy: 0.0000e+00\n",
      "Epoch 722/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7726 - accuracy: 0.2333\n",
      "Epoch 00722: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.7726 - accuracy: 0.2333 - val_loss: 3.8108 - val_accuracy: 0.2000\n",
      "Epoch 723/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6262 - accuracy: 0.2333\n",
      "Epoch 00723: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.6262 - accuracy: 0.2333 - val_loss: 3.8808 - val_accuracy: 0.0000e+00\n",
      "Epoch 724/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0038 - accuracy: 0.3000\n",
      "Epoch 00724: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.0038 - accuracy: 0.3000 - val_loss: 4.7226 - val_accuracy: 0.0000e+00\n",
      "Epoch 725/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8881 - accuracy: 0.3000\n",
      "Epoch 00725: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.8881 - accuracy: 0.3000 - val_loss: 7.0760 - val_accuracy: 0.0000e+00\n",
      "Epoch 726/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4991 - accuracy: 0.2667\n",
      "Epoch 00726: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.4991 - accuracy: 0.2667 - val_loss: 4.9847 - val_accuracy: 0.0000e+00\n",
      "Epoch 727/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0110 - accuracy: 0.2333\n",
      "Epoch 00727: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.0110 - accuracy: 0.2333 - val_loss: 3.8586 - val_accuracy: 0.0000e+00\n",
      "Epoch 728/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3679 - accuracy: 0.4333\n",
      "Epoch 00728: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.3679 - accuracy: 0.4333 - val_loss: 4.7152 - val_accuracy: 0.0000e+00\n",
      "Epoch 729/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8124 - accuracy: 0.2333\n",
      "Epoch 00729: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 3.8124 - accuracy: 0.2333 - val_loss: 3.5829 - val_accuracy: 0.0000e+00\n",
      "Epoch 730/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9520 - accuracy: 0.4000\n",
      "Epoch 00730: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 2.9520 - accuracy: 0.4000 - val_loss: 6.8811 - val_accuracy: 0.0000e+00\n",
      "Epoch 731/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.4101 - accuracy: 0.2667\n",
      "Epoch 00731: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.4469 - accuracy: 0.1667 - val_loss: 5.3744 - val_accuracy: 0.0000e+00\n",
      "Epoch 732/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3284 - accuracy: 0.2333\n",
      "Epoch 00732: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.3284 - accuracy: 0.2333 - val_loss: 1.5124 - val_accuracy: 0.6000\n",
      "Epoch 733/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3407 - accuracy: 0.3667\n",
      "Epoch 00733: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.3407 - accuracy: 0.3667 - val_loss: 2.9812 - val_accuracy: 0.2000\n",
      "Epoch 734/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7072 - accuracy: 0.3000\n",
      "Epoch 00734: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 2.7072 - accuracy: 0.3000 - val_loss: 3.7063 - val_accuracy: 0.3000\n",
      "Epoch 735/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3191 - accuracy: 0.2333\n",
      "Epoch 00735: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.3191 - accuracy: 0.2333 - val_loss: 4.2074 - val_accuracy: 0.0000e+00\n",
      "Epoch 736/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2659 - accuracy: 0.3667\n",
      "Epoch 00736: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.2659 - accuracy: 0.3667 - val_loss: 7.5607 - val_accuracy: 0.0000e+00\n",
      "Epoch 737/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9693 - accuracy: 0.3000\n",
      "Epoch 00737: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.9693 - accuracy: 0.3000 - val_loss: 4.2080 - val_accuracy: 0.2000\n",
      "Epoch 738/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2976 - accuracy: 0.3667\n",
      "Epoch 00738: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.2976 - accuracy: 0.3667 - val_loss: 3.9161 - val_accuracy: 0.1000\n",
      "Epoch 739/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9235 - accuracy: 0.0667\n",
      "Epoch 00739: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.9235 - accuracy: 0.0667 - val_loss: 4.0654 - val_accuracy: 0.2000\n",
      "Epoch 740/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9230 - accuracy: 0.1667\n",
      "Epoch 00740: val_loss did not improve from 0.01684\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.9230 - accuracy: 0.1667 - val_loss: 3.9599 - val_accuracy: 0.2000\n",
      "Epoch 741/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2884 - accuracy: 0.2333\n",
      "Epoch 00741: val_loss improved from 0.01684 to 0.01018, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 3.2884 - accuracy: 0.2333 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Epoch 742/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5010 - accuracy: 0.1333\n",
      "Epoch 00742: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.5010 - accuracy: 0.1333 - val_loss: 4.1864 - val_accuracy: 0.1000\n",
      "Epoch 743/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9916 - accuracy: 0.3333\n",
      "Epoch 00743: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.9916 - accuracy: 0.3333 - val_loss: 2.7953 - val_accuracy: 0.2000\n",
      "Epoch 744/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4138 - accuracy: 0.1333\n",
      "Epoch 00744: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.4138 - accuracy: 0.1333 - val_loss: 1.5705 - val_accuracy: 0.4000\n",
      "Epoch 745/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8184 - accuracy: 0.2333\n",
      "Epoch 00745: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.8184 - accuracy: 0.2333 - val_loss: 2.9437 - val_accuracy: 0.6000\n",
      "Epoch 746/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1410 - accuracy: 0.2667\n",
      "Epoch 00746: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.1410 - accuracy: 0.2667 - val_loss: 6.9232 - val_accuracy: 0.0000e+00\n",
      "Epoch 747/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0728 - accuracy: 0.3000\n",
      "Epoch 00747: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.0728 - accuracy: 0.3000 - val_loss: 5.7038 - val_accuracy: 0.0000e+00\n",
      "Epoch 748/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9316 - accuracy: 0.2000\n",
      "Epoch 00748: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.9316 - accuracy: 0.2000 - val_loss: 3.1150 - val_accuracy: 0.5000\n",
      "Epoch 749/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0482 - accuracy: 0.1667\n",
      "Epoch 00749: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.0482 - accuracy: 0.1667 - val_loss: 3.6746 - val_accuracy: 0.1000\n",
      "Epoch 750/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8055 - accuracy: 0.4000\n",
      "Epoch 00750: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.8055 - accuracy: 0.4000 - val_loss: 2.8937 - val_accuracy: 0.2000\n",
      "Epoch 751/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4701 - accuracy: 0.2000\n",
      "Epoch 00751: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.4701 - accuracy: 0.2000 - val_loss: 2.8897 - val_accuracy: 0.2000\n",
      "Epoch 752/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9384 - accuracy: 0.3333\n",
      "Epoch 00752: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.9384 - accuracy: 0.3333 - val_loss: 3.7014 - val_accuracy: 0.0000e+00\n",
      "Epoch 753/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5821 - accuracy: 0.3333\n",
      "Epoch 00753: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.5821 - accuracy: 0.3333 - val_loss: 4.6411 - val_accuracy: 0.1000\n",
      "Epoch 754/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6621 - accuracy: 0.1667\n",
      "Epoch 00754: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.6621 - accuracy: 0.1667 - val_loss: 2.3941 - val_accuracy: 0.5000\n",
      "Epoch 755/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0073 - accuracy: 0.1667\n",
      "Epoch 00755: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 4.0073 - accuracy: 0.1667 - val_loss: 3.4447 - val_accuracy: 0.2000\n",
      "Epoch 756/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8141 - accuracy: 0.2333\n",
      "Epoch 00756: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.8141 - accuracy: 0.2333 - val_loss: 4.5321 - val_accuracy: 0.2000\n",
      "Epoch 757/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7647 - accuracy: 0.2000\n",
      "Epoch 00757: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.7647 - accuracy: 0.2000 - val_loss: 3.7608 - val_accuracy: 0.1000\n",
      "Epoch 758/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4601 - accuracy: 0.3000\n",
      "Epoch 00758: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.4601 - accuracy: 0.3000 - val_loss: 6.9143 - val_accuracy: 0.0000e+00\n",
      "Epoch 759/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6565 - accuracy: 0.2000\n",
      "Epoch 00759: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.6565 - accuracy: 0.2000 - val_loss: 5.2646 - val_accuracy: 0.1000\n",
      "Epoch 760/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4230 - accuracy: 0.2667\n",
      "Epoch 00760: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.4230 - accuracy: 0.2667 - val_loss: 4.5205 - val_accuracy: 0.0000e+00\n",
      "Epoch 761/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5736 - accuracy: 0.1667\n",
      "Epoch 00761: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.5736 - accuracy: 0.1667 - val_loss: 5.4085 - val_accuracy: 0.0000e+00\n",
      "Epoch 762/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9031 - accuracy: 0.3000\n",
      "Epoch 00762: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 2.9031 - accuracy: 0.3000 - val_loss: 2.3146 - val_accuracy: 0.4000\n",
      "Epoch 763/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1246 - accuracy: 0.3000\n",
      "Epoch 00763: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.1246 - accuracy: 0.3000 - val_loss: 2.1704 - val_accuracy: 0.6000\n",
      "Epoch 764/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3022 - accuracy: 0.2000\n",
      "Epoch 00764: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.3022 - accuracy: 0.2000 - val_loss: 1.6951 - val_accuracy: 0.4000\n",
      "Epoch 765/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8427 - accuracy: 0.4000\n",
      "Epoch 00765: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.8427 - accuracy: 0.4000 - val_loss: 3.2499 - val_accuracy: 0.2000\n",
      "Epoch 766/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0906 - accuracy: 0.3333\n",
      "Epoch 00766: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.0906 - accuracy: 0.3333 - val_loss: 3.6292 - val_accuracy: 0.3000\n",
      "Epoch 767/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6459 - accuracy: 0.3667\n",
      "Epoch 00767: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.6459 - accuracy: 0.3667 - val_loss: 2.1240 - val_accuracy: 0.5000\n",
      "Epoch 768/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0569 - accuracy: 0.2667\n",
      "Epoch 00768: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.0569 - accuracy: 0.2667 - val_loss: 4.6442 - val_accuracy: 0.0000e+00\n",
      "Epoch 769/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3537 - accuracy: 0.2667\n",
      "Epoch 00769: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.3537 - accuracy: 0.2667 - val_loss: 4.4350 - val_accuracy: 0.0000e+00\n",
      "Epoch 770/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5949 - accuracy: 0.2667\n",
      "Epoch 00770: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.5949 - accuracy: 0.2667 - val_loss: 3.5155 - val_accuracy: 0.0000e+00\n",
      "Epoch 771/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2739 - accuracy: 0.1000\n",
      "Epoch 00771: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.2739 - accuracy: 0.1000 - val_loss: 3.3328 - val_accuracy: 0.1000\n",
      "Epoch 772/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5285 - accuracy: 0.1667\n",
      "Epoch 00772: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.5285 - accuracy: 0.1667 - val_loss: 2.4831 - val_accuracy: 0.3000\n",
      "Epoch 773/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8308 - accuracy: 0.2000\n",
      "Epoch 00773: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.8308 - accuracy: 0.2000 - val_loss: 3.4619 - val_accuracy: 0.0000e+00\n",
      "Epoch 774/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0269 - accuracy: 0.2333\n",
      "Epoch 00774: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.0269 - accuracy: 0.2333 - val_loss: 4.1610 - val_accuracy: 0.2000\n",
      "Epoch 775/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7003 - accuracy: 0.1333\n",
      "Epoch 00775: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 3.7003 - accuracy: 0.1333 - val_loss: 7.3240 - val_accuracy: 0.0000e+00\n",
      "Epoch 776/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7454 - accuracy: 0.3333\n",
      "Epoch 00776: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.7454 - accuracy: 0.3333 - val_loss: 5.6010 - val_accuracy: 0.0000e+00\n",
      "Epoch 777/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2757 - accuracy: 0.2000\n",
      "Epoch 00777: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.2757 - accuracy: 0.2000 - val_loss: 5.1382 - val_accuracy: 0.0000e+00\n",
      "Epoch 778/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5548 - accuracy: 0.1333\n",
      "Epoch 00778: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.5548 - accuracy: 0.1333 - val_loss: 2.3442 - val_accuracy: 0.6000\n",
      "Epoch 779/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4523 - accuracy: 0.2333\n",
      "Epoch 00779: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 3.4523 - accuracy: 0.2333 - val_loss: 1.0481 - val_accuracy: 0.8000\n",
      "Epoch 780/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4406 - accuracy: 0.3667\n",
      "Epoch 00780: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.4406 - accuracy: 0.3667 - val_loss: 4.5779 - val_accuracy: 0.0000e+00\n",
      "Epoch 781/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4980 - accuracy: 0.2333\n",
      "Epoch 00781: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.4980 - accuracy: 0.2333 - val_loss: 2.4691 - val_accuracy: 0.6000\n",
      "Epoch 782/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5904 - accuracy: 0.2333\n",
      "Epoch 00782: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.5904 - accuracy: 0.2333 - val_loss: 3.7715 - val_accuracy: 0.4000\n",
      "Epoch 783/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3237 - accuracy: 0.2000\n",
      "Epoch 00783: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.3237 - accuracy: 0.2000 - val_loss: 3.6663 - val_accuracy: 0.0000e+00\n",
      "Epoch 784/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3054 - accuracy: 0.2000\n",
      "Epoch 00784: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.3054 - accuracy: 0.2000 - val_loss: 8.6405 - val_accuracy: 0.0000e+00\n",
      "Epoch 785/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5464 - accuracy: 0.2333\n",
      "Epoch 00785: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.5464 - accuracy: 0.2333 - val_loss: 4.5249 - val_accuracy: 0.0000e+00\n",
      "Epoch 786/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8347 - accuracy: 0.4000\n",
      "Epoch 00786: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 2.8347 - accuracy: 0.4000 - val_loss: 4.7606 - val_accuracy: 0.0000e+00\n",
      "Epoch 787/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3772 - accuracy: 0.3000\n",
      "Epoch 00787: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 174ms/step - loss: 3.3772 - accuracy: 0.3000 - val_loss: 1.6644 - val_accuracy: 0.7000\n",
      "Epoch 788/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7095 - accuracy: 0.2667\n",
      "Epoch 00788: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 3.7095 - accuracy: 0.2667 - val_loss: 3.3980 - val_accuracy: 0.3000\n",
      "Epoch 789/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7523 - accuracy: 0.1667\n",
      "Epoch 00789: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.7523 - accuracy: 0.1667 - val_loss: 2.8501 - val_accuracy: 0.4000\n",
      "Epoch 790/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3032 - accuracy: 0.2000\n",
      "Epoch 00790: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.3032 - accuracy: 0.2000 - val_loss: 2.9279 - val_accuracy: 0.3000\n",
      "Epoch 791/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5156 - accuracy: 0.2667\n",
      "Epoch 00791: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.5156 - accuracy: 0.2667 - val_loss: 2.5990 - val_accuracy: 0.5000\n",
      "Epoch 792/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4955 - accuracy: 0.2000\n",
      "Epoch 00792: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.4955 - accuracy: 0.2000 - val_loss: 2.4732 - val_accuracy: 0.3000\n",
      "Epoch 793/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4456 - accuracy: 0.3667\n",
      "Epoch 00793: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 3.4456 - accuracy: 0.3667 - val_loss: 2.8228 - val_accuracy: 0.2000\n",
      "Epoch 794/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4815 - accuracy: 0.3000\n",
      "Epoch 00794: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 3.4815 - accuracy: 0.3000 - val_loss: 3.7810 - val_accuracy: 0.1000\n",
      "Epoch 795/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5447 - accuracy: 0.1667\n",
      "Epoch 00795: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.5447 - accuracy: 0.1667 - val_loss: 3.4498 - val_accuracy: 0.1000\n",
      "Epoch 796/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0372 - accuracy: 0.2333\n",
      "Epoch 00796: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.0372 - accuracy: 0.2333 - val_loss: 3.8971 - val_accuracy: 0.3000\n",
      "Epoch 797/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2730 - accuracy: 0.3000\n",
      "Epoch 00797: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.2730 - accuracy: 0.3000 - val_loss: 4.6817 - val_accuracy: 0.0000e+00\n",
      "Epoch 798/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9080 - accuracy: 0.2333\n",
      "Epoch 00798: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.9080 - accuracy: 0.2333 - val_loss: 0.3442 - val_accuracy: 0.9000\n",
      "Epoch 799/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1462 - accuracy: 0.3000\n",
      "Epoch 00799: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.1462 - accuracy: 0.3000 - val_loss: 3.1324 - val_accuracy: 0.4000\n",
      "Epoch 800/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5316 - accuracy: 0.3000\n",
      "Epoch 00800: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.5316 - accuracy: 0.3000 - val_loss: 1.5864 - val_accuracy: 0.7000\n",
      "Epoch 801/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1386 - accuracy: 0.2333\n",
      "Epoch 00801: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.1386 - accuracy: 0.2333 - val_loss: 0.8007 - val_accuracy: 0.8000\n",
      "Epoch 802/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8716 - accuracy: 0.3000\n",
      "Epoch 00802: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.8716 - accuracy: 0.3000 - val_loss: 2.0350 - val_accuracy: 0.6000\n",
      "Epoch 803/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5232 - accuracy: 0.4667\n",
      "Epoch 00803: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 2.5232 - accuracy: 0.4667 - val_loss: 5.9428 - val_accuracy: 0.0000e+00\n",
      "Epoch 804/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4250 - accuracy: 0.2333\n",
      "Epoch 00804: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.4250 - accuracy: 0.2333 - val_loss: 2.9340 - val_accuracy: 0.4000\n",
      "Epoch 805/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5173 - accuracy: 0.2333\n",
      "Epoch 00805: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.5173 - accuracy: 0.2333 - val_loss: 2.1275 - val_accuracy: 0.6000\n",
      "Epoch 806/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2646 - accuracy: 0.2667\n",
      "Epoch 00806: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.2646 - accuracy: 0.2667 - val_loss: 2.8274 - val_accuracy: 0.4000\n",
      "Epoch 807/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9754 - accuracy: 0.2667\n",
      "Epoch 00807: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.9754 - accuracy: 0.2667 - val_loss: 5.0920 - val_accuracy: 0.0000e+00\n",
      "Epoch 808/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6986 - accuracy: 0.3333\n",
      "Epoch 00808: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.6986 - accuracy: 0.3333 - val_loss: 3.0245 - val_accuracy: 0.4000\n",
      "Epoch 809/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8755 - accuracy: 0.2333\n",
      "Epoch 00809: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 3.8755 - accuracy: 0.2333 - val_loss: 2.1645 - val_accuracy: 0.3000\n",
      "Epoch 810/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9221 - accuracy: 0.2333\n",
      "Epoch 00810: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 2.9221 - accuracy: 0.2333 - val_loss: 1.3011 - val_accuracy: 0.8000\n",
      "Epoch 811/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3563 - accuracy: 0.2333\n",
      "Epoch 00811: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.3563 - accuracy: 0.2333 - val_loss: 6.3128 - val_accuracy: 0.0000e+00\n",
      "Epoch 812/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3065 - accuracy: 0.3000\n",
      "Epoch 00812: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 2s 868ms/step - loss: 3.3065 - accuracy: 0.3000 - val_loss: 2.6260 - val_accuracy: 0.4000\n",
      "Epoch 813/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1485 - accuracy: 0.3000\n",
      "Epoch 00813: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.1485 - accuracy: 0.3000 - val_loss: 2.5379 - val_accuracy: 0.2000\n",
      "Epoch 814/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3148 - accuracy: 0.2667\n",
      "Epoch 00814: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.3148 - accuracy: 0.2667 - val_loss: 3.6911 - val_accuracy: 0.2000\n",
      "Epoch 815/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0112 - accuracy: 0.2667\n",
      "Epoch 00815: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.0112 - accuracy: 0.2667 - val_loss: 0.6458 - val_accuracy: 0.9000\n",
      "Epoch 816/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3372 - accuracy: 0.1333\n",
      "Epoch 00816: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.3372 - accuracy: 0.1333 - val_loss: 2.4791 - val_accuracy: 0.4000\n",
      "Epoch 817/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7667 - accuracy: 0.1000\n",
      "Epoch 00817: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.7667 - accuracy: 0.1000 - val_loss: 6.3322 - val_accuracy: 0.0000e+00\n",
      "Epoch 818/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.5623 - accuracy: 0.1667\n",
      "Epoch 00818: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 4.5623 - accuracy: 0.1667 - val_loss: 4.0222 - val_accuracy: 0.1000\n",
      "Epoch 819/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4292 - accuracy: 0.3000\n",
      "Epoch 00819: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.4292 - accuracy: 0.3000 - val_loss: 5.4191 - val_accuracy: 0.0000e+00\n",
      "Epoch 820/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0355 - accuracy: 0.2667\n",
      "Epoch 00820: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0355 - accuracy: 0.2667 - val_loss: 2.4245 - val_accuracy: 0.6000\n",
      "Epoch 821/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7502 - accuracy: 0.1000\n",
      "Epoch 00821: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.7502 - accuracy: 0.1000 - val_loss: 2.9251 - val_accuracy: 0.2000\n",
      "Epoch 822/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9138 - accuracy: 0.3667\n",
      "Epoch 00822: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.9138 - accuracy: 0.3667 - val_loss: 3.4455 - val_accuracy: 0.1000\n",
      "Epoch 823/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8997 - accuracy: 0.3667\n",
      "Epoch 00823: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.8997 - accuracy: 0.3667 - val_loss: 2.7427 - val_accuracy: 0.2000\n",
      "Epoch 824/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5540 - accuracy: 0.2000\n",
      "Epoch 00824: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 3.5540 - accuracy: 0.2000 - val_loss: 4.9370 - val_accuracy: 0.0000e+00\n",
      "Epoch 825/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4836 - accuracy: 0.2000\n",
      "Epoch 00825: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.4836 - accuracy: 0.2000 - val_loss: 6.2336 - val_accuracy: 0.0000e+00\n",
      "Epoch 826/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2116 - accuracy: 0.1000\n",
      "Epoch 00826: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 4.2116 - accuracy: 0.1000 - val_loss: 3.8111 - val_accuracy: 0.1000\n",
      "Epoch 827/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4696 - accuracy: 0.4333\n",
      "Epoch 00827: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 2.4696 - accuracy: 0.4333 - val_loss: 3.1492 - val_accuracy: 0.3000\n",
      "Epoch 828/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6158 - accuracy: 0.2333\n",
      "Epoch 00828: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 2.6158 - accuracy: 0.2333 - val_loss: 2.5880 - val_accuracy: 0.4000\n",
      "Epoch 829/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6196 - accuracy: 0.3333\n",
      "Epoch 00829: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 2.6196 - accuracy: 0.3333 - val_loss: 4.1777 - val_accuracy: 0.0000e+00\n",
      "Epoch 830/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4612 - accuracy: 0.2667\n",
      "Epoch 00830: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.4612 - accuracy: 0.2667 - val_loss: 4.0984 - val_accuracy: 0.0000e+00\n",
      "Epoch 831/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5293 - accuracy: 0.4000\n",
      "Epoch 00831: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.5293 - accuracy: 0.4000 - val_loss: 6.4783 - val_accuracy: 0.0000e+00\n",
      "Epoch 832/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2704 - accuracy: 0.2667\n",
      "Epoch 00832: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.2704 - accuracy: 0.2667 - val_loss: 2.9062 - val_accuracy: 0.2000\n",
      "Epoch 833/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1328 - accuracy: 0.3667\n",
      "Epoch 00833: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 3.1328 - accuracy: 0.3667 - val_loss: 3.3376 - val_accuracy: 0.1000\n",
      "Epoch 834/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9998 - accuracy: 0.0667    \n",
      "Epoch 00834: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.9998 - accuracy: 0.0667 - val_loss: 5.5333 - val_accuracy: 0.0000e+00\n",
      "Epoch 835/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6450 - accuracy: 0.2667\n",
      "Epoch 00835: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.6450 - accuracy: 0.2667 - val_loss: 2.8954 - val_accuracy: 0.0000e+00\n",
      "Epoch 836/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5588 - accuracy: 0.1333\n",
      "Epoch 00836: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.5588 - accuracy: 0.1333 - val_loss: 2.4987 - val_accuracy: 0.1000\n",
      "Epoch 837/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8952 - accuracy: 0.1333\n",
      "Epoch 00837: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 3.8952 - accuracy: 0.1333 - val_loss: 3.7145 - val_accuracy: 0.0000e+00\n",
      "Epoch 838/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2094 - accuracy: 0.3333\n",
      "Epoch 00838: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.2094 - accuracy: 0.3333 - val_loss: 5.9645 - val_accuracy: 0.0000e+00\n",
      "Epoch 839/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6905 - accuracy: 0.2333\n",
      "Epoch 00839: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.6905 - accuracy: 0.2333 - val_loss: 4.0417 - val_accuracy: 0.1000\n",
      "Epoch 840/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4950 - accuracy: 0.3333\n",
      "Epoch 00840: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.4950 - accuracy: 0.3333 - val_loss: 3.4254 - val_accuracy: 0.2000\n",
      "Epoch 841/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4411 - accuracy: 0.1667\n",
      "Epoch 00841: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.4411 - accuracy: 0.1667 - val_loss: 1.9268 - val_accuracy: 0.5000\n",
      "Epoch 842/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0798 - accuracy: 0.3000\n",
      "Epoch 00842: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.0798 - accuracy: 0.3000 - val_loss: 1.8770 - val_accuracy: 0.6000\n",
      "Epoch 843/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5303 - accuracy: 0.1667\n",
      "Epoch 00843: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.5303 - accuracy: 0.1667 - val_loss: 2.6953 - val_accuracy: 0.3000\n",
      "Epoch 844/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3718 - accuracy: 0.2000\n",
      "Epoch 00844: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.3718 - accuracy: 0.2000 - val_loss: 4.3361 - val_accuracy: 0.0000e+00\n",
      "Epoch 845/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6484 - accuracy: 0.2333\n",
      "Epoch 00845: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.6484 - accuracy: 0.2333 - val_loss: 2.9824 - val_accuracy: 0.3000\n",
      "Epoch 846/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0164 - accuracy: 0.3333\n",
      "Epoch 00846: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.0164 - accuracy: 0.3333 - val_loss: 2.4354 - val_accuracy: 0.4000\n",
      "Epoch 847/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0945 - accuracy: 0.2667\n",
      "Epoch 00847: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 3.0945 - accuracy: 0.2667 - val_loss: 4.6158 - val_accuracy: 0.0000e+00\n",
      "Epoch 848/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7389 - accuracy: 0.2333\n",
      "Epoch 00848: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.7389 - accuracy: 0.2333 - val_loss: 4.5215 - val_accuracy: 0.1000\n",
      "Epoch 849/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7152 - accuracy: 0.4333\n",
      "Epoch 00849: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.7152 - accuracy: 0.4333 - val_loss: 4.1086 - val_accuracy: 0.4000\n",
      "Epoch 850/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5573 - accuracy: 0.3000\n",
      "Epoch 00850: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.5573 - accuracy: 0.3000 - val_loss: 5.8004 - val_accuracy: 0.0000e+00\n",
      "Epoch 851/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8136 - accuracy: 0.0667    \n",
      "Epoch 00851: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.8136 - accuracy: 0.0667 - val_loss: 5.6034 - val_accuracy: 0.0000e+00\n",
      "Epoch 852/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2718 - accuracy: 0.2667\n",
      "Epoch 00852: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.2718 - accuracy: 0.2667 - val_loss: 5.9183 - val_accuracy: 0.0000e+00\n",
      "Epoch 853/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8075 - accuracy: 0.3333\n",
      "Epoch 00853: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.8075 - accuracy: 0.3333 - val_loss: 2.5310 - val_accuracy: 0.3000\n",
      "Epoch 854/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6829 - accuracy: 0.2000\n",
      "Epoch 00854: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.6829 - accuracy: 0.2000 - val_loss: 3.2720 - val_accuracy: 0.1000\n",
      "Epoch 855/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7256 - accuracy: 0.3333\n",
      "Epoch 00855: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.7256 - accuracy: 0.3333 - val_loss: 0.9552 - val_accuracy: 0.7000\n",
      "Epoch 856/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4879 - accuracy: 0.2333\n",
      "Epoch 00856: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.4879 - accuracy: 0.2333 - val_loss: 1.2488 - val_accuracy: 0.6000\n",
      "Epoch 857/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2279 - accuracy: 0.3667\n",
      "Epoch 00857: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.2279 - accuracy: 0.3667 - val_loss: 0.9578 - val_accuracy: 0.7000\n",
      "Epoch 858/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9660 - accuracy: 0.2333\n",
      "Epoch 00858: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.9660 - accuracy: 0.2333 - val_loss: 4.5697 - val_accuracy: 0.0000e+00\n",
      "Epoch 859/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4478 - accuracy: 0.2000\n",
      "Epoch 00859: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.4478 - accuracy: 0.2000 - val_loss: 4.2832 - val_accuracy: 0.0000e+00\n",
      "Epoch 860/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9008 - accuracy: 0.1667\n",
      "Epoch 00860: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.9008 - accuracy: 0.1667 - val_loss: 4.6995 - val_accuracy: 0.0000e+00\n",
      "Epoch 861/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0619 - accuracy: 0.2333\n",
      "Epoch 00861: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.0619 - accuracy: 0.2333 - val_loss: 3.8802 - val_accuracy: 0.1000\n",
      "Epoch 862/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0736 - accuracy: 0.3333\n",
      "Epoch 00862: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.0736 - accuracy: 0.3333 - val_loss: 2.6044 - val_accuracy: 0.4000\n",
      "Epoch 863/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9999 - accuracy: 0.3333\n",
      "Epoch 00863: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.9999 - accuracy: 0.3333 - val_loss: 3.2539 - val_accuracy: 0.2000\n",
      "Epoch 864/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7491 - accuracy: 0.2667\n",
      "Epoch 00864: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.7491 - accuracy: 0.2667 - val_loss: 2.5371 - val_accuracy: 0.2000\n",
      "Epoch 865/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3402 - accuracy: 0.2667\n",
      "Epoch 00865: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.3402 - accuracy: 0.2667 - val_loss: 3.7616 - val_accuracy: 0.1000\n",
      "Epoch 866/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8365 - accuracy: 0.2333\n",
      "Epoch 00866: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.8365 - accuracy: 0.2333 - val_loss: 3.2978 - val_accuracy: 0.0000e+00\n",
      "Epoch 867/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1616 - accuracy: 0.3667\n",
      "Epoch 00867: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.1616 - accuracy: 0.3667 - val_loss: 2.6966 - val_accuracy: 0.6000\n",
      "Epoch 868/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4151 - accuracy: 0.2000\n",
      "Epoch 00868: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.4151 - accuracy: 0.2000 - val_loss: 2.1390 - val_accuracy: 0.5000\n",
      "Epoch 869/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2417 - accuracy: 0.1667\n",
      "Epoch 00869: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.2417 - accuracy: 0.1667 - val_loss: 5.2617 - val_accuracy: 0.6000\n",
      "Epoch 870/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3237 - accuracy: 0.1333\n",
      "Epoch 00870: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.3237 - accuracy: 0.1333 - val_loss: 2.6187 - val_accuracy: 0.5000\n",
      "Epoch 871/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6091 - accuracy: 0.5333\n",
      "Epoch 00871: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.6091 - accuracy: 0.5333 - val_loss: 4.4775 - val_accuracy: 0.4000\n",
      "Epoch 872/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5298 - accuracy: 0.4000\n",
      "Epoch 00872: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.5298 - accuracy: 0.4000 - val_loss: 5.5909 - val_accuracy: 0.1000\n",
      "Epoch 873/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5156 - accuracy: 0.2000\n",
      "Epoch 00873: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.5156 - accuracy: 0.2000 - val_loss: 0.8490 - val_accuracy: 0.8000\n",
      "Epoch 874/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3227 - accuracy: 0.1852\n",
      "Epoch 00874: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.3227 - accuracy: 0.1852 - val_loss: 2.0874 - val_accuracy: 0.7000\n",
      "Epoch 875/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7641 - accuracy: 0.3333\n",
      "Epoch 00875: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.7641 - accuracy: 0.3333 - val_loss: 1.8771 - val_accuracy: 0.3000\n",
      "Epoch 876/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0178 - accuracy: 0.2000\n",
      "Epoch 00876: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.0178 - accuracy: 0.2000 - val_loss: 2.1679 - val_accuracy: 0.3000\n",
      "Epoch 877/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7924 - accuracy: 0.2667\n",
      "Epoch 00877: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.7924 - accuracy: 0.2667 - val_loss: 1.8387 - val_accuracy: 0.2000\n",
      "Epoch 878/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4100 - accuracy: 0.2333\n",
      "Epoch 00878: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.4100 - accuracy: 0.2333 - val_loss: 1.7282 - val_accuracy: 0.5000\n",
      "Epoch 879/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3565 - accuracy: 0.3000\n",
      "Epoch 00879: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.3565 - accuracy: 0.3000 - val_loss: 1.2938 - val_accuracy: 0.6000\n",
      "Epoch 880/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0172 - accuracy: 0.3000\n",
      "Epoch 00880: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.0172 - accuracy: 0.3000 - val_loss: 2.3901 - val_accuracy: 0.6000\n",
      "Epoch 881/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1728 - accuracy: 0.1667\n",
      "Epoch 00881: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.1728 - accuracy: 0.1667 - val_loss: 2.1993 - val_accuracy: 0.6000\n",
      "Epoch 882/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7312 - accuracy: 0.3000\n",
      "Epoch 00882: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.7312 - accuracy: 0.3000 - val_loss: 2.4011 - val_accuracy: 0.7000\n",
      "Epoch 883/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1520 - accuracy: 0.3667\n",
      "Epoch 00883: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.1520 - accuracy: 0.3667 - val_loss: 2.4949 - val_accuracy: 0.1000\n",
      "Epoch 884/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4418 - accuracy: 0.2333\n",
      "Epoch 00884: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.4418 - accuracy: 0.2333 - val_loss: 1.4133 - val_accuracy: 0.6000\n",
      "Epoch 885/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.1510 - accuracy: 0.4000\n",
      "Epoch 00885: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.1510 - accuracy: 0.4000 - val_loss: 1.5782 - val_accuracy: 0.5000\n",
      "Epoch 886/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8163 - accuracy: 0.4667\n",
      "Epoch 00886: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.8163 - accuracy: 0.4667 - val_loss: 1.8616 - val_accuracy: 0.6000\n",
      "Epoch 887/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7554 - accuracy: 0.1667\n",
      "Epoch 00887: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.7554 - accuracy: 0.1667 - val_loss: 1.8966 - val_accuracy: 0.7000\n",
      "Epoch 888/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7026 - accuracy: 0.1667\n",
      "Epoch 00888: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.7026 - accuracy: 0.1667 - val_loss: 0.7577 - val_accuracy: 0.8000\n",
      "Epoch 889/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3042 - accuracy: 0.4667\n",
      "Epoch 00889: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.3042 - accuracy: 0.4667 - val_loss: 4.7754 - val_accuracy: 0.1000\n",
      "Epoch 890/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3211 - accuracy: 0.2333\n",
      "Epoch 00890: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.3211 - accuracy: 0.2333 - val_loss: 3.4760 - val_accuracy: 0.1000\n",
      "Epoch 891/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8002 - accuracy: 0.2000\n",
      "Epoch 00891: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.8002 - accuracy: 0.2000 - val_loss: 2.2447 - val_accuracy: 0.4000\n",
      "Epoch 892/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0556 - accuracy: 0.3333\n",
      "Epoch 00892: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.0556 - accuracy: 0.3333 - val_loss: 3.7499 - val_accuracy: 0.1000\n",
      "Epoch 893/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1831 - accuracy: 0.2333\n",
      "Epoch 00893: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.1831 - accuracy: 0.2333 - val_loss: 3.5605 - val_accuracy: 0.2000\n",
      "Epoch 894/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1279 - accuracy: 0.3333\n",
      "Epoch 00894: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 3.1279 - accuracy: 0.3333 - val_loss: 4.3062 - val_accuracy: 0.0000e+00\n",
      "Epoch 895/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9373 - accuracy: 0.4333\n",
      "Epoch 00895: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.9373 - accuracy: 0.4333 - val_loss: 3.3231 - val_accuracy: 0.3000\n",
      "Epoch 896/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2869 - accuracy: 0.1667\n",
      "Epoch 00896: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.2869 - accuracy: 0.1667 - val_loss: 2.0058 - val_accuracy: 0.5000\n",
      "Epoch 897/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6255 - accuracy: 0.2667\n",
      "Epoch 00897: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.6255 - accuracy: 0.2667 - val_loss: 4.6183 - val_accuracy: 0.1000\n",
      "Epoch 898/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3513 - accuracy: 0.3000\n",
      "Epoch 00898: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.3513 - accuracy: 0.3000 - val_loss: 4.2592 - val_accuracy: 0.0000e+00\n",
      "Epoch 899/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9789 - accuracy: 0.3333\n",
      "Epoch 00899: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.9789 - accuracy: 0.3333 - val_loss: 3.5749 - val_accuracy: 0.2000\n",
      "Epoch 900/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3705 - accuracy: 0.1667\n",
      "Epoch 00900: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.3705 - accuracy: 0.1667 - val_loss: 4.2413 - val_accuracy: 0.3000\n",
      "Epoch 901/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3153 - accuracy: 0.1333\n",
      "Epoch 00901: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.3153 - accuracy: 0.1333 - val_loss: 2.2319 - val_accuracy: 0.4000\n",
      "Epoch 902/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2246 - accuracy: 0.2667\n",
      "Epoch 00902: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.2246 - accuracy: 0.2667 - val_loss: 2.6150 - val_accuracy: 0.3000\n",
      "Epoch 903/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3544 - accuracy: 0.4000\n",
      "Epoch 00903: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 2.3544 - accuracy: 0.4000 - val_loss: 2.1283 - val_accuracy: 0.5000\n",
      "Epoch 904/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9384 - accuracy: 0.2333\n",
      "Epoch 00904: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.9384 - accuracy: 0.2333 - val_loss: 1.6065 - val_accuracy: 0.4000\n",
      "Epoch 905/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8056 - accuracy: 0.3333\n",
      "Epoch 00905: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.8056 - accuracy: 0.3333 - val_loss: 1.8522 - val_accuracy: 0.6000\n",
      "Epoch 906/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6108 - accuracy: 0.3333\n",
      "Epoch 00906: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.6108 - accuracy: 0.3333 - val_loss: 8.0981 - val_accuracy: 0.0000e+00\n",
      "Epoch 907/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8571 - accuracy: 0.3000\n",
      "Epoch 00907: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.8571 - accuracy: 0.3000 - val_loss: 2.8456 - val_accuracy: 0.3000\n",
      "Epoch 908/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9592 - accuracy: 0.2333\n",
      "Epoch 00908: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.9592 - accuracy: 0.2333 - val_loss: 2.9490 - val_accuracy: 0.4000\n",
      "Epoch 909/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7183 - accuracy: 0.2000\n",
      "Epoch 00909: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.7183 - accuracy: 0.2000 - val_loss: 2.2492 - val_accuracy: 0.6000\n",
      "Epoch 910/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6373 - accuracy: 0.3667\n",
      "Epoch 00910: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.6373 - accuracy: 0.3667 - val_loss: 2.7838 - val_accuracy: 0.4000\n",
      "Epoch 911/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3401 - accuracy: 0.2000\n",
      "Epoch 00911: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.3401 - accuracy: 0.2000 - val_loss: 4.7423 - val_accuracy: 0.0000e+00\n",
      "Epoch 912/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8291 - accuracy: 0.3333\n",
      "Epoch 00912: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.8291 - accuracy: 0.3333 - val_loss: 2.3934 - val_accuracy: 0.3000\n",
      "Epoch 913/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9783 - accuracy: 0.3000\n",
      "Epoch 00913: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.9783 - accuracy: 0.3000 - val_loss: 2.5918 - val_accuracy: 0.0000e+00\n",
      "Epoch 914/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1922 - accuracy: 0.3000\n",
      "Epoch 00914: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.1922 - accuracy: 0.3000 - val_loss: 4.0524 - val_accuracy: 0.0000e+00\n",
      "Epoch 915/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3712 - accuracy: 0.2000\n",
      "Epoch 00915: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.3712 - accuracy: 0.2000 - val_loss: 2.1379 - val_accuracy: 0.6000\n",
      "Epoch 916/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7742 - accuracy: 0.1667\n",
      "Epoch 00916: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.7742 - accuracy: 0.1667 - val_loss: 2.0537 - val_accuracy: 0.5000\n",
      "Epoch 917/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6994 - accuracy: 0.1000    \n",
      "Epoch 00917: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.6994 - accuracy: 0.1000 - val_loss: 3.1983 - val_accuracy: 0.3000\n",
      "Epoch 918/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1661 - accuracy: 0.2333\n",
      "Epoch 00918: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.1661 - accuracy: 0.2333 - val_loss: 5.2686 - val_accuracy: 0.1000\n",
      "Epoch 919/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1061 - accuracy: 0.2667\n",
      "Epoch 00919: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.1061 - accuracy: 0.2667 - val_loss: 2.1878 - val_accuracy: 0.6000\n",
      "Epoch 920/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2413 - accuracy: 0.1667    \n",
      "Epoch 00920: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 143ms/step - loss: 3.2413 - accuracy: 0.1667 - val_loss: 2.9351 - val_accuracy: 0.5000\n",
      "Epoch 921/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3592 - accuracy: 0.1333\n",
      "Epoch 00921: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.3592 - accuracy: 0.1333 - val_loss: 1.9356 - val_accuracy: 0.4000\n",
      "Epoch 922/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9273 - accuracy: 0.1333\n",
      "Epoch 00922: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.9273 - accuracy: 0.1333 - val_loss: 3.4894 - val_accuracy: 0.2000\n",
      "Epoch 923/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0745 - accuracy: 0.2333\n",
      "Epoch 00923: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.0745 - accuracy: 0.2333 - val_loss: 1.9084 - val_accuracy: 0.6000\n",
      "Epoch 924/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3095 - accuracy: 0.4333\n",
      "Epoch 00924: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 2.3095 - accuracy: 0.4333 - val_loss: 1.6526 - val_accuracy: 0.5000\n",
      "Epoch 925/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8298 - accuracy: 0.4000\n",
      "Epoch 00925: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.8298 - accuracy: 0.4000 - val_loss: 0.0564 - val_accuracy: 1.0000\n",
      "Epoch 926/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7195 - accuracy: 0.1667\n",
      "Epoch 00926: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.7195 - accuracy: 0.1667 - val_loss: 2.5802 - val_accuracy: 0.4000\n",
      "Epoch 927/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1635 - accuracy: 0.2333\n",
      "Epoch 00927: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.1635 - accuracy: 0.2333 - val_loss: 1.7151 - val_accuracy: 0.4000\n",
      "Epoch 928/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1884 - accuracy: 0.3000\n",
      "Epoch 00928: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.1884 - accuracy: 0.3000 - val_loss: 2.6922 - val_accuracy: 0.0000e+00\n",
      "Epoch 929/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3328 - accuracy: 0.1667\n",
      "Epoch 00929: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.3328 - accuracy: 0.1667 - val_loss: 4.3756 - val_accuracy: 0.0000e+00\n",
      "Epoch 930/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2015 - accuracy: 0.2000\n",
      "Epoch 00930: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.2015 - accuracy: 0.2000 - val_loss: 3.1256 - val_accuracy: 0.3000\n",
      "Epoch 931/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.2021 - accuracy: 0.3667\n",
      "Epoch 00931: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.2021 - accuracy: 0.3667 - val_loss: 2.4153 - val_accuracy: 0.4000\n",
      "Epoch 932/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1225 - accuracy: 0.2333\n",
      "Epoch 00932: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 4.1225 - accuracy: 0.2333 - val_loss: 2.8303 - val_accuracy: 0.4000\n",
      "Epoch 933/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2099 - accuracy: 0.2333\n",
      "Epoch 00933: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.2099 - accuracy: 0.2333 - val_loss: 2.0147 - val_accuracy: 0.6000\n",
      "Epoch 934/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9248 - accuracy: 0.2667\n",
      "Epoch 00934: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 2.9248 - accuracy: 0.2667 - val_loss: 2.2652 - val_accuracy: 0.3000\n",
      "Epoch 935/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0329 - accuracy: 0.3000\n",
      "Epoch 00935: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.0329 - accuracy: 0.3000 - val_loss: 1.4552 - val_accuracy: 0.5000\n",
      "Epoch 936/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9890 - accuracy: 0.3000\n",
      "Epoch 00936: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.9890 - accuracy: 0.3000 - val_loss: 3.1617 - val_accuracy: 0.0000e+00\n",
      "Epoch 937/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4083 - accuracy: 0.2000\n",
      "Epoch 00937: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.4083 - accuracy: 0.2000 - val_loss: 2.7501 - val_accuracy: 0.1000\n",
      "Epoch 938/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5115 - accuracy: 0.3000\n",
      "Epoch 00938: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.5115 - accuracy: 0.3000 - val_loss: 1.2714 - val_accuracy: 0.7000\n",
      "Epoch 939/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9249 - accuracy: 0.3000\n",
      "Epoch 00939: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.9249 - accuracy: 0.3000 - val_loss: 2.1000 - val_accuracy: 0.6000\n",
      "Epoch 940/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6482 - accuracy: 0.1333\n",
      "Epoch 00940: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.6482 - accuracy: 0.1333 - val_loss: 4.0183 - val_accuracy: 0.0000e+00\n",
      "Epoch 941/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0276 - accuracy: 0.2333\n",
      "Epoch 00941: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.0276 - accuracy: 0.2333 - val_loss: 5.3830 - val_accuracy: 0.0000e+00\n",
      "Epoch 942/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6765 - accuracy: 0.2667\n",
      "Epoch 00942: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.6765 - accuracy: 0.2667 - val_loss: 7.5829 - val_accuracy: 0.0000e+00\n",
      "Epoch 943/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8040 - accuracy: 0.3333\n",
      "Epoch 00943: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.8040 - accuracy: 0.3333 - val_loss: 3.1071 - val_accuracy: 0.2000\n",
      "Epoch 944/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0078 - accuracy: 0.3333\n",
      "Epoch 00944: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.0078 - accuracy: 0.3333 - val_loss: 3.1036 - val_accuracy: 0.1000\n",
      "Epoch 945/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4159 - accuracy: 0.2333\n",
      "Epoch 00945: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 3.4159 - accuracy: 0.2333 - val_loss: 4.9367 - val_accuracy: 0.1000\n",
      "Epoch 946/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4102 - accuracy: 0.2000\n",
      "Epoch 00946: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.4102 - accuracy: 0.2000 - val_loss: 2.0142 - val_accuracy: 0.6000\n",
      "Epoch 947/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6935 - accuracy: 0.1667    \n",
      "Epoch 00947: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.6935 - accuracy: 0.1667 - val_loss: 2.4900 - val_accuracy: 0.5000\n",
      "Epoch 948/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8639 - accuracy: 0.2000\n",
      "Epoch 00948: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.8639 - accuracy: 0.2000 - val_loss: 1.5162 - val_accuracy: 0.5000\n",
      "Epoch 949/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3723 - accuracy: 0.1333\n",
      "Epoch 00949: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.3723 - accuracy: 0.1333 - val_loss: 1.9719 - val_accuracy: 0.6000\n",
      "Epoch 950/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8357 - accuracy: 0.3333\n",
      "Epoch 00950: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 2.8357 - accuracy: 0.3333 - val_loss: 0.7989 - val_accuracy: 0.6000\n",
      "Epoch 951/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1928 - accuracy: 0.2667\n",
      "Epoch 00951: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 3.1928 - accuracy: 0.2667 - val_loss: 4.3813 - val_accuracy: 0.0000e+00\n",
      "Epoch 952/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0232 - accuracy: 0.4000\n",
      "Epoch 00952: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.0232 - accuracy: 0.4000 - val_loss: 1.9414 - val_accuracy: 0.4000\n",
      "Epoch 953/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8312 - accuracy: 0.3667\n",
      "Epoch 00953: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 2.8312 - accuracy: 0.3667 - val_loss: 2.0541 - val_accuracy: 0.5000\n",
      "Epoch 954/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3612 - accuracy: 0.2333\n",
      "Epoch 00954: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.3612 - accuracy: 0.2333 - val_loss: 2.9680 - val_accuracy: 0.4000\n",
      "Epoch 955/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1793 - accuracy: 0.2000\n",
      "Epoch 00955: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.1793 - accuracy: 0.2000 - val_loss: 1.1979 - val_accuracy: 0.9000\n",
      "Epoch 956/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0913 - accuracy: 0.1667\n",
      "Epoch 00956: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 231ms/step - loss: 4.0913 - accuracy: 0.1667 - val_loss: 1.7428 - val_accuracy: 0.7000\n",
      "Epoch 957/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2251 - accuracy: 0.2667\n",
      "Epoch 00957: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 3.2251 - accuracy: 0.2667 - val_loss: 4.8506 - val_accuracy: 0.0000e+00\n",
      "Epoch 958/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3115 - accuracy: 0.3333\n",
      "Epoch 00958: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.3115 - accuracy: 0.3333 - val_loss: 3.2287 - val_accuracy: 0.2000\n",
      "Epoch 959/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1444 - accuracy: 0.3000\n",
      "Epoch 00959: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.1444 - accuracy: 0.3000 - val_loss: 4.1453 - val_accuracy: 0.1000\n",
      "Epoch 960/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9544 - accuracy: 0.3333\n",
      "Epoch 00960: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 2.9544 - accuracy: 0.3333 - val_loss: 3.8689 - val_accuracy: 0.2000\n",
      "Epoch 961/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6913 - accuracy: 0.3667\n",
      "Epoch 00961: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.6913 - accuracy: 0.3667 - val_loss: 4.0506 - val_accuracy: 0.1000\n",
      "Epoch 962/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2154 - accuracy: 0.3000\n",
      "Epoch 00962: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 124ms/step - loss: 3.2154 - accuracy: 0.3000 - val_loss: 1.0640 - val_accuracy: 0.6000\n",
      "Epoch 963/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0614 - accuracy: 0.2667\n",
      "Epoch 00963: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.0614 - accuracy: 0.2667 - val_loss: 3.5488 - val_accuracy: 0.4000\n",
      "Epoch 964/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8482 - accuracy: 0.3333\n",
      "Epoch 00964: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 2.8482 - accuracy: 0.3333 - val_loss: 2.1200 - val_accuracy: 0.5000\n",
      "Epoch 965/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7934 - accuracy: 0.1667\n",
      "Epoch 00965: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.7934 - accuracy: 0.1667 - val_loss: 3.9005 - val_accuracy: 0.6000\n",
      "Epoch 966/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4936 - accuracy: 0.2333\n",
      "Epoch 00966: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.4936 - accuracy: 0.2333 - val_loss: 2.4108 - val_accuracy: 0.5000\n",
      "Epoch 967/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7894 - accuracy: 0.3333\n",
      "Epoch 00967: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.7894 - accuracy: 0.3333 - val_loss: 5.7999 - val_accuracy: 0.2000\n",
      "Epoch 968/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3211 - accuracy: 0.4000\n",
      "Epoch 00968: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.3211 - accuracy: 0.4000 - val_loss: 1.6553 - val_accuracy: 0.6000\n",
      "Epoch 969/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1108 - accuracy: 0.3333\n",
      "Epoch 00969: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.1108 - accuracy: 0.3333 - val_loss: 2.5132 - val_accuracy: 0.4000\n",
      "Epoch 970/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8288 - accuracy: 0.3000\n",
      "Epoch 00970: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 140ms/step - loss: 2.8288 - accuracy: 0.3000 - val_loss: 1.5399 - val_accuracy: 0.6000\n",
      "Epoch 971/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8690 - accuracy: 0.4667\n",
      "Epoch 00971: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.8690 - accuracy: 0.4667 - val_loss: 3.9229 - val_accuracy: 0.4000\n",
      "Epoch 972/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2500 - accuracy: 0.2333\n",
      "Epoch 00972: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 4.2500 - accuracy: 0.2333 - val_loss: 2.0900 - val_accuracy: 0.5000\n",
      "Epoch 973/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8857 - accuracy: 0.3333\n",
      "Epoch 00973: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.8857 - accuracy: 0.3333 - val_loss: 3.7521 - val_accuracy: 0.0000e+00\n",
      "Epoch 974/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5906 - accuracy: 0.1333\n",
      "Epoch 00974: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.5906 - accuracy: 0.1333 - val_loss: 4.1659 - val_accuracy: 0.1000\n",
      "Epoch 975/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9963 - accuracy: 0.3333\n",
      "Epoch 00975: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.9963 - accuracy: 0.3333 - val_loss: 4.0560 - val_accuracy: 0.2000\n",
      "Epoch 976/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9789 - accuracy: 0.3667\n",
      "Epoch 00976: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 2.9789 - accuracy: 0.3667 - val_loss: 5.4480 - val_accuracy: 0.1000\n",
      "Epoch 977/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1068 - accuracy: 0.1667    \n",
      "Epoch 00977: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 4.1068 - accuracy: 0.1667 - val_loss: 5.7515 - val_accuracy: 0.0000e+00\n",
      "Epoch 978/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3993 - accuracy: 0.2333\n",
      "Epoch 00978: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.3993 - accuracy: 0.2333 - val_loss: 1.6346 - val_accuracy: 0.6000\n",
      "Epoch 979/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1673 - accuracy: 0.3000\n",
      "Epoch 00979: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.1673 - accuracy: 0.3000 - val_loss: 1.7231 - val_accuracy: 0.6000\n",
      "Epoch 980/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3056 - accuracy: 0.1667\n",
      "Epoch 00980: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.3056 - accuracy: 0.1667 - val_loss: 3.9522 - val_accuracy: 0.0000e+00\n",
      "Epoch 981/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2147 - accuracy: 0.4333\n",
      "Epoch 00981: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 3.2147 - accuracy: 0.4333 - val_loss: 3.1814 - val_accuracy: 0.1000\n",
      "Epoch 982/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1881 - accuracy: 0.2000\n",
      "Epoch 00982: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 120ms/step - loss: 3.1881 - accuracy: 0.2000 - val_loss: 3.6311 - val_accuracy: 0.2000\n",
      "Epoch 983/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4936 - accuracy: 0.2333\n",
      "Epoch 00983: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 3.4936 - accuracy: 0.2333 - val_loss: 3.2054 - val_accuracy: 0.1000\n",
      "Epoch 984/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2201 - accuracy: 0.3000\n",
      "Epoch 00984: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 3.2201 - accuracy: 0.3000 - val_loss: 2.3942 - val_accuracy: 0.5000\n",
      "Epoch 985/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7272 - accuracy: 0.3667\n",
      "Epoch 00985: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 2.7272 - accuracy: 0.3667 - val_loss: 3.9240 - val_accuracy: 0.4000\n",
      "Epoch 986/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6090 - accuracy: 0.2667\n",
      "Epoch 00986: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 3.6090 - accuracy: 0.2667 - val_loss: 1.0524 - val_accuracy: 0.8000\n",
      "Epoch 987/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1106 - accuracy: 0.3000\n",
      "Epoch 00987: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 3.1106 - accuracy: 0.3000 - val_loss: 3.2496 - val_accuracy: 0.2000\n",
      "Epoch 988/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6748 - accuracy: 0.2333\n",
      "Epoch 00988: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 137ms/step - loss: 3.6748 - accuracy: 0.2333 - val_loss: 2.3375 - val_accuracy: 0.4000\n",
      "Epoch 989/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1035 - accuracy: 0.3667\n",
      "Epoch 00989: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.1035 - accuracy: 0.3667 - val_loss: 2.7418 - val_accuracy: 0.4000\n",
      "Epoch 990/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8910 - accuracy: 0.3333\n",
      "Epoch 00990: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.8910 - accuracy: 0.3333 - val_loss: 1.9083 - val_accuracy: 0.4000\n",
      "Epoch 991/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8203 - accuracy: 0.3667\n",
      "Epoch 00991: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.8203 - accuracy: 0.3667 - val_loss: 1.9843 - val_accuracy: 0.5000\n",
      "Epoch 992/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9841 - accuracy: 0.2333\n",
      "Epoch 00992: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.9841 - accuracy: 0.2333 - val_loss: 1.7409 - val_accuracy: 0.7000\n",
      "Epoch 993/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5691 - accuracy: 0.2333\n",
      "Epoch 00993: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 2.5691 - accuracy: 0.2333 - val_loss: 2.7068 - val_accuracy: 0.3000\n",
      "Epoch 994/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6161 - accuracy: 0.2000\n",
      "Epoch 00994: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.6161 - accuracy: 0.2000 - val_loss: 3.9120 - val_accuracy: 0.0000e+00\n",
      "Epoch 995/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8801 - accuracy: 0.2667\n",
      "Epoch 00995: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.8801 - accuracy: 0.2667 - val_loss: 2.4292 - val_accuracy: 0.5000\n",
      "Epoch 996/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2884 - accuracy: 0.2333\n",
      "Epoch 00996: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.2884 - accuracy: 0.2333 - val_loss: 3.2914 - val_accuracy: 0.2000\n",
      "Epoch 997/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7001 - accuracy: 0.1333\n",
      "Epoch 00997: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.7001 - accuracy: 0.1333 - val_loss: 3.9969 - val_accuracy: 0.2000\n",
      "Epoch 998/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6361 - accuracy: 0.3000\n",
      "Epoch 00998: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 3.6361 - accuracy: 0.3000 - val_loss: 2.0187 - val_accuracy: 0.6000\n",
      "Epoch 999/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9310 - accuracy: 0.4000\n",
      "Epoch 00999: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 2.9310 - accuracy: 0.4000 - val_loss: 2.2560 - val_accuracy: 0.5000\n",
      "Epoch 1000/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5749 - accuracy: 0.2000\n",
      "Epoch 01000: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.5749 - accuracy: 0.2000 - val_loss: 2.0898 - val_accuracy: 0.7000\n",
      "Epoch 1001/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0477 - accuracy: 0.3667\n",
      "Epoch 01001: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.0477 - accuracy: 0.3667 - val_loss: 1.6620 - val_accuracy: 0.6000\n",
      "Epoch 1002/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1435 - accuracy: 0.3667\n",
      "Epoch 01002: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.1435 - accuracy: 0.3667 - val_loss: 2.4389 - val_accuracy: 0.5000\n",
      "Epoch 1003/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7874 - accuracy: 0.1333    \n",
      "Epoch 01003: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.7874 - accuracy: 0.1333 - val_loss: 2.4951 - val_accuracy: 0.4000\n",
      "Epoch 1004/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2732 - accuracy: 0.2000\n",
      "Epoch 01004: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.2732 - accuracy: 0.2000 - val_loss: 1.4515 - val_accuracy: 0.5000\n",
      "Epoch 1005/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0739 - accuracy: 0.2333\n",
      "Epoch 01005: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 3.0739 - accuracy: 0.2333 - val_loss: 5.1553 - val_accuracy: 0.1000\n",
      "Epoch 1006/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.3675 - accuracy: 0.1667\n",
      "Epoch 01006: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 4.3675 - accuracy: 0.1667 - val_loss: 5.0172 - val_accuracy: 0.0000e+00\n",
      "Epoch 1007/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4464 - accuracy: 0.2333\n",
      "Epoch 01007: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.4464 - accuracy: 0.2333 - val_loss: 2.2477 - val_accuracy: 0.5000\n",
      "Epoch 1008/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3893 - accuracy: 0.2667\n",
      "Epoch 01008: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.3893 - accuracy: 0.2667 - val_loss: 2.5627 - val_accuracy: 0.2000\n",
      "Epoch 1009/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3480 - accuracy: 0.4333\n",
      "Epoch 01009: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 2.3480 - accuracy: 0.4333 - val_loss: 2.5749 - val_accuracy: 0.3000\n",
      "Epoch 1010/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0873 - accuracy: 0.1667\n",
      "Epoch 01010: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 4.0873 - accuracy: 0.1667 - val_loss: 2.9011 - val_accuracy: 0.2000\n",
      "Epoch 1011/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9252 - accuracy: 0.3333\n",
      "Epoch 01011: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 2.9252 - accuracy: 0.3333 - val_loss: 4.3214 - val_accuracy: 0.0000e+00\n",
      "Epoch 1012/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5997 - accuracy: 0.1333\n",
      "Epoch 01012: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.5997 - accuracy: 0.1333 - val_loss: 2.7315 - val_accuracy: 0.2000\n",
      "Epoch 1013/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4700 - accuracy: 0.4000\n",
      "Epoch 01013: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.4700 - accuracy: 0.4000 - val_loss: 2.2513 - val_accuracy: 0.5000\n",
      "Epoch 1014/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8569 - accuracy: 0.3333\n",
      "Epoch 01014: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 2.8569 - accuracy: 0.3333 - val_loss: 4.3562 - val_accuracy: 0.0000e+00\n",
      "Epoch 1015/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5110 - accuracy: 0.2000\n",
      "Epoch 01015: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.5110 - accuracy: 0.2000 - val_loss: 1.6934 - val_accuracy: 0.4000\n",
      "Epoch 1016/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4720 - accuracy: 0.4333\n",
      "Epoch 01016: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 165ms/step - loss: 2.4720 - accuracy: 0.4333 - val_loss: 1.8420 - val_accuracy: 0.0000e+00\n",
      "Epoch 1017/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0433 - accuracy: 0.3000\n",
      "Epoch 01017: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.0433 - accuracy: 0.3000 - val_loss: 2.2823 - val_accuracy: 0.6000\n",
      "Epoch 1018/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4668 - accuracy: 0.3000\n",
      "Epoch 01018: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 3.4668 - accuracy: 0.3000 - val_loss: 1.2978 - val_accuracy: 0.6000\n",
      "Epoch 1019/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1433 - accuracy: 0.3000\n",
      "Epoch 01019: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.1433 - accuracy: 0.3000 - val_loss: 1.6739 - val_accuracy: 0.7000\n",
      "Epoch 1020/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3083 - accuracy: 0.3000\n",
      "Epoch 01020: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.3083 - accuracy: 0.3000 - val_loss: 2.4318 - val_accuracy: 0.3000\n",
      "Epoch 1021/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8828 - accuracy: 0.2333\n",
      "Epoch 01021: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 2.8828 - accuracy: 0.2333 - val_loss: 2.0471 - val_accuracy: 0.5000\n",
      "Epoch 1022/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6502 - accuracy: 0.2667\n",
      "Epoch 01022: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 2.6502 - accuracy: 0.2667 - val_loss: 2.1222 - val_accuracy: 0.5000\n",
      "Epoch 1023/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6018 - accuracy: 0.4333\n",
      "Epoch 01023: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 2.6018 - accuracy: 0.4333 - val_loss: 2.1920 - val_accuracy: 0.3000\n",
      "Epoch 1024/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4920 - accuracy: 0.2667\n",
      "Epoch 01024: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.4920 - accuracy: 0.2667 - val_loss: 3.1832 - val_accuracy: 0.4000\n",
      "Epoch 1025/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2701 - accuracy: 0.2667\n",
      "Epoch 01025: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.2701 - accuracy: 0.2667 - val_loss: 2.6141 - val_accuracy: 0.2000\n",
      "Epoch 1026/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7830 - accuracy: 0.2667\n",
      "Epoch 01026: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 2.7830 - accuracy: 0.2667 - val_loss: 2.4996 - val_accuracy: 0.4000\n",
      "Epoch 1027/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3377 - accuracy: 0.2000\n",
      "Epoch 01027: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.3377 - accuracy: 0.2000 - val_loss: 2.7129 - val_accuracy: 0.3000\n",
      "Epoch 1028/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3293 - accuracy: 0.3000\n",
      "Epoch 01028: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 3.3293 - accuracy: 0.3000 - val_loss: 2.4623 - val_accuracy: 0.5000\n",
      "Epoch 1029/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2596 - accuracy: 0.2333\n",
      "Epoch 01029: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 4.2596 - accuracy: 0.2333 - val_loss: 4.9100 - val_accuracy: 0.1000\n",
      "Epoch 1030/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5287 - accuracy: 0.1667\n",
      "Epoch 01030: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.5287 - accuracy: 0.1667 - val_loss: 4.0388 - val_accuracy: 0.1000\n",
      "Epoch 1031/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1797 - accuracy: 0.3333\n",
      "Epoch 01031: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.1797 - accuracy: 0.3333 - val_loss: 3.4400 - val_accuracy: 0.0000e+00\n",
      "Epoch 1032/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3129 - accuracy: 0.3333\n",
      "Epoch 01032: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.3129 - accuracy: 0.3333 - val_loss: 3.1226 - val_accuracy: 0.2000\n",
      "Epoch 1033/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7164 - accuracy: 0.3000\n",
      "Epoch 01033: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.7164 - accuracy: 0.3000 - val_loss: 3.3505 - val_accuracy: 0.1000\n",
      "Epoch 1034/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7414 - accuracy: 0.3333\n",
      "Epoch 01034: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 2.7414 - accuracy: 0.3333 - val_loss: 2.1998 - val_accuracy: 0.6000\n",
      "Epoch 1035/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6233 - accuracy: 0.2667\n",
      "Epoch 01035: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.6233 - accuracy: 0.2667 - val_loss: 4.6192 - val_accuracy: 0.0000e+00\n",
      "Epoch 1036/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7644 - accuracy: 0.1667\n",
      "Epoch 01036: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.7644 - accuracy: 0.1667 - val_loss: 4.1009 - val_accuracy: 0.2000\n",
      "Epoch 1037/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8023 - accuracy: 0.2667\n",
      "Epoch 01037: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.8023 - accuracy: 0.2667 - val_loss: 4.1118 - val_accuracy: 0.0000e+00\n",
      "Epoch 1038/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4228 - accuracy: 0.2667\n",
      "Epoch 01038: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.4228 - accuracy: 0.2667 - val_loss: 5.6291 - val_accuracy: 0.0000e+00\n",
      "Epoch 1039/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5510 - accuracy: 0.2000\n",
      "Epoch 01039: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.5510 - accuracy: 0.2000 - val_loss: 3.4718 - val_accuracy: 0.2000\n",
      "Epoch 1040/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.0201 - accuracy: 0.2667\n",
      "Epoch 01040: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.4921 - accuracy: 0.1667 - val_loss: 6.1642 - val_accuracy: 0.0000e+00\n",
      "Epoch 1041/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4490 - accuracy: 0.4333\n",
      "Epoch 01041: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.4490 - accuracy: 0.4333 - val_loss: 4.3429 - val_accuracy: 0.2000\n",
      "Epoch 1042/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2615 - accuracy: 0.2000\n",
      "Epoch 01042: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.2615 - accuracy: 0.2000 - val_loss: 6.3054 - val_accuracy: 0.1000\n",
      "Epoch 1043/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3131 - accuracy: 0.3333\n",
      "Epoch 01043: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 3.3131 - accuracy: 0.3333 - val_loss: 3.3714 - val_accuracy: 0.4000\n",
      "Epoch 1044/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.2034 - accuracy: 0.2000\n",
      "Epoch 01044: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 4.2034 - accuracy: 0.2000 - val_loss: 3.8662 - val_accuracy: 0.3000\n",
      "Epoch 1045/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4356 - accuracy: 0.1333\n",
      "Epoch 01045: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.4356 - accuracy: 0.1333 - val_loss: 4.0496 - val_accuracy: 0.1000\n",
      "Epoch 1046/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4182 - accuracy: 0.2000\n",
      "Epoch 01046: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.4182 - accuracy: 0.2000 - val_loss: 4.7286 - val_accuracy: 0.1000\n",
      "Epoch 1047/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1478 - accuracy: 0.2000\n",
      "Epoch 01047: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.1478 - accuracy: 0.2000 - val_loss: 3.5437 - val_accuracy: 0.2000\n",
      "Epoch 1048/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4522 - accuracy: 0.4667\n",
      "Epoch 01048: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 2.4522 - accuracy: 0.4667 - val_loss: 3.5691 - val_accuracy: 0.3000\n",
      "Epoch 1049/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5113 - accuracy: 0.4000\n",
      "Epoch 01049: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 2.5113 - accuracy: 0.4000 - val_loss: 1.6758 - val_accuracy: 0.6000\n",
      "Epoch 1050/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4214 - accuracy: 0.2333\n",
      "Epoch 01050: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 131ms/step - loss: 3.4214 - accuracy: 0.2333 - val_loss: 1.5378 - val_accuracy: 0.8000\n",
      "Epoch 1051/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4696 - accuracy: 0.2000\n",
      "Epoch 01051: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.4696 - accuracy: 0.2000 - val_loss: 3.8051 - val_accuracy: 0.0000e+00\n",
      "Epoch 1052/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4748 - accuracy: 0.2333\n",
      "Epoch 01052: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.4748 - accuracy: 0.2333 - val_loss: 2.5324 - val_accuracy: 0.1000\n",
      "Epoch 1053/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8361 - accuracy: 0.3667\n",
      "Epoch 01053: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.8361 - accuracy: 0.3667 - val_loss: 4.9562 - val_accuracy: 0.0000e+00\n",
      "Epoch 1054/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6084 - accuracy: 0.1667\n",
      "Epoch 01054: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.6084 - accuracy: 0.1667 - val_loss: 4.8075 - val_accuracy: 0.0000e+00\n",
      "Epoch 1055/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8270 - accuracy: 0.2333\n",
      "Epoch 01055: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.8270 - accuracy: 0.2333 - val_loss: 4.7013 - val_accuracy: 0.1000\n",
      "Epoch 1056/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4899 - accuracy: 0.1667\n",
      "Epoch 01056: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.4899 - accuracy: 0.1667 - val_loss: 5.7265 - val_accuracy: 0.0000e+00\n",
      "Epoch 1057/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8873 - accuracy: 0.2333\n",
      "Epoch 01057: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.8873 - accuracy: 0.2333 - val_loss: 6.0116 - val_accuracy: 0.0000e+00\n",
      "Epoch 1058/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2676 - accuracy: 0.2333\n",
      "Epoch 01058: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.2676 - accuracy: 0.2333 - val_loss: 5.2063 - val_accuracy: 0.1000\n",
      "Epoch 1059/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5716 - accuracy: 0.2333\n",
      "Epoch 01059: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.5716 - accuracy: 0.2333 - val_loss: 4.0138 - val_accuracy: 0.0000e+00\n",
      "Epoch 1060/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6128 - accuracy: 0.1667\n",
      "Epoch 01060: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.6128 - accuracy: 0.1667 - val_loss: 4.1494 - val_accuracy: 0.2000\n",
      "Epoch 1061/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1347 - accuracy: 0.3667\n",
      "Epoch 01061: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.1347 - accuracy: 0.3667 - val_loss: 4.9187 - val_accuracy: 0.1000\n",
      "Epoch 1062/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8823 - accuracy: 0.3000\n",
      "Epoch 01062: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.8823 - accuracy: 0.3000 - val_loss: 5.8115 - val_accuracy: 0.0000e+00\n",
      "Epoch 1063/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2936 - accuracy: 0.3667\n",
      "Epoch 01063: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 3.2936 - accuracy: 0.3667 - val_loss: 4.7410 - val_accuracy: 0.2000\n",
      "Epoch 1064/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9508 - accuracy: 0.2667\n",
      "Epoch 01064: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.9508 - accuracy: 0.2667 - val_loss: 2.2766 - val_accuracy: 0.4000\n",
      "Epoch 1065/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5880 - accuracy: 0.1667\n",
      "Epoch 01065: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.5880 - accuracy: 0.1667 - val_loss: 2.6168 - val_accuracy: 0.3000\n",
      "Epoch 1066/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1924 - accuracy: 0.4667\n",
      "Epoch 01066: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.1924 - accuracy: 0.4667 - val_loss: 3.0516 - val_accuracy: 0.4000\n",
      "Epoch 1067/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3860 - accuracy: 0.2000\n",
      "Epoch 01067: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.3860 - accuracy: 0.2000 - val_loss: 3.4272 - val_accuracy: 0.2000\n",
      "Epoch 1068/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5376 - accuracy: 0.3000\n",
      "Epoch 01068: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.5376 - accuracy: 0.3000 - val_loss: 6.9081 - val_accuracy: 0.0000e+00\n",
      "Epoch 1069/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5523 - accuracy: 0.2667\n",
      "Epoch 01069: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.5523 - accuracy: 0.2667 - val_loss: 3.9462 - val_accuracy: 0.1000\n",
      "Epoch 1070/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8191 - accuracy: 0.4000\n",
      "Epoch 01070: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 2.8191 - accuracy: 0.4000 - val_loss: 3.5003 - val_accuracy: 0.4000\n",
      "Epoch 1071/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3817 - accuracy: 0.1333\n",
      "Epoch 01071: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.3817 - accuracy: 0.1333 - val_loss: 3.7005 - val_accuracy: 0.2000\n",
      "Epoch 1072/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4538 - accuracy: 0.2333\n",
      "Epoch 01072: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 3.4538 - accuracy: 0.2333 - val_loss: 2.8154 - val_accuracy: 0.1000\n",
      "Epoch 1073/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3400 - accuracy: 0.3000\n",
      "Epoch 01073: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.3400 - accuracy: 0.3000 - val_loss: 0.1678 - val_accuracy: 0.9000\n",
      "Epoch 1074/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5117 - accuracy: 0.3333\n",
      "Epoch 01074: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 2.5117 - accuracy: 0.3333 - val_loss: 3.1129 - val_accuracy: 0.1000\n",
      "Epoch 1075/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5152 - accuracy: 0.2000\n",
      "Epoch 01075: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 3.5152 - accuracy: 0.2000 - val_loss: 1.6536 - val_accuracy: 0.4000\n",
      "Epoch 1076/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9975 - accuracy: 0.2333\n",
      "Epoch 01076: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 2.9975 - accuracy: 0.2333 - val_loss: 1.9030 - val_accuracy: 0.5000\n",
      "Epoch 1077/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5736 - accuracy: 0.3333\n",
      "Epoch 01077: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.5736 - accuracy: 0.3333 - val_loss: 2.9145 - val_accuracy: 0.5000\n",
      "Epoch 1078/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7229 - accuracy: 0.3000\n",
      "Epoch 01078: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.7229 - accuracy: 0.3000 - val_loss: 5.0722 - val_accuracy: 0.0000e+00\n",
      "Epoch 1079/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2722 - accuracy: 0.2000\n",
      "Epoch 01079: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.2722 - accuracy: 0.2000 - val_loss: 5.0419 - val_accuracy: 0.0000e+00\n",
      "Epoch 1080/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0553 - accuracy: 0.3333\n",
      "Epoch 01080: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.0553 - accuracy: 0.3333 - val_loss: 2.0866 - val_accuracy: 0.8000\n",
      "Epoch 1081/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1025 - accuracy: 0.3333\n",
      "Epoch 01081: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.1025 - accuracy: 0.3333 - val_loss: 3.1614 - val_accuracy: 0.3000\n",
      "Epoch 1082/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4742 - accuracy: 0.2000\n",
      "Epoch 01082: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 156ms/step - loss: 3.4742 - accuracy: 0.2000 - val_loss: 2.7269 - val_accuracy: 0.4000\n",
      "Epoch 1083/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1797 - accuracy: 0.3000\n",
      "Epoch 01083: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.1797 - accuracy: 0.3000 - val_loss: 3.2239 - val_accuracy: 0.1000\n",
      "Epoch 1084/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7268 - accuracy: 0.3667\n",
      "Epoch 01084: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.7268 - accuracy: 0.3667 - val_loss: 4.1650 - val_accuracy: 0.0000e+00\n",
      "Epoch 1085/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1999 - accuracy: 0.2333\n",
      "Epoch 01085: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.1999 - accuracy: 0.2333 - val_loss: 5.1828 - val_accuracy: 0.0000e+00\n",
      "Epoch 1086/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0666 - accuracy: 0.3333\n",
      "Epoch 01086: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.0666 - accuracy: 0.3333 - val_loss: 3.2441 - val_accuracy: 0.1000\n",
      "Epoch 1087/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7225 - accuracy: 0.3000\n",
      "Epoch 01087: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.7225 - accuracy: 0.3000 - val_loss: 3.6005 - val_accuracy: 0.3000\n",
      "Epoch 1088/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1220 - accuracy: 0.2667\n",
      "Epoch 01088: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.1220 - accuracy: 0.2667 - val_loss: 2.4783 - val_accuracy: 0.5000\n",
      "Epoch 1089/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1386 - accuracy: 0.2667\n",
      "Epoch 01089: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.1386 - accuracy: 0.2667 - val_loss: 3.5391 - val_accuracy: 0.1000\n",
      "Epoch 1090/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4981 - accuracy: 0.2667\n",
      "Epoch 01090: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.4981 - accuracy: 0.2667 - val_loss: 7.5571 - val_accuracy: 0.0000e+00\n",
      "Epoch 1091/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7525 - accuracy: 0.4333\n",
      "Epoch 01091: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.7525 - accuracy: 0.4333 - val_loss: 5.3491 - val_accuracy: 0.1000\n",
      "Epoch 1092/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3427 - accuracy: 0.1667\n",
      "Epoch 01092: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.3427 - accuracy: 0.1667 - val_loss: 4.1503 - val_accuracy: 0.1000\n",
      "Epoch 1093/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0572 - accuracy: 0.3000\n",
      "Epoch 01093: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.0572 - accuracy: 0.3000 - val_loss: 6.7922 - val_accuracy: 0.0000e+00\n",
      "Epoch 1094/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1712 - accuracy: 0.2667\n",
      "Epoch 01094: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 3.1712 - accuracy: 0.2667 - val_loss: 2.5170 - val_accuracy: 0.2000\n",
      "Epoch 1095/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5320 - accuracy: 0.4000\n",
      "Epoch 01095: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.5320 - accuracy: 0.4000 - val_loss: 2.9434 - val_accuracy: 0.6000\n",
      "Epoch 1096/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5119 - accuracy: 0.3000\n",
      "Epoch 01096: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.5119 - accuracy: 0.3000 - val_loss: 2.6169 - val_accuracy: 0.2000\n",
      "Epoch 1097/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9706 - accuracy: 0.2667\n",
      "Epoch 01097: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.9706 - accuracy: 0.2667 - val_loss: 3.9666 - val_accuracy: 0.1000\n",
      "Epoch 1098/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3168 - accuracy: 0.3000\n",
      "Epoch 01098: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.3168 - accuracy: 0.3000 - val_loss: 2.5242 - val_accuracy: 0.5000\n",
      "Epoch 1099/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3482 - accuracy: 0.4333\n",
      "Epoch 01099: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.3482 - accuracy: 0.4333 - val_loss: 2.0449 - val_accuracy: 0.6000\n",
      "Epoch 1100/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7025 - accuracy: 0.3667\n",
      "Epoch 01100: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.7025 - accuracy: 0.3667 - val_loss: 5.6355 - val_accuracy: 0.0000e+00\n",
      "Epoch 1101/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4509 - accuracy: 0.2333\n",
      "Epoch 01101: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.4509 - accuracy: 0.2333 - val_loss: 3.4469 - val_accuracy: 0.2000\n",
      "Epoch 1102/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8783 - accuracy: 0.2000\n",
      "Epoch 01102: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.8783 - accuracy: 0.2000 - val_loss: 3.6651 - val_accuracy: 0.1000\n",
      "Epoch 1103/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7948 - accuracy: 0.3667\n",
      "Epoch 01103: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.7948 - accuracy: 0.3667 - val_loss: 2.7203 - val_accuracy: 0.3000\n",
      "Epoch 1104/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7665 - accuracy: 0.1333\n",
      "Epoch 01104: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.7665 - accuracy: 0.1333 - val_loss: 2.6457 - val_accuracy: 0.5000\n",
      "Epoch 1105/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5299 - accuracy: 0.2000\n",
      "Epoch 01105: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.5299 - accuracy: 0.2000 - val_loss: 4.0186 - val_accuracy: 0.1000\n",
      "Epoch 1106/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2609 - accuracy: 0.3000\n",
      "Epoch 01106: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 3.2609 - accuracy: 0.3000 - val_loss: 4.4958 - val_accuracy: 0.3000\n",
      "Epoch 1107/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4223 - accuracy: 0.3000\n",
      "Epoch 01107: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.4223 - accuracy: 0.3000 - val_loss: 6.7061 - val_accuracy: 0.0000e+00\n",
      "Epoch 1108/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0120 - accuracy: 0.3000\n",
      "Epoch 01108: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.0120 - accuracy: 0.3000 - val_loss: 6.4203 - val_accuracy: 0.0000e+00\n",
      "Epoch 1109/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3228 - accuracy: 0.2333\n",
      "Epoch 01109: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.3228 - accuracy: 0.2333 - val_loss: 6.4213 - val_accuracy: 0.0000e+00\n",
      "Epoch 1110/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4591 - accuracy: 0.2000\n",
      "Epoch 01110: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.4591 - accuracy: 0.2000 - val_loss: 2.4390 - val_accuracy: 0.5000\n",
      "Epoch 1111/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4890 - accuracy: 0.2000\n",
      "Epoch 01111: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.4890 - accuracy: 0.2000 - val_loss: 0.6974 - val_accuracy: 0.8000\n",
      "Epoch 1112/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2557 - accuracy: 0.2667\n",
      "Epoch 01112: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.2557 - accuracy: 0.2667 - val_loss: 4.5901 - val_accuracy: 0.0000e+00\n",
      "Epoch 1113/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9013 - accuracy: 0.3667\n",
      "Epoch 01113: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.9013 - accuracy: 0.3667 - val_loss: 2.1934 - val_accuracy: 0.6000\n",
      "Epoch 1114/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1709 - accuracy: 0.2000\n",
      "Epoch 01114: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.1709 - accuracy: 0.2000 - val_loss: 3.7777 - val_accuracy: 0.5000\n",
      "Epoch 1115/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0484 - accuracy: 0.2333\n",
      "Epoch 01115: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.0484 - accuracy: 0.2333 - val_loss: 4.7715 - val_accuracy: 0.0000e+00\n",
      "Epoch 1116/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1687 - accuracy: 0.2667\n",
      "Epoch 01116: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.1687 - accuracy: 0.2667 - val_loss: 6.5786 - val_accuracy: 0.0000e+00\n",
      "Epoch 1117/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3405 - accuracy: 0.3000\n",
      "Epoch 01117: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.3405 - accuracy: 0.3000 - val_loss: 3.9837 - val_accuracy: 0.0000e+00\n",
      "Epoch 1118/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0530 - accuracy: 0.2667\n",
      "Epoch 01118: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.0530 - accuracy: 0.2667 - val_loss: 3.8845 - val_accuracy: 0.0000e+00\n",
      "Epoch 1119/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3884 - accuracy: 0.4667\n",
      "Epoch 01119: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 2.3884 - accuracy: 0.4667 - val_loss: 2.3140 - val_accuracy: 0.6000\n",
      "Epoch 1120/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2281 - accuracy: 0.2000\n",
      "Epoch 01120: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 3.2281 - accuracy: 0.2000 - val_loss: 4.5786 - val_accuracy: 0.2000\n",
      "Epoch 1121/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3364 - accuracy: 0.2333\n",
      "Epoch 01121: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.3364 - accuracy: 0.2333 - val_loss: 3.0989 - val_accuracy: 0.3000\n",
      "Epoch 1122/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2373 - accuracy: 0.3000\n",
      "Epoch 01122: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.2373 - accuracy: 0.3000 - val_loss: 2.3827 - val_accuracy: 0.3000\n",
      "Epoch 1123/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8289 - accuracy: 0.2000\n",
      "Epoch 01123: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.8289 - accuracy: 0.2000 - val_loss: 2.5323 - val_accuracy: 0.3000\n",
      "Epoch 1124/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8559 - accuracy: 0.2667\n",
      "Epoch 01124: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 2.8559 - accuracy: 0.2667 - val_loss: 2.8039 - val_accuracy: 0.1000\n",
      "Epoch 1125/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8719 - accuracy: 0.2333\n",
      "Epoch 01125: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 2.8719 - accuracy: 0.2333 - val_loss: 2.5365 - val_accuracy: 0.4000\n",
      "Epoch 1126/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8801 - accuracy: 0.3000\n",
      "Epoch 01126: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 160ms/step - loss: 2.8801 - accuracy: 0.3000 - val_loss: 3.5395 - val_accuracy: 0.3000\n",
      "Epoch 1127/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7935 - accuracy: 0.2667\n",
      "Epoch 01127: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 2.7935 - accuracy: 0.2667 - val_loss: 3.2584 - val_accuracy: 0.4000\n",
      "Epoch 1128/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4317 - accuracy: 0.2000\n",
      "Epoch 01128: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.4317 - accuracy: 0.2000 - val_loss: 3.1536 - val_accuracy: 0.5000\n",
      "Epoch 1129/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6402 - accuracy: 0.1000    \n",
      "Epoch 01129: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.6402 - accuracy: 0.1000 - val_loss: 4.1406 - val_accuracy: 0.0000e+00\n",
      "Epoch 1130/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5934 - accuracy: 0.1000\n",
      "Epoch 01130: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 155ms/step - loss: 3.5934 - accuracy: 0.1000 - val_loss: 0.1587 - val_accuracy: 1.0000\n",
      "Epoch 1131/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4485 - accuracy: 0.1667\n",
      "Epoch 01131: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 3.4485 - accuracy: 0.1667 - val_loss: 2.7196 - val_accuracy: 0.5000\n",
      "Epoch 1132/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0417 - accuracy: 0.2333\n",
      "Epoch 01132: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 4.0417 - accuracy: 0.2333 - val_loss: 1.5068 - val_accuracy: 0.7000\n",
      "Epoch 1133/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5331 - accuracy: 0.1333\n",
      "Epoch 01133: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.5331 - accuracy: 0.1333 - val_loss: 0.7334 - val_accuracy: 0.9000\n",
      "Epoch 1134/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6443 - accuracy: 0.1333\n",
      "Epoch 01134: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.6443 - accuracy: 0.1333 - val_loss: 2.5182 - val_accuracy: 0.5000\n",
      "Epoch 1135/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2921 - accuracy: 0.2333\n",
      "Epoch 01135: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.2921 - accuracy: 0.2333 - val_loss: 5.4118 - val_accuracy: 0.0000e+00\n",
      "Epoch 1136/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1676 - accuracy: 0.2333\n",
      "Epoch 01136: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 4.1676 - accuracy: 0.2333 - val_loss: 2.8400 - val_accuracy: 0.1000\n",
      "Epoch 1137/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1960 - accuracy: 0.2000    \n",
      "Epoch 01137: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.1960 - accuracy: 0.2000 - val_loss: 3.5738 - val_accuracy: 0.3000\n",
      "Epoch 1138/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0010 - accuracy: 0.1667\n",
      "Epoch 01138: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 4.0010 - accuracy: 0.1667 - val_loss: 3.5231 - val_accuracy: 0.1000\n",
      "Epoch 1139/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6816 - accuracy: 0.1333\n",
      "Epoch 01139: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.6816 - accuracy: 0.1333 - val_loss: 4.4758 - val_accuracy: 0.0000e+00\n",
      "Epoch 1140/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2399 - accuracy: 0.3333\n",
      "Epoch 01140: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.2399 - accuracy: 0.3333 - val_loss: 2.2393 - val_accuracy: 0.4000\n",
      "Epoch 1141/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1426 - accuracy: 0.2667\n",
      "Epoch 01141: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 3.1426 - accuracy: 0.2667 - val_loss: 0.6280 - val_accuracy: 0.9000\n",
      "Epoch 1142/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3113 - accuracy: 0.3333\n",
      "Epoch 01142: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.3113 - accuracy: 0.3333 - val_loss: 1.8880 - val_accuracy: 0.6000\n",
      "Epoch 1143/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7178 - accuracy: 0.3000\n",
      "Epoch 01143: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.7178 - accuracy: 0.3000 - val_loss: 6.2477 - val_accuracy: 0.0000e+00\n",
      "Epoch 1144/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4639 - accuracy: 0.2333\n",
      "Epoch 01144: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.4639 - accuracy: 0.2333 - val_loss: 2.8004 - val_accuracy: 0.3000\n",
      "Epoch 1145/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7919 - accuracy: 0.2333\n",
      "Epoch 01145: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.7919 - accuracy: 0.2333 - val_loss: 2.9976 - val_accuracy: 0.1000\n",
      "Epoch 1146/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0080 - accuracy: 0.2667\n",
      "Epoch 01146: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0080 - accuracy: 0.2667 - val_loss: 3.6518 - val_accuracy: 0.1000\n",
      "Epoch 1147/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4557 - accuracy: 0.3667\n",
      "Epoch 01147: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.4557 - accuracy: 0.3667 - val_loss: 2.3956 - val_accuracy: 0.3000\n",
      "Epoch 1148/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6679 - accuracy: 0.3667\n",
      "Epoch 01148: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.6679 - accuracy: 0.3667 - val_loss: 4.2371 - val_accuracy: 0.3000\n",
      "Epoch 1149/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7894 - accuracy: 0.3000\n",
      "Epoch 01149: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.7894 - accuracy: 0.3000 - val_loss: 4.6509 - val_accuracy: 0.0000e+00\n",
      "Epoch 1150/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1974 - accuracy: 0.2667\n",
      "Epoch 01150: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.1974 - accuracy: 0.2667 - val_loss: 3.5293 - val_accuracy: 0.2000\n",
      "Epoch 1151/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2470 - accuracy: 0.2333\n",
      "Epoch 01151: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.2470 - accuracy: 0.2333 - val_loss: 5.2387 - val_accuracy: 0.0000e+00\n",
      "Epoch 1152/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5099 - accuracy: 0.1333\n",
      "Epoch 01152: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.5099 - accuracy: 0.1333 - val_loss: 1.4075 - val_accuracy: 0.4000\n",
      "Epoch 1153/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2237 - accuracy: 0.4000\n",
      "Epoch 01153: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.2237 - accuracy: 0.4000 - val_loss: 2.4841 - val_accuracy: 0.2000\n",
      "Epoch 1154/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1890 - accuracy: 0.3333\n",
      "Epoch 01154: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.1890 - accuracy: 0.3333 - val_loss: 3.6070 - val_accuracy: 0.1000\n",
      "Epoch 1155/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6686 - accuracy: 0.2000\n",
      "Epoch 01155: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.6686 - accuracy: 0.2000 - val_loss: 3.0362 - val_accuracy: 0.0000e+00\n",
      "Epoch 1156/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1157 - accuracy: 0.3000\n",
      "Epoch 01156: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.1157 - accuracy: 0.3000 - val_loss: 4.3139 - val_accuracy: 0.0000e+00\n",
      "Epoch 1157/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1658 - accuracy: 0.3333\n",
      "Epoch 01157: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.1658 - accuracy: 0.3333 - val_loss: 5.2721 - val_accuracy: 0.0000e+00\n",
      "Epoch 1158/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4661 - accuracy: 0.2333\n",
      "Epoch 01158: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.4661 - accuracy: 0.2333 - val_loss: 2.8180 - val_accuracy: 0.3000\n",
      "Epoch 1159/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8897 - accuracy: 0.1667\n",
      "Epoch 01159: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.8897 - accuracy: 0.1667 - val_loss: 3.4177 - val_accuracy: 0.4000\n",
      "Epoch 1160/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0191 - accuracy: 0.3333\n",
      "Epoch 01160: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.0191 - accuracy: 0.3333 - val_loss: 2.3155 - val_accuracy: 0.5000\n",
      "Epoch 1161/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4203 - accuracy: 0.2333\n",
      "Epoch 01161: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.4203 - accuracy: 0.2333 - val_loss: 4.7460 - val_accuracy: 0.0000e+00\n",
      "Epoch 1162/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7281 - accuracy: 0.4000\n",
      "Epoch 01162: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 2.7281 - accuracy: 0.4000 - val_loss: 3.6595 - val_accuracy: 0.1000\n",
      "Epoch 1163/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0359 - accuracy: 0.3667\n",
      "Epoch 01163: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0359 - accuracy: 0.3667 - val_loss: 5.7221 - val_accuracy: 0.0000e+00\n",
      "Epoch 1164/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0601 - accuracy: 0.2000\n",
      "Epoch 01164: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.0601 - accuracy: 0.2000 - val_loss: 3.4904 - val_accuracy: 0.1000\n",
      "Epoch 1165/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3761 - accuracy: 0.2000\n",
      "Epoch 01165: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.3761 - accuracy: 0.2000 - val_loss: 3.0758 - val_accuracy: 0.1000\n",
      "Epoch 1166/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9833 - accuracy: 0.2333\n",
      "Epoch 01166: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.9833 - accuracy: 0.2333 - val_loss: 5.6809 - val_accuracy: 0.0000e+00\n",
      "Epoch 1167/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1703 - accuracy: 0.3000\n",
      "Epoch 01167: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.1703 - accuracy: 0.3000 - val_loss: 3.4380 - val_accuracy: 0.4000\n",
      "Epoch 1168/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0714 - accuracy: 0.3333\n",
      "Epoch 01168: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.0714 - accuracy: 0.3333 - val_loss: 2.2130 - val_accuracy: 0.4000\n",
      "Epoch 1169/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2947 - accuracy: 0.3000\n",
      "Epoch 01169: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.2947 - accuracy: 0.3000 - val_loss: 2.5628 - val_accuracy: 0.3000\n",
      "Epoch 1170/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5830 - accuracy: 0.2667\n",
      "Epoch 01170: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.5830 - accuracy: 0.2667 - val_loss: 5.4462 - val_accuracy: 0.0000e+00\n",
      "Epoch 1171/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0768 - accuracy: 0.3333\n",
      "Epoch 01171: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.0768 - accuracy: 0.3333 - val_loss: 5.2307 - val_accuracy: 0.0000e+00\n",
      "Epoch 1172/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0269 - accuracy: 0.3667\n",
      "Epoch 01172: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.0269 - accuracy: 0.3667 - val_loss: 3.1241 - val_accuracy: 0.3000\n",
      "Epoch 1173/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0047 - accuracy: 0.2667\n",
      "Epoch 01173: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.0047 - accuracy: 0.2667 - val_loss: 2.5132 - val_accuracy: 0.3000\n",
      "Epoch 1174/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6544 - accuracy: 0.2000\n",
      "Epoch 01174: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.6544 - accuracy: 0.2000 - val_loss: 1.6464 - val_accuracy: 0.6000\n",
      "Epoch 1175/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9105 - accuracy: 0.3333\n",
      "Epoch 01175: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.9105 - accuracy: 0.3333 - val_loss: 2.4765 - val_accuracy: 0.2000\n",
      "Epoch 1176/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5758 - accuracy: 0.2333\n",
      "Epoch 01176: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.5758 - accuracy: 0.2333 - val_loss: 6.2566 - val_accuracy: 0.0000e+00\n",
      "Epoch 1177/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1769 - accuracy: 0.2667\n",
      "Epoch 01177: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.1769 - accuracy: 0.2667 - val_loss: 2.9658 - val_accuracy: 0.2000\n",
      "Epoch 1178/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3605 - accuracy: 0.2667\n",
      "Epoch 01178: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.3605 - accuracy: 0.2667 - val_loss: 1.1831 - val_accuracy: 0.8000\n",
      "Epoch 1179/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5267 - accuracy: 0.2667\n",
      "Epoch 01179: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.5267 - accuracy: 0.2667 - val_loss: 5.5164 - val_accuracy: 0.0000e+00\n",
      "Epoch 1180/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1163 - accuracy: 0.3333\n",
      "Epoch 01180: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.1163 - accuracy: 0.3333 - val_loss: 4.8957 - val_accuracy: 0.1000\n",
      "Epoch 1181/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3750 - accuracy: 0.2000\n",
      "Epoch 01181: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.3750 - accuracy: 0.2000 - val_loss: 4.4581 - val_accuracy: 0.3000\n",
      "Epoch 1182/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8564 - accuracy: 0.3667\n",
      "Epoch 01182: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.8564 - accuracy: 0.3667 - val_loss: 5.8932 - val_accuracy: 0.0000e+00\n",
      "Epoch 1183/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3250 - accuracy: 0.3333\n",
      "Epoch 01183: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.3250 - accuracy: 0.3333 - val_loss: 5.6666 - val_accuracy: 0.2000\n",
      "Epoch 1184/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4299 - accuracy: 0.2333\n",
      "Epoch 01184: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.4299 - accuracy: 0.2333 - val_loss: 5.6519 - val_accuracy: 0.0000e+00\n",
      "Epoch 1185/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2563 - accuracy: 0.2333\n",
      "Epoch 01185: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.2563 - accuracy: 0.2333 - val_loss: 3.7771 - val_accuracy: 0.1000\n",
      "Epoch 1186/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4389 - accuracy: 0.1333\n",
      "Epoch 01186: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.4389 - accuracy: 0.1333 - val_loss: 3.2536 - val_accuracy: 0.1000\n",
      "Epoch 1187/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7724 - accuracy: 0.2000\n",
      "Epoch 01187: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.7724 - accuracy: 0.2000 - val_loss: 1.3710 - val_accuracy: 0.6000\n",
      "Epoch 1188/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9628 - accuracy: 0.2333\n",
      "Epoch 01188: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.9628 - accuracy: 0.2333 - val_loss: 1.3093 - val_accuracy: 0.5000\n",
      "Epoch 1189/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0437 - accuracy: 0.3667\n",
      "Epoch 01189: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.0437 - accuracy: 0.3667 - val_loss: 2.6796 - val_accuracy: 0.5000\n",
      "Epoch 1190/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4335 - accuracy: 0.2333\n",
      "Epoch 01190: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.4335 - accuracy: 0.2333 - val_loss: 3.4537 - val_accuracy: 0.1000\n",
      "Epoch 1191/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7595 - accuracy: 0.2000\n",
      "Epoch 01191: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.7595 - accuracy: 0.2000 - val_loss: 4.7146 - val_accuracy: 0.0000e+00\n",
      "Epoch 1192/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3115 - accuracy: 0.3000\n",
      "Epoch 01192: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.3115 - accuracy: 0.3000 - val_loss: 4.8645 - val_accuracy: 0.0000e+00\n",
      "Epoch 1193/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2545 - accuracy: 0.2333\n",
      "Epoch 01193: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.2545 - accuracy: 0.2333 - val_loss: 3.0928 - val_accuracy: 0.3000\n",
      "Epoch 1194/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3461 - accuracy: 0.2667\n",
      "Epoch 01194: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.3461 - accuracy: 0.2667 - val_loss: 3.1217 - val_accuracy: 0.3000\n",
      "Epoch 1195/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9115 - accuracy: 0.1852\n",
      "Epoch 01195: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.9115 - accuracy: 0.1852 - val_loss: 4.3000 - val_accuracy: 0.1000\n",
      "Epoch 1196/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5057 - accuracy: 0.2000\n",
      "Epoch 01196: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.5057 - accuracy: 0.2000 - val_loss: 2.0998 - val_accuracy: 0.4000\n",
      "Epoch 1197/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6245 - accuracy: 0.4000\n",
      "Epoch 01197: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 2.6245 - accuracy: 0.4000 - val_loss: 2.4080 - val_accuracy: 0.4000\n",
      "Epoch 1198/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9585 - accuracy: 0.3667\n",
      "Epoch 01198: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.9585 - accuracy: 0.3667 - val_loss: 3.0314 - val_accuracy: 0.1000\n",
      "Epoch 1199/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0796 - accuracy: 0.3667\n",
      "Epoch 01199: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.0796 - accuracy: 0.3667 - val_loss: 2.9669 - val_accuracy: 0.3000\n",
      "Epoch 1200/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7024 - accuracy: 0.3000\n",
      "Epoch 01200: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.7024 - accuracy: 0.3000 - val_loss: 3.0344 - val_accuracy: 0.4000\n",
      "Epoch 1201/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5179 - accuracy: 0.3000\n",
      "Epoch 01201: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.5179 - accuracy: 0.3000 - val_loss: 5.1974 - val_accuracy: 0.3000\n",
      "Epoch 1202/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0087 - accuracy: 0.2333\n",
      "Epoch 01202: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.0087 - accuracy: 0.2333 - val_loss: 3.1390 - val_accuracy: 0.3000\n",
      "Epoch 1203/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6703 - accuracy: 0.3000\n",
      "Epoch 01203: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.6703 - accuracy: 0.3000 - val_loss: 4.4116 - val_accuracy: 0.2000\n",
      "Epoch 1204/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4707 - accuracy: 0.3000\n",
      "Epoch 01204: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.4707 - accuracy: 0.3000 - val_loss: 4.0861 - val_accuracy: 0.2000\n",
      "Epoch 1205/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4658 - accuracy: 0.3667\n",
      "Epoch 01205: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 171ms/step - loss: 2.4658 - accuracy: 0.3667 - val_loss: 2.4615 - val_accuracy: 0.3000\n",
      "Epoch 1206/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8404 - accuracy: 0.3667\n",
      "Epoch 01206: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 2.8404 - accuracy: 0.3667 - val_loss: 2.2335 - val_accuracy: 0.6000\n",
      "Epoch 1207/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7191 - accuracy: 0.2333\n",
      "Epoch 01207: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.7191 - accuracy: 0.2333 - val_loss: 2.1727 - val_accuracy: 0.4000\n",
      "Epoch 1208/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5858 - accuracy: 0.3667\n",
      "Epoch 01208: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.5858 - accuracy: 0.3667 - val_loss: 2.7490 - val_accuracy: 0.3000\n",
      "Epoch 1209/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6052 - accuracy: 0.4333\n",
      "Epoch 01209: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 2.6052 - accuracy: 0.4333 - val_loss: 1.8597 - val_accuracy: 0.4000\n",
      "Epoch 1210/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8786 - accuracy: 0.3333\n",
      "Epoch 01210: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 2.8786 - accuracy: 0.3333 - val_loss: 1.3973 - val_accuracy: 0.5000\n",
      "Epoch 1211/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5808 - accuracy: 0.1667\n",
      "Epoch 01211: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 3.5808 - accuracy: 0.1667 - val_loss: 1.3880 - val_accuracy: 0.6000\n",
      "Epoch 1212/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3581 - accuracy: 0.3000\n",
      "Epoch 01212: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 3.3581 - accuracy: 0.3000 - val_loss: 2.0549 - val_accuracy: 0.6000\n",
      "Epoch 1213/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9347 - accuracy: 0.2667\n",
      "Epoch 01213: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 2.9347 - accuracy: 0.2667 - val_loss: 2.6292 - val_accuracy: 0.6000\n",
      "Epoch 1214/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3071 - accuracy: 0.1667\n",
      "Epoch 01214: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.3071 - accuracy: 0.1667 - val_loss: 2.6397 - val_accuracy: 0.6000\n",
      "Epoch 1215/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.1103 - accuracy: 0.5000\n",
      "Epoch 01215: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 2.1103 - accuracy: 0.5000 - val_loss: 1.6764 - val_accuracy: 0.5000\n",
      "Epoch 1216/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0412 - accuracy: 0.2667\n",
      "Epoch 01216: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 3.0412 - accuracy: 0.2667 - val_loss: 1.7253 - val_accuracy: 0.6000\n",
      "Epoch 1217/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3075 - accuracy: 0.2000\n",
      "Epoch 01217: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 3.3075 - accuracy: 0.2000 - val_loss: 1.9281 - val_accuracy: 0.5000\n",
      "Epoch 1218/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4994 - accuracy: 0.2333\n",
      "Epoch 01218: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 3.4994 - accuracy: 0.2333 - val_loss: 1.8041 - val_accuracy: 0.7000\n",
      "Epoch 1219/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0409 - accuracy: 0.4000\n",
      "Epoch 01219: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 145ms/step - loss: 3.0409 - accuracy: 0.4000 - val_loss: 1.9601 - val_accuracy: 0.7000\n",
      "Epoch 1220/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1524 - accuracy: 0.3000\n",
      "Epoch 01220: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.1524 - accuracy: 0.3000 - val_loss: 0.9877 - val_accuracy: 0.9000\n",
      "Epoch 1221/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3407 - accuracy: 0.2000\n",
      "Epoch 01221: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 3.3407 - accuracy: 0.2000 - val_loss: 3.9244 - val_accuracy: 0.0000e+00\n",
      "Epoch 1222/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7965 - accuracy: 0.3333\n",
      "Epoch 01222: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 2.7965 - accuracy: 0.3333 - val_loss: 3.9166 - val_accuracy: 0.1000\n",
      "Epoch 1223/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1360 - accuracy: 0.2000\n",
      "Epoch 01223: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.1360 - accuracy: 0.2000 - val_loss: 2.3757 - val_accuracy: 0.5000\n",
      "Epoch 1224/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0018 - accuracy: 0.1667\n",
      "Epoch 01224: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 129ms/step - loss: 4.0018 - accuracy: 0.1667 - val_loss: 3.4819 - val_accuracy: 0.1000\n",
      "Epoch 1225/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1307 - accuracy: 0.2000\n",
      "Epoch 01225: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.1307 - accuracy: 0.2000 - val_loss: 2.7534 - val_accuracy: 0.5000\n",
      "Epoch 1226/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6666 - accuracy: 0.3000\n",
      "Epoch 01226: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 2.6666 - accuracy: 0.3000 - val_loss: 4.7292 - val_accuracy: 0.0000e+00\n",
      "Epoch 1227/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5624 - accuracy: 0.4000\n",
      "Epoch 01227: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 2.5624 - accuracy: 0.4000 - val_loss: 3.5709 - val_accuracy: 0.3000\n",
      "Epoch 1228/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0901 - accuracy: 0.3000\n",
      "Epoch 01228: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0901 - accuracy: 0.3000 - val_loss: 1.6137 - val_accuracy: 0.5000\n",
      "Epoch 1229/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2819 - accuracy: 0.2000\n",
      "Epoch 01229: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.2819 - accuracy: 0.2000 - val_loss: 4.4641 - val_accuracy: 0.0000e+00\n",
      "Epoch 1230/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2439 - accuracy: 0.2667\n",
      "Epoch 01230: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.2439 - accuracy: 0.2667 - val_loss: 4.4630 - val_accuracy: 0.1000\n",
      "Epoch 1231/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.1659 - accuracy: 0.2000\n",
      "Epoch 01231: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 3.4608 - accuracy: 0.2333 - val_loss: 2.4958 - val_accuracy: 0.5000\n",
      "Epoch 1232/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1695 - accuracy: 0.3667\n",
      "Epoch 01232: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 239ms/step - loss: 3.1695 - accuracy: 0.3667 - val_loss: 3.5398 - val_accuracy: 0.5000\n",
      "Epoch 1233/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.1277 - accuracy: 0.4000\n",
      "Epoch 01233: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.0326 - accuracy: 0.2333 - val_loss: 2.0471 - val_accuracy: 0.6000\n",
      "Epoch 1234/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3300 - accuracy: 0.2667\n",
      "Epoch 01234: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 3.3300 - accuracy: 0.2667 - val_loss: 2.1115 - val_accuracy: 0.6000\n",
      "Epoch 1235/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8848 - accuracy: 0.1333\n",
      "Epoch 01235: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.8848 - accuracy: 0.1333 - val_loss: 2.3429 - val_accuracy: 0.3000\n",
      "Epoch 1236/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7704 - accuracy: 0.2667\n",
      "Epoch 01236: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 119ms/step - loss: 3.7704 - accuracy: 0.2667 - val_loss: 2.1817 - val_accuracy: 0.4000\n",
      "Epoch 1237/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9573 - accuracy: 0.2333\n",
      "Epoch 01237: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.9573 - accuracy: 0.2333 - val_loss: 2.3572 - val_accuracy: 0.2000\n",
      "Epoch 1238/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5533 - accuracy: 0.2000\n",
      "Epoch 01238: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.5533 - accuracy: 0.2000 - val_loss: 5.2283 - val_accuracy: 0.3000\n",
      "Epoch 1239/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1054 - accuracy: 0.2667\n",
      "Epoch 01239: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.1054 - accuracy: 0.2667 - val_loss: 2.3621 - val_accuracy: 0.5000\n",
      "Epoch 1240/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1023 - accuracy: 0.3000\n",
      "Epoch 01240: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.1023 - accuracy: 0.3000 - val_loss: 2.5772 - val_accuracy: 0.4000\n",
      "Epoch 1241/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3583 - accuracy: 0.1667\n",
      "Epoch 01241: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.3583 - accuracy: 0.1667 - val_loss: 2.2831 - val_accuracy: 0.7000\n",
      "Epoch 1242/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7912 - accuracy: 0.3000\n",
      "Epoch 01242: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.7912 - accuracy: 0.3000 - val_loss: 2.5234 - val_accuracy: 0.5000\n",
      "Epoch 1243/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3267 - accuracy: 0.3333\n",
      "Epoch 01243: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.3267 - accuracy: 0.3333 - val_loss: 4.3339 - val_accuracy: 0.1000\n",
      "Epoch 1244/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9484 - accuracy: 0.4000\n",
      "Epoch 01244: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.9484 - accuracy: 0.4000 - val_loss: 3.3751 - val_accuracy: 0.4000\n",
      "Epoch 1245/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1403 - accuracy: 0.2333\n",
      "Epoch 01245: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.1403 - accuracy: 0.2333 - val_loss: 3.1340 - val_accuracy: 0.1000\n",
      "Epoch 1246/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7140 - accuracy: 0.3667\n",
      "Epoch 01246: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.7140 - accuracy: 0.3667 - val_loss: 4.4336 - val_accuracy: 0.1000\n",
      "Epoch 1247/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8165 - accuracy: 0.1667\n",
      "Epoch 01247: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.8165 - accuracy: 0.1667 - val_loss: 2.0470 - val_accuracy: 0.6000\n",
      "Epoch 1248/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3837 - accuracy: 0.1667\n",
      "Epoch 01248: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 3.3837 - accuracy: 0.1667 - val_loss: 2.5096 - val_accuracy: 0.3000\n",
      "Epoch 1249/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4675 - accuracy: 0.2667\n",
      "Epoch 01249: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.4675 - accuracy: 0.2667 - val_loss: 3.8426 - val_accuracy: 0.1000\n",
      "Epoch 1250/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.4139 - accuracy: 0.4000\n",
      "Epoch 01250: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.4139 - accuracy: 0.4000 - val_loss: 5.7323 - val_accuracy: 0.2000\n",
      "Epoch 1251/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3369 - accuracy: 0.3000\n",
      "Epoch 01251: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.3369 - accuracy: 0.3000 - val_loss: 3.5464 - val_accuracy: 0.5000\n",
      "Epoch 1252/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5651 - accuracy: 0.3000\n",
      "Epoch 01252: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.5651 - accuracy: 0.3000 - val_loss: 2.7438 - val_accuracy: 0.3000\n",
      "Epoch 1253/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7781 - accuracy: 0.3333\n",
      "Epoch 01253: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 2.7781 - accuracy: 0.3333 - val_loss: 2.6972 - val_accuracy: 0.3000\n",
      "Epoch 1254/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6252 - accuracy: 0.3000\n",
      "Epoch 01254: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.6252 - accuracy: 0.3000 - val_loss: 4.6193 - val_accuracy: 0.1000\n",
      "Epoch 1255/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8923 - accuracy: 0.3000\n",
      "Epoch 01255: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.8923 - accuracy: 0.3000 - val_loss: 2.2949 - val_accuracy: 0.5000\n",
      "Epoch 1256/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1365 - accuracy: 0.3667\n",
      "Epoch 01256: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 3.1365 - accuracy: 0.3667 - val_loss: 1.5161 - val_accuracy: 0.5000\n",
      "Epoch 1257/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.4500 - accuracy: 0.1667\n",
      "Epoch 01257: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 4.4500 - accuracy: 0.1667 - val_loss: 0.6980 - val_accuracy: 0.8000\n",
      "Epoch 1258/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5134 - accuracy: 0.1667\n",
      "Epoch 01258: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.5134 - accuracy: 0.1667 - val_loss: 3.2758 - val_accuracy: 0.0000e+00\n",
      "Epoch 1259/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8150 - accuracy: 0.3667\n",
      "Epoch 01259: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.8150 - accuracy: 0.3667 - val_loss: 2.9029 - val_accuracy: 0.0000e+00\n",
      "Epoch 1260/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0122 - accuracy: 0.2667\n",
      "Epoch 01260: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.0122 - accuracy: 0.2667 - val_loss: 2.4949 - val_accuracy: 0.1000\n",
      "Epoch 1261/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8882 - accuracy: 0.3333\n",
      "Epoch 01261: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.8882 - accuracy: 0.3333 - val_loss: 3.7848 - val_accuracy: 0.0000e+00\n",
      "Epoch 1262/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0986 - accuracy: 0.3000\n",
      "Epoch 01262: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 3.0986 - accuracy: 0.3000 - val_loss: 3.4431 - val_accuracy: 0.2000\n",
      "Epoch 1263/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4387 - accuracy: 0.2000\n",
      "Epoch 01263: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.4387 - accuracy: 0.2000 - val_loss: 2.2310 - val_accuracy: 0.4000\n",
      "Epoch 1264/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8206 - accuracy: 0.2333\n",
      "Epoch 01264: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.8206 - accuracy: 0.2333 - val_loss: 2.2188 - val_accuracy: 0.5000\n",
      "Epoch 1265/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7457 - accuracy: 0.3333\n",
      "Epoch 01265: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 79ms/step - loss: 3.7457 - accuracy: 0.3333 - val_loss: 1.7114 - val_accuracy: 0.7000\n",
      "Epoch 1266/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3965 - accuracy: 0.2667\n",
      "Epoch 01266: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.3965 - accuracy: 0.2667 - val_loss: 1.9836 - val_accuracy: 0.6000\n",
      "Epoch 1267/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3102 - accuracy: 0.4333\n",
      "Epoch 01267: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 2.3102 - accuracy: 0.4333 - val_loss: 3.0004 - val_accuracy: 0.1000\n",
      "Epoch 1268/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0610 - accuracy: 0.4333\n",
      "Epoch 01268: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.0610 - accuracy: 0.4333 - val_loss: 3.4373 - val_accuracy: 0.0000e+00\n",
      "Epoch 1269/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8739 - accuracy: 0.4000\n",
      "Epoch 01269: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.8739 - accuracy: 0.4000 - val_loss: 3.6175 - val_accuracy: 0.0000e+00\n",
      "Epoch 1270/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.1323 - accuracy: 0.5667\n",
      "Epoch 01270: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 83ms/step - loss: 2.1323 - accuracy: 0.5667 - val_loss: 2.7241 - val_accuracy: 0.3000\n",
      "Epoch 1271/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6431 - accuracy: 0.2000\n",
      "Epoch 01271: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.6431 - accuracy: 0.2000 - val_loss: 2.1517 - val_accuracy: 0.5000\n",
      "Epoch 1272/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0450 - accuracy: 0.3000\n",
      "Epoch 01272: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.0450 - accuracy: 0.3000 - val_loss: 3.0046 - val_accuracy: 0.2000\n",
      "Epoch 1273/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3078 - accuracy: 0.4333\n",
      "Epoch 01273: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.3078 - accuracy: 0.4333 - val_loss: 5.6995 - val_accuracy: 0.0000e+00\n",
      "Epoch 1274/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1766 - accuracy: 0.3000\n",
      "Epoch 01274: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.1766 - accuracy: 0.3000 - val_loss: 7.5671 - val_accuracy: 0.0000e+00\n",
      "Epoch 1275/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8358 - accuracy: 0.3000\n",
      "Epoch 01275: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.8358 - accuracy: 0.3000 - val_loss: 2.5715 - val_accuracy: 0.3000\n",
      "Epoch 1276/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7821 - accuracy: 0.1667\n",
      "Epoch 01276: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.7821 - accuracy: 0.1667 - val_loss: 3.0304 - val_accuracy: 0.2000\n",
      "Epoch 1277/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6867 - accuracy: 0.3000\n",
      "Epoch 01277: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.6867 - accuracy: 0.3000 - val_loss: 3.4495 - val_accuracy: 0.2000\n",
      "Epoch 1278/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1910 - accuracy: 0.2667\n",
      "Epoch 01278: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 3.1910 - accuracy: 0.2667 - val_loss: 2.1331 - val_accuracy: 0.5000\n",
      "Epoch 1279/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8569 - accuracy: 0.2667\n",
      "Epoch 01279: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.8569 - accuracy: 0.2667 - val_loss: 2.0094 - val_accuracy: 0.5000\n",
      "Epoch 1280/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7932 - accuracy: 0.2667\n",
      "Epoch 01280: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.7932 - accuracy: 0.2667 - val_loss: 1.0316 - val_accuracy: 0.7000\n",
      "Epoch 1281/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2739 - accuracy: 0.2333\n",
      "Epoch 01281: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.2739 - accuracy: 0.2333 - val_loss: 1.6710 - val_accuracy: 0.5000\n",
      "Epoch 1282/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7026 - accuracy: 0.3000\n",
      "Epoch 01282: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 2.7026 - accuracy: 0.3000 - val_loss: 0.6439 - val_accuracy: 0.9000\n",
      "Epoch 1283/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6290 - accuracy: 0.2000\n",
      "Epoch 01283: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.6290 - accuracy: 0.2000 - val_loss: 4.9118 - val_accuracy: 0.0000e+00\n",
      "Epoch 1284/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2018 - accuracy: 0.2333\n",
      "Epoch 01284: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.2018 - accuracy: 0.2333 - val_loss: 2.0361 - val_accuracy: 0.3000\n",
      "Epoch 1285/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3307 - accuracy: 0.3333\n",
      "Epoch 01285: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 3.3307 - accuracy: 0.3333 - val_loss: 2.3950 - val_accuracy: 0.1000\n",
      "Epoch 1286/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2191 - accuracy: 0.1667\n",
      "Epoch 01286: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.2191 - accuracy: 0.1667 - val_loss: 2.7332 - val_accuracy: 0.6000\n",
      "Epoch 1287/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7868 - accuracy: 0.4333\n",
      "Epoch 01287: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.7868 - accuracy: 0.4333 - val_loss: 1.2396 - val_accuracy: 0.5000\n",
      "Epoch 1288/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5320 - accuracy: 0.2667\n",
      "Epoch 01288: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.5320 - accuracy: 0.2667 - val_loss: 1.6726 - val_accuracy: 0.5000\n",
      "Epoch 1289/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5438 - accuracy: 0.4333\n",
      "Epoch 01289: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.5438 - accuracy: 0.4333 - val_loss: 4.4009 - val_accuracy: 0.0000e+00\n",
      "Epoch 1290/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4542 - accuracy: 0.2333\n",
      "Epoch 01290: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.4542 - accuracy: 0.2333 - val_loss: 3.1647 - val_accuracy: 0.3000\n",
      "Epoch 1291/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9679 - accuracy: 0.2000\n",
      "Epoch 01291: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 2.9679 - accuracy: 0.2000 - val_loss: 3.8579 - val_accuracy: 0.0000e+00\n",
      "Epoch 1292/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8843 - accuracy: 0.3000\n",
      "Epoch 01292: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 2.8843 - accuracy: 0.3000 - val_loss: 4.3872 - val_accuracy: 0.0000e+00\n",
      "Epoch 1293/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2353 - accuracy: 0.3333\n",
      "Epoch 01293: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.2353 - accuracy: 0.3333 - val_loss: 4.3191 - val_accuracy: 0.1000\n",
      "Epoch 1294/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1327 - accuracy: 0.2667\n",
      "Epoch 01294: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.1327 - accuracy: 0.2667 - val_loss: 1.2172 - val_accuracy: 0.6000\n",
      "Epoch 1295/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1297 - accuracy: 0.2333\n",
      "Epoch 01295: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.1297 - accuracy: 0.2333 - val_loss: 3.0406 - val_accuracy: 0.4000\n",
      "Epoch 1296/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5687 - accuracy: 0.1333\n",
      "Epoch 01296: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.5687 - accuracy: 0.1333 - val_loss: 3.7091 - val_accuracy: 0.4000\n",
      "Epoch 1297/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1140 - accuracy: 0.3333\n",
      "Epoch 01297: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 3.1140 - accuracy: 0.3333 - val_loss: 5.0984 - val_accuracy: 0.4000\n",
      "Epoch 1298/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0735 - accuracy: 0.2333\n",
      "Epoch 01298: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.0735 - accuracy: 0.2333 - val_loss: 1.8850 - val_accuracy: 0.5000\n",
      "Epoch 1299/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2494 - accuracy: 0.3000\n",
      "Epoch 01299: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.2494 - accuracy: 0.3000 - val_loss: 3.8336 - val_accuracy: 0.2000\n",
      "Epoch 1300/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0681 - accuracy: 0.3333\n",
      "Epoch 01300: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.0681 - accuracy: 0.3333 - val_loss: 1.6010 - val_accuracy: 0.7000\n",
      "Epoch 1301/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1073 - accuracy: 0.2667\n",
      "Epoch 01301: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 3.1073 - accuracy: 0.2667 - val_loss: 3.3867 - val_accuracy: 0.4000\n",
      "Epoch 1302/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1964 - accuracy: 0.3333\n",
      "Epoch 01302: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 187ms/step - loss: 3.1964 - accuracy: 0.3333 - val_loss: 1.4698 - val_accuracy: 0.7000\n",
      "Epoch 1303/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9946 - accuracy: 0.2000\n",
      "Epoch 01303: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 2.9946 - accuracy: 0.2000 - val_loss: 3.5713 - val_accuracy: 0.7000\n",
      "Epoch 1304/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9845 - accuracy: 0.1333\n",
      "Epoch 01304: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.9845 - accuracy: 0.1333 - val_loss: 2.4600 - val_accuracy: 0.3000\n",
      "Epoch 1305/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5163 - accuracy: 0.2333\n",
      "Epoch 01305: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.5163 - accuracy: 0.2333 - val_loss: 5.2670 - val_accuracy: 0.1000\n",
      "Epoch 1306/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8107 - accuracy: 0.2333\n",
      "Epoch 01306: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 2.8107 - accuracy: 0.2333 - val_loss: 4.3341 - val_accuracy: 0.0000e+00\n",
      "Epoch 1307/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5690 - accuracy: 0.4000\n",
      "Epoch 01307: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 148ms/step - loss: 2.5690 - accuracy: 0.4000 - val_loss: 3.5386 - val_accuracy: 0.1000\n",
      "Epoch 1308/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6091 - accuracy: 0.3667\n",
      "Epoch 01308: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 2.6091 - accuracy: 0.3667 - val_loss: 4.1303 - val_accuracy: 0.3000\n",
      "Epoch 1309/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3281 - accuracy: 0.3667\n",
      "Epoch 01309: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 2.3281 - accuracy: 0.3667 - val_loss: 7.1001 - val_accuracy: 0.0000e+00\n",
      "Epoch 1310/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9962 - accuracy: 0.3333\n",
      "Epoch 01310: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 117ms/step - loss: 2.9962 - accuracy: 0.3333 - val_loss: 1.8058 - val_accuracy: 0.6000\n",
      "Epoch 1311/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1101 - accuracy: 0.3333\n",
      "Epoch 01311: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.1101 - accuracy: 0.3333 - val_loss: 1.2817 - val_accuracy: 0.6000\n",
      "Epoch 1312/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1395 - accuracy: 0.3333\n",
      "Epoch 01312: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 3.1395 - accuracy: 0.3333 - val_loss: 3.4178 - val_accuracy: 0.1000\n",
      "Epoch 1313/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6168 - accuracy: 0.2667\n",
      "Epoch 01313: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 123ms/step - loss: 2.6168 - accuracy: 0.2667 - val_loss: 2.5701 - val_accuracy: 0.3000\n",
      "Epoch 1314/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0202 - accuracy: 0.2667\n",
      "Epoch 01314: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 3.0202 - accuracy: 0.2667 - val_loss: 3.8253 - val_accuracy: 0.2000\n",
      "Epoch 1315/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9170 - accuracy: 0.2667\n",
      "Epoch 01315: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 2.9170 - accuracy: 0.2667 - val_loss: 3.6924 - val_accuracy: 0.0000e+00\n",
      "Epoch 1316/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2077 - accuracy: 0.2667\n",
      "Epoch 01316: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.2077 - accuracy: 0.2667 - val_loss: 2.4888 - val_accuracy: 0.4000\n",
      "Epoch 1317/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3760 - accuracy: 0.3333\n",
      "Epoch 01317: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 111ms/step - loss: 3.3760 - accuracy: 0.3333 - val_loss: 2.4685 - val_accuracy: 0.3000\n",
      "Epoch 1318/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1914 - accuracy: 0.2333\n",
      "Epoch 01318: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.1914 - accuracy: 0.2333 - val_loss: 1.8029 - val_accuracy: 0.6000\n",
      "Epoch 1319/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2667 - accuracy: 0.3333\n",
      "Epoch 01319: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.2667 - accuracy: 0.3333 - val_loss: 3.0192 - val_accuracy: 0.4000\n",
      "Epoch 1320/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3946 - accuracy: 0.3000\n",
      "Epoch 01320: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.3946 - accuracy: 0.3000 - val_loss: 1.3104 - val_accuracy: 0.6000\n",
      "Epoch 1321/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0279 - accuracy: 0.3333\n",
      "Epoch 01321: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.0279 - accuracy: 0.3333 - val_loss: 3.5979 - val_accuracy: 0.2000\n",
      "Epoch 1322/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6260 - accuracy: 0.2000\n",
      "Epoch 01322: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.6260 - accuracy: 0.2000 - val_loss: 2.8685 - val_accuracy: 0.4000\n",
      "Epoch 1323/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2173 - accuracy: 0.3000\n",
      "Epoch 01323: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.2173 - accuracy: 0.3000 - val_loss: 2.0705 - val_accuracy: 0.4000\n",
      "Epoch 1324/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2895 - accuracy: 0.2667\n",
      "Epoch 01324: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.2895 - accuracy: 0.2667 - val_loss: 1.4965 - val_accuracy: 0.7000\n",
      "Epoch 1325/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0633 - accuracy: 0.3667\n",
      "Epoch 01325: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.0633 - accuracy: 0.3667 - val_loss: 2.0469 - val_accuracy: 0.4000\n",
      "Epoch 1326/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8985 - accuracy: 0.4000\n",
      "Epoch 01326: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 2.8985 - accuracy: 0.4000 - val_loss: 4.3246 - val_accuracy: 0.0000e+00\n",
      "Epoch 1327/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5421 - accuracy: 0.1667\n",
      "Epoch 01327: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 121ms/step - loss: 3.5421 - accuracy: 0.1667 - val_loss: 2.1267 - val_accuracy: 0.4000\n",
      "Epoch 1328/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1959 - accuracy: 0.1333\n",
      "Epoch 01328: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.1959 - accuracy: 0.1333 - val_loss: 2.9412 - val_accuracy: 0.2000\n",
      "Epoch 1329/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7449 - accuracy: 0.3333\n",
      "Epoch 01329: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.7449 - accuracy: 0.3333 - val_loss: 4.8138 - val_accuracy: 0.1000\n",
      "Epoch 1330/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9511 - accuracy: 0.3667\n",
      "Epoch 01330: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 127ms/step - loss: 2.9511 - accuracy: 0.3667 - val_loss: 2.8744 - val_accuracy: 0.1000\n",
      "Epoch 1331/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6681 - accuracy: 0.2333\n",
      "Epoch 01331: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.6681 - accuracy: 0.2333 - val_loss: 2.9447 - val_accuracy: 0.3000\n",
      "Epoch 1332/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5520 - accuracy: 0.2667\n",
      "Epoch 01332: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.5520 - accuracy: 0.2667 - val_loss: 3.4222 - val_accuracy: 0.5000\n",
      "Epoch 1333/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8644 - accuracy: 0.3667\n",
      "Epoch 01333: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 2.8644 - accuracy: 0.3667 - val_loss: 1.9443 - val_accuracy: 0.4000\n",
      "Epoch 1334/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3730 - accuracy: 0.2667\n",
      "Epoch 01334: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 3.3730 - accuracy: 0.2667 - val_loss: 1.7331 - val_accuracy: 0.4000\n",
      "Epoch 1335/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7817 - accuracy: 0.2333\n",
      "Epoch 01335: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 3.7817 - accuracy: 0.2333 - val_loss: 2.8892 - val_accuracy: 0.4000\n",
      "Epoch 1336/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8465 - accuracy: 0.2000\n",
      "Epoch 01336: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.8465 - accuracy: 0.2000 - val_loss: 2.5419 - val_accuracy: 0.4000\n",
      "Epoch 1337/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0069 - accuracy: 0.4333\n",
      "Epoch 01337: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.0069 - accuracy: 0.4333 - val_loss: 4.5794 - val_accuracy: 0.0000e+00\n",
      "Epoch 1338/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6750 - accuracy: 0.3000\n",
      "Epoch 01338: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.6750 - accuracy: 0.3000 - val_loss: 2.5054 - val_accuracy: 0.5000\n",
      "Epoch 1339/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2160 - accuracy: 0.3000\n",
      "Epoch 01339: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.2160 - accuracy: 0.3000 - val_loss: 1.3691 - val_accuracy: 0.7000\n",
      "Epoch 1340/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0333 - accuracy: 0.3000\n",
      "Epoch 01340: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.0333 - accuracy: 0.3000 - val_loss: 1.8341 - val_accuracy: 0.3000\n",
      "Epoch 1341/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6130 - accuracy: 0.2667\n",
      "Epoch 01341: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 3.6130 - accuracy: 0.2667 - val_loss: 1.5777 - val_accuracy: 0.5000\n",
      "Epoch 1342/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7718 - accuracy: 0.1000\n",
      "Epoch 01342: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.7718 - accuracy: 0.1000 - val_loss: 1.4813 - val_accuracy: 0.7000\n",
      "Epoch 1343/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2001 - accuracy: 0.3333\n",
      "Epoch 01343: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.2001 - accuracy: 0.3333 - val_loss: 3.8822 - val_accuracy: 0.2000\n",
      "Epoch 1344/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4252 - accuracy: 0.2000\n",
      "Epoch 01344: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.4252 - accuracy: 0.2000 - val_loss: 4.3419 - val_accuracy: 0.1000\n",
      "Epoch 1345/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0039 - accuracy: 0.2333\n",
      "Epoch 01345: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 4.0039 - accuracy: 0.2333 - val_loss: 2.7872 - val_accuracy: 0.1000\n",
      "Epoch 1346/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4762 - accuracy: 0.1333\n",
      "Epoch 01346: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.4762 - accuracy: 0.1333 - val_loss: 4.0998 - val_accuracy: 0.2000\n",
      "Epoch 1347/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4037 - accuracy: 0.3333\n",
      "Epoch 01347: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.4037 - accuracy: 0.3333 - val_loss: 2.5004 - val_accuracy: 0.2000\n",
      "Epoch 1348/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2186 - accuracy: 0.3333\n",
      "Epoch 01348: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.2186 - accuracy: 0.3333 - val_loss: 2.3908 - val_accuracy: 0.0000e+00\n",
      "Epoch 1349/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1617 - accuracy: 0.3333\n",
      "Epoch 01349: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.1617 - accuracy: 0.3333 - val_loss: 3.2536 - val_accuracy: 0.5000\n",
      "Epoch 1350/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0653 - accuracy: 0.3000\n",
      "Epoch 01350: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.0653 - accuracy: 0.3000 - val_loss: 1.9291 - val_accuracy: 0.3000\n",
      "Epoch 1351/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1541 - accuracy: 0.2333\n",
      "Epoch 01351: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.1541 - accuracy: 0.2333 - val_loss: 1.9383 - val_accuracy: 0.5000\n",
      "Epoch 1352/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0361 - accuracy: 0.2333\n",
      "Epoch 01352: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.0361 - accuracy: 0.2333 - val_loss: 2.5887 - val_accuracy: 0.3000\n",
      "Epoch 1353/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9575 - accuracy: 0.3000\n",
      "Epoch 01353: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.9575 - accuracy: 0.3000 - val_loss: 2.2846 - val_accuracy: 0.4000\n",
      "Epoch 1354/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0210 - accuracy: 0.3333\n",
      "Epoch 01354: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.0210 - accuracy: 0.3333 - val_loss: 2.7833 - val_accuracy: 0.4000\n",
      "Epoch 1355/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4593 - accuracy: 0.2667\n",
      "Epoch 01355: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.4593 - accuracy: 0.2667 - val_loss: 2.8422 - val_accuracy: 0.3000\n",
      "Epoch 1356/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1748 - accuracy: 0.3333\n",
      "Epoch 01356: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.1748 - accuracy: 0.3333 - val_loss: 3.1922 - val_accuracy: 0.4000\n",
      "Epoch 1357/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3892 - accuracy: 0.2000\n",
      "Epoch 01357: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.3892 - accuracy: 0.2000 - val_loss: 3.5444 - val_accuracy: 0.2000\n",
      "Epoch 1358/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1054 - accuracy: 0.4000\n",
      "Epoch 01358: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 3.1054 - accuracy: 0.4000 - val_loss: 2.5523 - val_accuracy: 0.4000\n",
      "Epoch 1359/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7525 - accuracy: 0.4000\n",
      "Epoch 01359: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 2.7525 - accuracy: 0.4000 - val_loss: 2.5386 - val_accuracy: 0.5000\n",
      "Epoch 1360/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2180 - accuracy: 0.2000\n",
      "Epoch 01360: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.2180 - accuracy: 0.2000 - val_loss: 2.1022 - val_accuracy: 0.6000\n",
      "Epoch 1361/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5128 - accuracy: 0.2667\n",
      "Epoch 01361: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.5128 - accuracy: 0.2667 - val_loss: 5.4428 - val_accuracy: 0.2000\n",
      "Epoch 1362/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7681 - accuracy: 0.2667\n",
      "Epoch 01362: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.7681 - accuracy: 0.2667 - val_loss: 3.8486 - val_accuracy: 0.2000\n",
      "Epoch 1363/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8755 - accuracy: 0.3000\n",
      "Epoch 01363: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 146ms/step - loss: 2.8755 - accuracy: 0.3000 - val_loss: 3.1098 - val_accuracy: 0.1000\n",
      "Epoch 1364/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9380 - accuracy: 0.3333\n",
      "Epoch 01364: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.9380 - accuracy: 0.3333 - val_loss: 2.9087 - val_accuracy: 0.4000\n",
      "Epoch 1365/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8772 - accuracy: 0.1000\n",
      "Epoch 01365: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 3.8772 - accuracy: 0.1000 - val_loss: 2.9984 - val_accuracy: 0.4000\n",
      "Epoch 1366/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.3685 - accuracy: 0.4333\n",
      "Epoch 01366: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 152ms/step - loss: 2.3685 - accuracy: 0.4333 - val_loss: 1.6691 - val_accuracy: 0.6000\n",
      "Epoch 1367/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6336 - accuracy: 0.2667\n",
      "Epoch 01367: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.6336 - accuracy: 0.2667 - val_loss: 4.6851 - val_accuracy: 0.0000e+00\n",
      "Epoch 1368/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4363 - accuracy: 0.2000\n",
      "Epoch 01368: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.4363 - accuracy: 0.2000 - val_loss: 4.2465 - val_accuracy: 0.0000e+00\n",
      "Epoch 1369/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8756 - accuracy: 0.3000\n",
      "Epoch 01369: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 84ms/step - loss: 2.8756 - accuracy: 0.3000 - val_loss: 3.3686 - val_accuracy: 0.1000\n",
      "Epoch 1370/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4229 - accuracy: 0.3000\n",
      "Epoch 01370: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.4229 - accuracy: 0.3000 - val_loss: 5.8489 - val_accuracy: 0.0000e+00\n",
      "Epoch 1371/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7172 - accuracy: 0.2333\n",
      "Epoch 01371: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.7172 - accuracy: 0.2333 - val_loss: 2.9832 - val_accuracy: 0.5000\n",
      "Epoch 1372/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5820 - accuracy: 0.2667\n",
      "Epoch 01372: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.5820 - accuracy: 0.2667 - val_loss: 6.7105 - val_accuracy: 0.0000e+00\n",
      "Epoch 1373/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4402 - accuracy: 0.2333\n",
      "Epoch 01373: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.4402 - accuracy: 0.2333 - val_loss: 3.5641 - val_accuracy: 0.3000\n",
      "Epoch 1374/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4171 - accuracy: 0.3333\n",
      "Epoch 01374: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.4171 - accuracy: 0.3333 - val_loss: 5.8479 - val_accuracy: 0.1000\n",
      "Epoch 1375/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1073 - accuracy: 0.4000\n",
      "Epoch 01375: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.1073 - accuracy: 0.4000 - val_loss: 2.6252 - val_accuracy: 0.5000\n",
      "Epoch 1376/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3654 - accuracy: 0.1667\n",
      "Epoch 01376: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.3654 - accuracy: 0.1667 - val_loss: 3.3314 - val_accuracy: 0.3000\n",
      "Epoch 1377/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1482 - accuracy: 0.3000\n",
      "Epoch 01377: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.1482 - accuracy: 0.3000 - val_loss: 3.4008 - val_accuracy: 0.1000\n",
      "Epoch 1378/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0891 - accuracy: 0.3000\n",
      "Epoch 01378: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0891 - accuracy: 0.3000 - val_loss: 5.0530 - val_accuracy: 0.1000\n",
      "Epoch 1379/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1033 - accuracy: 0.3333\n",
      "Epoch 01379: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.1033 - accuracy: 0.3333 - val_loss: 3.5494 - val_accuracy: 0.2000\n",
      "Epoch 1380/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2350 - accuracy: 0.3000\n",
      "Epoch 01380: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.2350 - accuracy: 0.3000 - val_loss: 4.3284 - val_accuracy: 0.1000\n",
      "Epoch 1381/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1893 - accuracy: 0.2667\n",
      "Epoch 01381: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.1893 - accuracy: 0.2667 - val_loss: 1.5964 - val_accuracy: 0.7000\n",
      "Epoch 1382/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6430 - accuracy: 0.3667\n",
      "Epoch 01382: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 2.6430 - accuracy: 0.3667 - val_loss: 2.1958 - val_accuracy: 0.4000\n",
      "Epoch 1383/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4246 - accuracy: 0.3000\n",
      "Epoch 01383: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.4246 - accuracy: 0.3000 - val_loss: 4.4879 - val_accuracy: 0.0000e+00\n",
      "Epoch 1384/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5326 - accuracy: 0.2667\n",
      "Epoch 01384: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.5326 - accuracy: 0.2667 - val_loss: 2.6655 - val_accuracy: 0.0000e+00\n",
      "Epoch 1385/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8701 - accuracy: 0.1667\n",
      "Epoch 01385: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.8701 - accuracy: 0.1667 - val_loss: 4.0273 - val_accuracy: 0.0000e+00\n",
      "Epoch 1386/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7459 - accuracy: 0.4667\n",
      "Epoch 01386: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.7459 - accuracy: 0.4667 - val_loss: 3.9360 - val_accuracy: 0.0000e+00\n",
      "Epoch 1387/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9120 - accuracy: 0.3333\n",
      "Epoch 01387: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.9120 - accuracy: 0.3333 - val_loss: 4.3137 - val_accuracy: 0.0000e+00\n",
      "Epoch 1388/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5340 - accuracy: 0.1000\n",
      "Epoch 01388: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.5340 - accuracy: 0.1000 - val_loss: 5.1273 - val_accuracy: 0.0000e+00\n",
      "Epoch 1389/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2542 - accuracy: 0.3000\n",
      "Epoch 01389: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.2542 - accuracy: 0.3000 - val_loss: 5.2634 - val_accuracy: 0.2000\n",
      "Epoch 1390/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5174 - accuracy: 0.2000\n",
      "Epoch 01390: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.5174 - accuracy: 0.2000 - val_loss: 3.7981 - val_accuracy: 0.2000\n",
      "Epoch 1391/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.1933 - accuracy: 0.5667\n",
      "Epoch 01391: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 2.1933 - accuracy: 0.5667 - val_loss: 3.9722 - val_accuracy: 0.1000\n",
      "Epoch 1392/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3265 - accuracy: 0.2667\n",
      "Epoch 01392: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.3265 - accuracy: 0.2667 - val_loss: 4.4996 - val_accuracy: 0.0000e+00\n",
      "Epoch 1393/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1997 - accuracy: 0.2667\n",
      "Epoch 01393: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 3.1997 - accuracy: 0.2667 - val_loss: 4.0377 - val_accuracy: 0.0000e+00\n",
      "Epoch 1394/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7893 - accuracy: 0.1000\n",
      "Epoch 01394: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 3.7893 - accuracy: 0.1000 - val_loss: 5.1246 - val_accuracy: 0.0000e+00\n",
      "Epoch 1395/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1702 - accuracy: 0.2333\n",
      "Epoch 01395: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.1702 - accuracy: 0.2333 - val_loss: 4.2089 - val_accuracy: 0.0000e+00\n",
      "Epoch 1396/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1105 - accuracy: 0.2667\n",
      "Epoch 01396: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.1105 - accuracy: 0.2667 - val_loss: 1.1222 - val_accuracy: 0.8000\n",
      "Epoch 1397/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3675 - accuracy: 0.3000\n",
      "Epoch 01397: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.3675 - accuracy: 0.3000 - val_loss: 3.2820 - val_accuracy: 0.3000\n",
      "Epoch 1398/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6815 - accuracy: 0.2667\n",
      "Epoch 01398: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.6815 - accuracy: 0.2667 - val_loss: 3.2873 - val_accuracy: 0.3000\n",
      "Epoch 1399/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.5727 - accuracy: 0.3667\n",
      "Epoch 01399: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.5727 - accuracy: 0.3667 - val_loss: 3.4955 - val_accuracy: 0.2000\n",
      "Epoch 1400/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7648 - accuracy: 0.1667\n",
      "Epoch 01400: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 3.7648 - accuracy: 0.1667 - val_loss: 5.1956 - val_accuracy: 0.0000e+00\n",
      "Epoch 1401/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0168 - accuracy: 0.2333\n",
      "Epoch 01401: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.0168 - accuracy: 0.2333 - val_loss: 3.6842 - val_accuracy: 0.3000\n",
      "Epoch 1402/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5124 - accuracy: 0.1333\n",
      "Epoch 01402: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 3.5124 - accuracy: 0.1333 - val_loss: 3.4031 - val_accuracy: 0.3000\n",
      "Epoch 1403/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0359 - accuracy: 0.2667\n",
      "Epoch 01403: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.0359 - accuracy: 0.2667 - val_loss: 4.1444 - val_accuracy: 0.1000\n",
      "Epoch 1404/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1218 - accuracy: 0.2000\n",
      "Epoch 01404: val_loss did not improve from 0.01018\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.1218 - accuracy: 0.2000 - val_loss: 3.6642 - val_accuracy: 0.3000\n",
      "Epoch 1405/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4917 - accuracy: 0.2333\n",
      "Epoch 01405: val_loss improved from 0.01018 to 0.00079, saving model to face_recog_vgg.h5\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 3.4917 - accuracy: 0.2333 - val_loss: 7.8550e-04 - val_accuracy: 1.0000\n",
      "Epoch 1406/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8274 - accuracy: 0.3333\n",
      "Epoch 01406: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 2.8274 - accuracy: 0.3333 - val_loss: 2.8880 - val_accuracy: 0.1000\n",
      "Epoch 1407/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1112 - accuracy: 0.3667\n",
      "Epoch 01407: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 3.1112 - accuracy: 0.3667 - val_loss: 2.0082 - val_accuracy: 0.3000\n",
      "Epoch 1408/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4758 - accuracy: 0.3333\n",
      "Epoch 01408: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 3.4758 - accuracy: 0.3333 - val_loss: 1.4931 - val_accuracy: 0.7000\n",
      "Epoch 1409/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5255 - accuracy: 0.2667\n",
      "Epoch 01409: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 87ms/step - loss: 3.5255 - accuracy: 0.2667 - val_loss: 2.2372 - val_accuracy: 0.6000\n",
      "Epoch 1410/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6130 - accuracy: 0.1667\n",
      "Epoch 01410: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.6130 - accuracy: 0.1667 - val_loss: 5.7499 - val_accuracy: 0.0000e+00\n",
      "Epoch 1411/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7123 - accuracy: 0.2333\n",
      "Epoch 01411: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 2.7123 - accuracy: 0.2333 - val_loss: 5.0641 - val_accuracy: 0.0000e+00\n",
      "Epoch 1412/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3841 - accuracy: 0.2333\n",
      "Epoch 01412: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.3841 - accuracy: 0.2333 - val_loss: 2.3340 - val_accuracy: 0.5000\n",
      "Epoch 1413/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9705 - accuracy: 0.3667\n",
      "Epoch 01413: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 2.9705 - accuracy: 0.3667 - val_loss: 3.8648 - val_accuracy: 0.1000\n",
      "Epoch 1414/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7456 - accuracy: 0.2333\n",
      "Epoch 01414: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 2.7456 - accuracy: 0.2333 - val_loss: 3.3982 - val_accuracy: 0.4000\n",
      "Epoch 1415/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2538 - accuracy: 0.2000\n",
      "Epoch 01415: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.2538 - accuracy: 0.2000 - val_loss: 3.8929 - val_accuracy: 0.1000\n",
      "Epoch 1416/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9624 - accuracy: 0.3333\n",
      "Epoch 01416: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.9624 - accuracy: 0.3333 - val_loss: 3.6162 - val_accuracy: 0.0000e+00\n",
      "Epoch 1417/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0976 - accuracy: 0.2000\n",
      "Epoch 01417: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.0976 - accuracy: 0.2000 - val_loss: 5.3836 - val_accuracy: 0.0000e+00\n",
      "Epoch 1418/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8418 - accuracy: 0.1333\n",
      "Epoch 01418: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 2.8418 - accuracy: 0.1333 - val_loss: 2.9329 - val_accuracy: 0.1000\n",
      "Epoch 1419/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9228 - accuracy: 0.3000\n",
      "Epoch 01419: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 2.9228 - accuracy: 0.3000 - val_loss: 3.2754 - val_accuracy: 0.3000\n",
      "Epoch 1420/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0473 - accuracy: 0.2333\n",
      "Epoch 01420: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.0473 - accuracy: 0.2333 - val_loss: 2.2997 - val_accuracy: 0.6000\n",
      "Epoch 1421/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6876 - accuracy: 0.4667\n",
      "Epoch 01421: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 2.6876 - accuracy: 0.4667 - val_loss: 4.5131 - val_accuracy: 0.0000e+00\n",
      "Epoch 1422/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7296 - accuracy: 0.1667\n",
      "Epoch 01422: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 3.7296 - accuracy: 0.1667 - val_loss: 7.5709 - val_accuracy: 0.0000e+00\n",
      "Epoch 1423/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.9697 - accuracy: 0.2333\n",
      "Epoch 01423: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 2.9697 - accuracy: 0.2333 - val_loss: 5.8056 - val_accuracy: 0.0000e+00\n",
      "Epoch 1424/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3546 - accuracy: 0.2000\n",
      "Epoch 01424: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 3.3546 - accuracy: 0.2000 - val_loss: 4.5784 - val_accuracy: 0.1000\n",
      "Epoch 1425/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5685 - accuracy: 0.3333\n",
      "Epoch 01425: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.5685 - accuracy: 0.3333 - val_loss: 5.9060 - val_accuracy: 0.0000e+00\n",
      "Epoch 1426/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3011 - accuracy: 0.2667\n",
      "Epoch 01426: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.3011 - accuracy: 0.2667 - val_loss: 3.0557 - val_accuracy: 0.1000\n",
      "Epoch 1427/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5429 - accuracy: 0.3333\n",
      "Epoch 01427: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 3.5429 - accuracy: 0.3333 - val_loss: 2.8778 - val_accuracy: 0.4000\n",
      "Epoch 1428/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7121 - accuracy: 0.0667    \n",
      "Epoch 01428: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 118ms/step - loss: 3.7121 - accuracy: 0.0667 - val_loss: 2.8479 - val_accuracy: 0.2000\n",
      "Epoch 1429/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2470 - accuracy: 0.2667\n",
      "Epoch 01429: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 3.2470 - accuracy: 0.2667 - val_loss: 2.3087 - val_accuracy: 0.4000\n",
      "Epoch 1430/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9780 - accuracy: 0.2000\n",
      "Epoch 01430: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 3.9780 - accuracy: 0.2000 - val_loss: 3.7842 - val_accuracy: 0.3000\n",
      "Epoch 1431/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1851 - accuracy: 0.1667\n",
      "Epoch 01431: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 3.1851 - accuracy: 0.1667 - val_loss: 2.3316 - val_accuracy: 0.5000\n",
      "Epoch 1432/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1351 - accuracy: 0.3000\n",
      "Epoch 01432: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 128ms/step - loss: 3.1351 - accuracy: 0.3000 - val_loss: 4.9690 - val_accuracy: 0.0000e+00\n",
      "Epoch 1433/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.8436 - accuracy: 0.1667\n",
      "Epoch 01433: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 126ms/step - loss: 3.8436 - accuracy: 0.1667 - val_loss: 4.0562 - val_accuracy: 0.1000\n",
      "Epoch 1434/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4603 - accuracy: 0.2667\n",
      "Epoch 01434: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 125ms/step - loss: 3.4603 - accuracy: 0.2667 - val_loss: 3.9434 - val_accuracy: 0.0000e+00\n",
      "Epoch 1435/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1386 - accuracy: 0.2667\n",
      "Epoch 01435: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.1386 - accuracy: 0.2667 - val_loss: 4.2539 - val_accuracy: 0.0000e+00\n",
      "Epoch 1436/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6590 - accuracy: 0.1333\n",
      "Epoch 01436: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.6590 - accuracy: 0.1333 - val_loss: 2.9269 - val_accuracy: 0.3000\n",
      "Epoch 1437/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2264 - accuracy: 0.3000\n",
      "Epoch 01437: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.2264 - accuracy: 0.3000 - val_loss: 4.0021 - val_accuracy: 0.0000e+00\n",
      "Epoch 1438/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.1360 - accuracy: 0.0333\n",
      "Epoch 01438: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 4.1360 - accuracy: 0.0333 - val_loss: 4.3912 - val_accuracy: 0.1000\n",
      "Epoch 1439/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5940 - accuracy: 0.1000\n",
      "Epoch 01439: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 3.5940 - accuracy: 0.1000 - val_loss: 5.9708 - val_accuracy: 0.0000e+00\n",
      "Epoch 1440/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1394 - accuracy: 0.3000\n",
      "Epoch 01440: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 3.1394 - accuracy: 0.3000 - val_loss: 6.0930 - val_accuracy: 0.0000e+00\n",
      "Epoch 1441/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7575 - accuracy: 0.2333\n",
      "Epoch 01441: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 2.7575 - accuracy: 0.2333 - val_loss: 5.4595 - val_accuracy: 0.0000e+00\n",
      "Epoch 1442/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2646 - accuracy: 0.3000\n",
      "Epoch 01442: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 3.2646 - accuracy: 0.3000 - val_loss: 1.5616 - val_accuracy: 0.7000\n",
      "Epoch 1443/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8275 - accuracy: 0.4000\n",
      "Epoch 01443: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 2.8275 - accuracy: 0.4000 - val_loss: 1.5567 - val_accuracy: 0.6000\n",
      "Epoch 1444/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8704 - accuracy: 0.2333\n",
      "Epoch 01444: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 2.8704 - accuracy: 0.2333 - val_loss: 4.2811 - val_accuracy: 0.1000\n",
      "Epoch 1445/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5193 - accuracy: 0.2333\n",
      "Epoch 01445: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.5193 - accuracy: 0.2333 - val_loss: 2.4820 - val_accuracy: 0.6000\n",
      "Epoch 1446/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 4.0778 - accuracy: 0.2000\n",
      "Epoch 01446: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 4.0778 - accuracy: 0.2000 - val_loss: 2.7144 - val_accuracy: 0.7000\n",
      "Epoch 1447/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1228 - accuracy: 0.2000\n",
      "Epoch 01447: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 3.1228 - accuracy: 0.2000 - val_loss: 4.5191 - val_accuracy: 0.0000e+00\n",
      "Epoch 1448/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1494 - accuracy: 0.2667\n",
      "Epoch 01448: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.1494 - accuracy: 0.2667 - val_loss: 6.6587 - val_accuracy: 0.0000e+00\n",
      "Epoch 1449/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1112 - accuracy: 0.1667\n",
      "Epoch 01449: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.1112 - accuracy: 0.1667 - val_loss: 4.2681 - val_accuracy: 0.0000e+00\n",
      "Epoch 1450/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7538 - accuracy: 0.4000\n",
      "Epoch 01450: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 2.7538 - accuracy: 0.4000 - val_loss: 3.8507 - val_accuracy: 0.2000\n",
      "Epoch 1451/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8083 - accuracy: 0.3667\n",
      "Epoch 01451: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 2.8083 - accuracy: 0.3667 - val_loss: 2.7023 - val_accuracy: 0.3000\n",
      "Epoch 1452/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.1696 - accuracy: 0.2000\n",
      "Epoch 01452: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 3.1696 - accuracy: 0.2000 - val_loss: 4.9596 - val_accuracy: 0.2000\n",
      "Epoch 1453/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7514 - accuracy: 0.2667\n",
      "Epoch 01453: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 3.7514 - accuracy: 0.2667 - val_loss: 3.1072 - val_accuracy: 0.2000\n",
      "Epoch 1454/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2759 - accuracy: 0.2667\n",
      "Epoch 01454: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.2759 - accuracy: 0.2667 - val_loss: 2.6884 - val_accuracy: 0.3000\n",
      "Epoch 1455/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2747 - accuracy: 0.2667\n",
      "Epoch 01455: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 3.2747 - accuracy: 0.2667 - val_loss: 2.1944 - val_accuracy: 0.5000\n",
      "Epoch 1456/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4185 - accuracy: 0.2667\n",
      "Epoch 01456: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.4185 - accuracy: 0.2667 - val_loss: 2.3157 - val_accuracy: 0.3000\n",
      "Epoch 1457/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.5912 - accuracy: 0.2000\n",
      "Epoch 01457: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 3.5912 - accuracy: 0.2000 - val_loss: 3.9797 - val_accuracy: 0.1000\n",
      "Epoch 1458/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.4096 - accuracy: 0.3000\n",
      "Epoch 01458: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 122ms/step - loss: 3.4096 - accuracy: 0.3000 - val_loss: 2.5918 - val_accuracy: 0.4000\n",
      "Epoch 1459/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7807 - accuracy: 0.1667\n",
      "Epoch 01459: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 3.7807 - accuracy: 0.1667 - val_loss: 2.5261 - val_accuracy: 0.5000\n",
      "Epoch 1460/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7320 - accuracy: 0.1667\n",
      "Epoch 01460: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 246ms/step - loss: 3.7320 - accuracy: 0.1667 - val_loss: 3.4399 - val_accuracy: 0.5000\n",
      "Epoch 1461/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.2419 - accuracy: 0.3000\n",
      "Epoch 01461: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 184ms/step - loss: 3.2419 - accuracy: 0.3000 - val_loss: 4.3707 - val_accuracy: 0.0000e+00\n",
      "Epoch 1462/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0457 - accuracy: 0.2667\n",
      "Epoch 01462: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 198ms/step - loss: 3.0457 - accuracy: 0.2667 - val_loss: 0.2099 - val_accuracy: 0.9000\n",
      "Epoch 1463/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6619 - accuracy: 0.3333\n",
      "Epoch 01463: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 195ms/step - loss: 3.6619 - accuracy: 0.3333 - val_loss: 3.3404 - val_accuracy: 0.5000\n",
      "Epoch 1464/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.2926 - accuracy: 0.3333\n",
      "Epoch 01464: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 190ms/step - loss: 2.2926 - accuracy: 0.3333 - val_loss: 1.7458 - val_accuracy: 0.7000\n",
      "Epoch 1465/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.0757 - accuracy: 0.3000\n",
      "Epoch 01465: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 3.0757 - accuracy: 0.3000 - val_loss: 0.2719 - val_accuracy: 1.0000\n",
      "Epoch 1466/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7942 - accuracy: 0.1667\n",
      "Epoch 01466: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 183ms/step - loss: 3.7942 - accuracy: 0.1667 - val_loss: 1.7002 - val_accuracy: 0.5000\n",
      "Epoch 1467/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.7622 - accuracy: 0.2333\n",
      "Epoch 01467: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 180ms/step - loss: 3.7622 - accuracy: 0.2333 - val_loss: 4.0264 - val_accuracy: 0.0000e+00\n",
      "Epoch 1468/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.6326 - accuracy: 0.2333\n",
      "Epoch 01468: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 170ms/step - loss: 3.6326 - accuracy: 0.2333 - val_loss: 2.3892 - val_accuracy: 0.5000\n",
      "Epoch 1469/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3090 - accuracy: 0.2667\n",
      "Epoch 01469: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 1s 377ms/step - loss: 3.3090 - accuracy: 0.2667 - val_loss: 1.9364 - val_accuracy: 0.5000\n",
      "Epoch 1470/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.6167 - accuracy: 0.3000\n",
      "Epoch 01470: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 2.6167 - accuracy: 0.3000 - val_loss: 2.8578 - val_accuracy: 0.4000\n",
      "Epoch 1471/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.9978 - accuracy: 0.1333\n",
      "Epoch 01471: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 182ms/step - loss: 3.9978 - accuracy: 0.1333 - val_loss: 3.9536 - val_accuracy: 0.0000e+00\n",
      "Epoch 1472/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 3.3623 - accuracy: 0.2000\n",
      "Epoch 01472: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 161ms/step - loss: 3.3623 - accuracy: 0.2000 - val_loss: 2.6275 - val_accuracy: 0.4000\n",
      "Epoch 1473/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8872 - accuracy: 0.3333\n",
      "Epoch 01473: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 2.8872 - accuracy: 0.3333 - val_loss: 2.3878 - val_accuracy: 0.6000\n",
      "Epoch 1474/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.8698 - accuracy: 0.2667\n",
      "Epoch 01474: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 1s 270ms/step - loss: 2.8698 - accuracy: 0.2667 - val_loss: 1.1830 - val_accuracy: 0.7000\n",
      "Epoch 1475/10000\n",
      "2/2 [==============================] - ETA: 0s - loss: 2.7906 - accuracy: 0.2667\n",
      "Epoch 01475: val_loss did not improve from 0.00079\n",
      "2/2 [==============================] - 0s 159ms/step - loss: 2.7906 - accuracy: 0.2667 - val_loss: 6.1352 - val_accuracy: 0.0000e+00\n",
      "Epoch 1476/10000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 3.6444 - accuracy: 0.3333"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-0ee4a0cf502e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                  \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                                  validation_steps=nb_validation_samples // batch_size)\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mmodelnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"face_recog_vgg.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1665\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1667\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"face_recog_vgg.h5\", monitor=\"val_loss\", mode=\"min\", save_best_only = True, verbose=1)\n",
    "# earlystop = EarlyStopping(monitor= 'val_loss', min_delta = 0, patience = 3, verbose = 1, restore_best_weights = True)\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "modelnew.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.001),metrics=['accuracy'])\n",
    "\n",
    "nb_train_samples=180\n",
    "nb_validation_samples=100 \n",
    "epochs=10000\n",
    "batch_size=64\n",
    "\n",
    "history = modelnew.fit(train_generator,\n",
    "                                 steps_per_epoch=nb_train_samples // batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 callbacks=callbacks,\n",
    "                                 validation_data=validation_generator,\n",
    "                                 validation_steps=nb_validation_samples // batch_size)\n",
    "modelnew.save(\"face_recog_vgg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-01-07T12:21:26.703079</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 277.314375 \r\nL 385.78125 277.314375 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\nL 43.78125 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m519d9c70a4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.999432\" xlink:href=\"#m519d9c70a4\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(55.818182 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.933093\" xlink:href=\"#m519d9c70a4\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(110.389343 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.866754\" xlink:href=\"#m519d9c70a4\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(171.323004 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.800415\" xlink:href=\"#m519d9c70a4\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(232.256665 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.734076\" xlink:href=\"#m519d9c70a4\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(293.190326 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.667736\" xlink:href=\"#m519d9c70a4\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(350.942736 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- epoch -->\r\n     <g transform=\"translate(195.953125 268.034687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mdc14aca7cd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mdc14aca7cd\" y=\"229.874489\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.0 -->\r\n      <g transform=\"translate(20.878125 233.673707)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mdc14aca7cd\" y=\"190.339943\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(20.878125 194.139162)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mdc14aca7cd\" y=\"150.805398\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(20.878125 154.604616)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mdc14aca7cd\" y=\"111.270852\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(20.878125 115.070071)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mdc14aca7cd\" y=\"71.736307\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(20.878125 75.535526)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mdc14aca7cd\" y=\"32.201761\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(20.878125 36.00098)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- accuracy -->\r\n     <g transform=\"translate(14.798438 153.5975)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p833c1f4436)\" d=\"M 58.999432 216.696306 \r\nL 59.3041 196.929033 \r\nL 59.913437 210.107216 \r\nL 60.218105 183.750852 \r\nL 60.827442 210.107216 \r\nL 61.13211 196.929033 \r\nL 61.741447 210.107216 \r\nL 62.046115 196.929033 \r\nL 62.350783 196.929033 \r\nL 62.655451 190.339943 \r\nL 62.96012 210.107216 \r\nL 63.264788 203.518124 \r\nL 63.569456 190.339943 \r\nL 63.874125 216.696306 \r\nL 64.178793 210.107216 \r\nL 64.483461 216.696306 \r\nL 64.78813 183.750852 \r\nL 65.092798 190.339943 \r\nL 65.397466 183.750852 \r\nL 65.702135 190.339943 \r\nL 66.006803 177.161759 \r\nL 66.311471 203.518124 \r\nL 66.616139 190.339943 \r\nL 66.920808 190.339943 \r\nL 67.834813 170.572668 \r\nL 68.139481 210.107216 \r\nL 68.748818 210.107216 \r\nL 69.053486 203.518124 \r\nL 69.358154 216.696306 \r\nL 69.967491 183.750852 \r\nL 70.272159 216.696306 \r\nL 70.576827 190.339943 \r\nL 70.881496 216.696306 \r\nL 71.186164 203.518124 \r\nL 71.490832 223.285397 \r\nL 71.795501 203.518124 \r\nL 72.100169 196.929033 \r\nL 72.404837 216.696306 \r\nL 72.709506 203.518124 \r\nL 73.014174 196.929033 \r\nL 73.318842 196.929033 \r\nL 73.62351 177.161759 \r\nL 73.928179 203.518124 \r\nL 74.232847 203.518124 \r\nL 74.537515 216.696306 \r\nL 75.146852 190.339943 \r\nL 75.756189 177.161759 \r\nL 76.060857 210.107216 \r\nL 76.365525 196.929033 \r\nL 76.670193 216.696306 \r\nL 76.974862 196.929033 \r\nL 77.27953 190.339943 \r\nL 77.584198 177.161759 \r\nL 77.888867 203.518124 \r\nL 78.193535 203.518124 \r\nL 78.498203 183.750852 \r\nL 78.802872 183.750852 \r\nL 79.10754 177.161759 \r\nL 79.412208 216.696306 \r\nL 79.716877 190.339943 \r\nL 80.021545 190.339943 \r\nL 80.326213 203.518124 \r\nL 80.630881 190.339943 \r\nL 80.93555 203.518124 \r\nL 81.240218 190.339943 \r\nL 81.544886 190.339943 \r\nL 81.849555 203.518124 \r\nL 82.154223 196.929033 \r\nL 82.458891 196.929033 \r\nL 82.76356 210.107216 \r\nL 83.068228 203.518124 \r\nL 83.372896 203.518124 \r\nL 83.677564 210.107216 \r\nL 83.982233 190.339943 \r\nL 84.286901 203.518124 \r\nL 84.591569 196.929033 \r\nL 84.896238 203.518124 \r\nL 85.200906 216.696306 \r\nL 85.810243 216.696306 \r\nL 86.114911 190.339943 \r\nL 86.419579 196.929033 \r\nL 86.724248 183.750852 \r\nL 87.028916 203.518124 \r\nL 87.333584 196.929033 \r\nL 87.638252 210.107216 \r\nL 87.942921 183.750852 \r\nL 88.247589 196.929033 \r\nL 88.552257 203.518124 \r\nL 88.856926 203.518124 \r\nL 89.161594 216.696306 \r\nL 89.466262 203.518124 \r\nL 89.770931 203.518124 \r\nL 90.380267 190.339943 \r\nL 90.684936 203.518124 \r\nL 90.989604 163.983578 \r\nL 91.294272 170.572668 \r\nL 91.59894 163.983578 \r\nL 91.903609 203.518124 \r\nL 92.208277 190.339943 \r\nL 92.817614 190.339943 \r\nL 93.122282 216.696306 \r\nL 93.42695 183.750852 \r\nL 93.731619 183.750852 \r\nL 94.036287 203.518124 \r\nL 94.340955 170.572668 \r\nL 94.645623 210.107216 \r\nL 94.950292 196.929033 \r\nL 95.25496 190.339943 \r\nL 95.559628 196.929033 \r\nL 95.864297 210.107216 \r\nL 96.168965 183.750852 \r\nL 96.473633 203.518124 \r\nL 96.778302 203.518124 \r\nL 97.08297 210.107216 \r\nL 97.387638 210.107216 \r\nL 97.692307 190.339943 \r\nL 97.996975 196.929033 \r\nL 98.301643 210.107216 \r\nL 98.606311 190.339943 \r\nL 98.91098 183.750852 \r\nL 99.215648 190.339943 \r\nL 99.520316 210.107216 \r\nL 99.824985 190.339943 \r\nL 100.129653 210.107216 \r\nL 100.434321 196.929033 \r\nL 101.348326 216.696306 \r\nL 101.957663 183.750852 \r\nL 102.262331 190.339943 \r\nL 102.566999 190.339943 \r\nL 103.176336 203.518124 \r\nL 103.481004 216.696306 \r\nL 103.785673 177.161759 \r\nL 104.090341 196.929033 \r\nL 104.395009 190.339943 \r\nL 104.699678 203.518124 \r\nL 105.004346 203.518124 \r\nL 105.309014 190.339943 \r\nL 105.613682 196.929033 \r\nL 105.918351 210.107216 \r\nL 106.223019 183.750852 \r\nL 106.527687 177.161759 \r\nL 106.832356 190.339943 \r\nL 107.137024 196.929033 \r\nL 107.746361 170.572668 \r\nL 108.051029 196.929033 \r\nL 108.355697 203.518124 \r\nL 108.660365 183.750852 \r\nL 108.965034 203.518124 \r\nL 109.269702 190.339943 \r\nL 109.57437 210.107216 \r\nL 109.879039 210.107216 \r\nL 110.183707 163.983578 \r\nL 110.793044 216.696306 \r\nL 111.097712 163.983578 \r\nL 111.707049 210.107216 \r\nL 112.011717 190.339943 \r\nL 112.316385 216.696306 \r\nL 112.621053 177.161759 \r\nL 112.925722 203.518124 \r\nL 113.23039 210.107216 \r\nL 113.535058 177.161759 \r\nL 113.839727 177.161759 \r\nL 114.144395 210.107216 \r\nL 114.449063 210.107216 \r\nL 115.0584 183.750852 \r\nL 115.363068 223.285397 \r\nL 115.667736 216.696306 \r\nL 115.972405 183.750852 \r\nL 116.277073 196.929033 \r\nL 116.581741 196.929033 \r\nL 116.88641 203.518124 \r\nL 117.191078 203.518124 \r\nL 117.495746 190.339943 \r\nL 117.800415 216.696306 \r\nL 118.105083 190.339943 \r\nL 118.409751 196.929033 \r\nL 118.71442 229.874489 \r\nL 119.019088 203.518124 \r\nL 119.323756 210.107216 \r\nL 119.628424 196.929033 \r\nL 119.933093 196.929033 \r\nL 120.237761 203.518124 \r\nL 120.542429 196.929033 \r\nL 120.847098 177.161759 \r\nL 121.456434 223.285397 \r\nL 121.761103 196.929033 \r\nL 122.065771 216.696306 \r\nL 122.370439 190.339943 \r\nL 122.979776 203.518124 \r\nL 123.284444 190.339943 \r\nL 123.589112 216.696306 \r\nL 123.893781 196.929033 \r\nL 124.198449 203.518124 \r\nL 124.503117 196.929033 \r\nL 124.807786 183.750852 \r\nL 125.112454 216.696306 \r\nL 125.417122 203.518124 \r\nL 126.026459 203.518124 \r\nL 126.331127 183.750852 \r\nL 126.635795 203.518124 \r\nL 126.940464 210.107216 \r\nL 127.245132 203.518124 \r\nL 127.5498 216.696306 \r\nL 127.854469 210.107216 \r\nL 128.159137 216.696306 \r\nL 128.463805 196.929033 \r\nL 128.768474 210.107216 \r\nL 129.073142 177.161759 \r\nL 129.37781 170.572668 \r\nL 129.682479 203.518124 \r\nL 129.987147 183.750852 \r\nL 130.291815 196.929033 \r\nL 130.596483 183.750852 \r\nL 130.901152 203.518124 \r\nL 131.20582 203.518124 \r\nL 131.510488 170.572668 \r\nL 131.815157 177.161759 \r\nL 132.119825 210.107216 \r\nL 132.424493 196.929033 \r\nL 132.729162 170.572668 \r\nL 133.03383 210.107216 \r\nL 133.643166 183.750852 \r\nL 134.252503 210.107216 \r\nL 134.557171 210.107216 \r\nL 134.86184 190.339943 \r\nL 135.166508 203.518124 \r\nL 135.471176 196.929033 \r\nL 135.775845 183.750852 \r\nL 136.080513 210.107216 \r\nL 136.385181 170.572668 \r\nL 136.994518 216.696306 \r\nL 137.299186 203.518124 \r\nL 138.213191 203.518124 \r\nL 138.822528 190.339943 \r\nL 139.127196 177.161759 \r\nL 139.431864 210.107216 \r\nL 139.736533 203.518124 \r\nL 140.041201 207.910852 \r\nL 140.345869 177.161759 \r\nL 140.650537 203.518124 \r\nL 140.955206 177.161759 \r\nL 141.259874 203.518124 \r\nL 141.869211 177.161759 \r\nL 142.173879 190.339943 \r\nL 142.478547 183.750852 \r\nL 142.783216 190.339943 \r\nL 143.087884 157.394487 \r\nL 143.392552 216.696306 \r\nL 143.697221 210.107216 \r\nL 144.001889 190.339943 \r\nL 144.306557 196.929033 \r\nL 144.611225 177.161759 \r\nL 144.915894 170.572668 \r\nL 145.220562 203.518124 \r\nL 145.52523 150.805397 \r\nL 145.829899 190.339943 \r\nL 146.439235 203.518124 \r\nL 147.048572 170.572668 \r\nL 147.35324 196.929033 \r\nL 147.657908 183.750852 \r\nL 147.962577 196.929033 \r\nL 148.267245 183.750852 \r\nL 148.571913 190.339943 \r\nL 148.876582 203.518124 \r\nL 149.18125 190.339943 \r\nL 149.485918 190.339943 \r\nL 149.790587 196.929033 \r\nL 150.095255 183.750852 \r\nL 150.399923 203.518124 \r\nL 150.704592 190.339943 \r\nL 151.00926 190.339943 \r\nL 151.313928 183.750852 \r\nL 151.618596 210.107216 \r\nL 151.923265 190.339943 \r\nL 152.227933 190.339943 \r\nL 152.532601 170.572668 \r\nL 152.83727 170.572668 \r\nL 153.141938 216.696306 \r\nL 153.446606 203.518124 \r\nL 153.751275 177.161759 \r\nL 154.360611 190.339943 \r\nL 154.665279 203.518124 \r\nL 154.969948 190.339943 \r\nL 155.274616 196.929033 \r\nL 155.579284 196.929033 \r\nL 155.883953 203.518124 \r\nL 156.188621 203.518124 \r\nL 156.797958 150.805397 \r\nL 157.102626 177.161759 \r\nL 157.407294 183.750852 \r\nL 157.711963 196.929033 \r\nL 158.016631 196.929033 \r\nL 158.321299 190.339943 \r\nL 158.625967 177.161759 \r\nL 158.930636 177.161759 \r\nL 159.235304 203.518124 \r\nL 159.539972 210.107216 \r\nL 160.149309 177.161759 \r\nL 160.453977 190.339943 \r\nL 160.758646 190.339943 \r\nL 161.063314 170.572668 \r\nL 161.367982 203.518124 \r\nL 161.67265 190.339943 \r\nL 161.977319 183.750852 \r\nL 162.281987 190.339943 \r\nL 162.586655 210.107216 \r\nL 162.891324 190.339943 \r\nL 163.195992 203.518124 \r\nL 164.109997 163.983578 \r\nL 164.414665 190.339943 \r\nL 164.719334 183.750852 \r\nL 165.024002 196.929033 \r\nL 165.32867 190.339943 \r\nL 165.633338 210.107216 \r\nL 165.938007 196.929033 \r\nL 166.242675 196.929033 \r\nL 166.547343 210.107216 \r\nL 166.852012 157.394487 \r\nL 167.15668 177.161759 \r\nL 167.461348 210.107216 \r\nL 167.766017 163.983578 \r\nL 168.070685 183.750852 \r\nL 168.375353 190.339943 \r\nL 168.680021 216.696306 \r\nL 168.98469 216.696306 \r\nL 169.289358 203.518124 \r\nL 169.594026 183.750852 \r\nL 169.898695 203.518124 \r\nL 170.203363 177.161759 \r\nL 170.508031 177.161759 \r\nL 170.8127 203.518124 \r\nL 171.117368 177.161759 \r\nL 171.422036 183.750852 \r\nL 171.726705 183.750852 \r\nL 172.031373 210.107216 \r\nL 172.336041 203.518124 \r\nL 172.640709 183.750852 \r\nL 172.945378 210.107216 \r\nL 173.250046 177.161759 \r\nL 173.554714 177.161759 \r\nL 173.859383 190.339943 \r\nL 174.773388 190.339943 \r\nL 175.078056 183.750852 \r\nL 175.382724 157.394487 \r\nL 175.687393 190.339943 \r\nL 175.992061 177.161759 \r\nL 176.601397 210.107216 \r\nL 176.906066 210.107216 \r\nL 177.210734 170.572668 \r\nL 177.515402 163.983578 \r\nL 177.820071 196.929033 \r\nL 178.124739 177.161759 \r\nL 178.429407 196.929033 \r\nL 178.734076 190.339943 \r\nL 179.038744 190.339943 \r\nL 179.343412 170.572668 \r\nL 179.64808 190.339943 \r\nL 179.952749 190.339943 \r\nL 180.257417 183.750852 \r\nL 180.562085 196.929033 \r\nL 180.866754 216.696306 \r\nL 181.171422 190.339943 \r\nL 181.47609 177.161759 \r\nL 181.780759 190.339943 \r\nL 182.085427 196.929033 \r\nL 182.694764 196.929033 \r\nL 182.999432 157.394487 \r\nL 183.608768 210.107216 \r\nL 183.913437 177.161759 \r\nL 184.218105 190.339943 \r\nL 184.827442 177.161759 \r\nL 185.13211 196.929033 \r\nL 185.436778 183.750852 \r\nL 186.046115 210.107216 \r\nL 186.350783 196.929033 \r\nL 186.655451 157.394487 \r\nL 186.96012 183.750852 \r\nL 187.264788 183.750852 \r\nL 187.569456 210.107216 \r\nL 187.874125 183.750852 \r\nL 188.483461 196.929033 \r\nL 188.78813 190.339943 \r\nL 189.092798 163.983578 \r\nL 189.702135 177.161759 \r\nL 190.006803 170.572668 \r\nL 190.311471 216.696306 \r\nL 190.616139 196.929033 \r\nL 190.920808 203.518124 \r\nL 191.225476 196.929033 \r\nL 191.530144 196.929033 \r\nL 191.834813 190.339943 \r\nL 192.139481 203.518124 \r\nL 192.444149 203.518124 \r\nL 192.748818 183.750852 \r\nL 193.053486 203.518124 \r\nL 193.358154 196.929033 \r\nL 193.662822 203.518124 \r\nL 193.967491 190.339943 \r\nL 194.576827 203.518124 \r\nL 194.881496 137.627216 \r\nL 195.186164 190.339943 \r\nL 195.490832 203.518124 \r\nL 195.795501 223.285397 \r\nL 196.100169 183.750852 \r\nL 196.404837 196.929033 \r\nL 196.709506 183.750852 \r\nL 197.014174 210.107216 \r\nL 197.318842 203.518124 \r\nL 197.62351 190.339943 \r\nL 197.928179 183.750852 \r\nL 198.232847 170.572668 \r\nL 198.842184 216.696306 \r\nL 199.146852 177.161759 \r\nL 199.45152 170.572668 \r\nL 199.756189 203.518124 \r\nL 200.060857 183.750852 \r\nL 200.365525 203.518124 \r\nL 200.670193 210.107216 \r\nL 200.974862 183.750852 \r\nL 201.584198 183.750852 \r\nL 201.888867 223.285397 \r\nL 202.193535 196.929033 \r\nL 202.802872 196.929033 \r\nL 203.10754 183.750852 \r\nL 203.412208 190.339943 \r\nL 203.716877 177.161759 \r\nL 204.021545 170.572668 \r\nL 204.326213 183.750852 \r\nL 204.630881 203.518124 \r\nL 204.93555 183.750852 \r\nL 205.240218 190.339943 \r\nL 205.544886 163.983578 \r\nL 206.154223 196.929033 \r\nL 206.458891 177.161759 \r\nL 206.76356 190.339943 \r\nL 207.068228 177.161759 \r\nL 207.372896 196.929033 \r\nL 207.677564 190.339943 \r\nL 208.286901 203.518124 \r\nL 208.591569 170.572668 \r\nL 209.200906 203.518124 \r\nL 209.505574 183.750852 \r\nL 209.810243 177.161759 \r\nL 210.114911 203.518124 \r\nL 210.419579 183.750852 \r\nL 210.724248 183.750852 \r\nL 211.028916 170.572668 \r\nL 211.333584 216.696306 \r\nL 211.638252 183.750852 \r\nL 211.942921 177.161759 \r\nL 212.247589 196.929033 \r\nL 212.552257 203.518124 \r\nL 212.856926 196.929033 \r\nL 213.161594 196.929033 \r\nL 213.466262 177.161759 \r\nL 213.770931 170.572668 \r\nL 214.075599 203.518124 \r\nL 214.380267 196.929033 \r\nL 214.684936 183.750852 \r\nL 214.989604 190.339943 \r\nL 215.294272 190.339943 \r\nL 215.59894 157.394487 \r\nL 215.903609 177.161759 \r\nL 216.208277 216.696306 \r\nL 216.512945 190.339943 \r\nL 216.817614 203.518124 \r\nL 217.42695 190.339943 \r\nL 217.731619 190.339943 \r\nL 218.036287 203.518124 \r\nL 218.645623 216.696306 \r\nL 218.950292 183.750852 \r\nL 219.559628 216.696306 \r\nL 220.168965 170.572668 \r\nL 220.473633 190.339943 \r\nL 220.778302 183.750852 \r\nL 221.08297 196.929033 \r\nL 221.387638 183.750852 \r\nL 221.692307 190.339943 \r\nL 221.996975 203.518124 \r\nL 222.606311 170.572668 \r\nL 222.91098 190.339943 \r\nL 223.215648 177.161759 \r\nL 223.520316 196.929033 \r\nL 223.824985 196.929033 \r\nL 224.129653 170.572668 \r\nL 224.434321 183.750852 \r\nL 224.73899 190.339943 \r\nL 225.043658 203.518124 \r\nL 225.652994 157.394487 \r\nL 226.262331 203.518124 \r\nL 226.871668 163.983578 \r\nL 227.481004 196.929033 \r\nL 227.785673 190.339943 \r\nL 228.090341 177.161759 \r\nL 228.395009 177.161759 \r\nL 228.699678 203.518124 \r\nL 229.004346 190.339943 \r\nL 229.309014 196.929033 \r\nL 229.613682 216.696306 \r\nL 229.918351 190.339943 \r\nL 230.223019 177.161759 \r\nL 230.527687 210.107216 \r\nL 230.832356 190.339943 \r\nL 231.137024 190.339943 \r\nL 231.441692 203.518124 \r\nL 231.746361 196.929033 \r\nL 232.355697 196.929033 \r\nL 232.965034 183.750852 \r\nL 233.269702 183.750852 \r\nL 233.57437 190.339943 \r\nL 233.879039 183.750852 \r\nL 234.183707 203.518124 \r\nL 234.488375 196.929033 \r\nL 234.793044 183.750852 \r\nL 235.097712 183.750852 \r\nL 235.40238 210.107216 \r\nL 235.707049 190.339943 \r\nL 236.011717 203.518124 \r\nL 236.316385 203.518124 \r\nL 236.621053 210.107216 \r\nL 236.925722 190.339943 \r\nL 237.23039 190.339943 \r\nL 237.535058 183.750852 \r\nL 237.839727 163.983578 \r\nL 238.144395 196.929033 \r\nL 238.449063 196.929033 \r\nL 238.753732 190.339943 \r\nL 239.0584 170.572668 \r\nL 239.363068 183.750852 \r\nL 239.667736 163.983578 \r\nL 239.972405 177.161759 \r\nL 240.277073 196.929033 \r\nL 240.581741 190.339943 \r\nL 240.88641 196.929033 \r\nL 241.191078 177.161759 \r\nL 241.495746 170.572668 \r\nL 241.800415 210.107216 \r\nL 242.105083 170.572668 \r\nL 242.409751 190.339943 \r\nL 242.71442 177.161759 \r\nL 243.628424 177.161759 \r\nL 243.933093 210.107216 \r\nL 244.237761 203.518124 \r\nL 244.542429 183.750852 \r\nL 245.456434 183.750852 \r\nL 245.761103 196.929033 \r\nL 246.065771 177.161759 \r\nL 246.370439 190.339943 \r\nL 246.675107 177.161759 \r\nL 246.979776 177.161759 \r\nL 247.284444 170.572668 \r\nL 247.589112 150.805397 \r\nL 247.893781 196.929033 \r\nL 248.198449 190.339943 \r\nL 248.503117 157.394487 \r\nL 248.807786 183.750852 \r\nL 249.112454 183.750852 \r\nL 249.417122 203.518124 \r\nL 249.721791 183.750852 \r\nL 250.026459 203.518124 \r\nL 250.331127 177.161759 \r\nL 250.635795 177.161759 \r\nL 250.940464 183.750852 \r\nL 251.245132 196.929033 \r\nL 251.5498 196.929033 \r\nL 251.854469 163.983578 \r\nL 252.159137 163.983578 \r\nL 252.768474 203.518124 \r\nL 253.073142 190.339943 \r\nL 253.37781 183.750852 \r\nL 254.291815 183.750852 \r\nL 254.596483 177.161759 \r\nL 255.20582 190.339943 \r\nL 255.510488 210.107216 \r\nL 256.119825 177.161759 \r\nL 256.424493 203.518124 \r\nL 256.729162 183.750852 \r\nL 257.03383 216.696306 \r\nL 257.338498 203.518124 \r\nL 257.947835 157.394487 \r\nL 258.252503 190.339943 \r\nL 258.557171 177.161759 \r\nL 259.166508 203.518124 \r\nL 259.471176 190.339943 \r\nL 260.080513 190.339943 \r\nL 260.385181 163.983578 \r\nL 260.68985 183.750852 \r\nL 260.994518 177.161759 \r\nL 261.299186 203.518124 \r\nL 261.603854 190.339943 \r\nL 261.908523 183.750852 \r\nL 262.213191 203.518124 \r\nL 262.517859 163.983578 \r\nL 262.822528 183.750852 \r\nL 263.127196 163.983578 \r\nL 263.431864 183.750852 \r\nL 263.736533 170.572668 \r\nL 264.041201 183.750852 \r\nL 264.650537 196.929033 \r\nL 264.955206 190.339943 \r\nL 265.259874 177.161759 \r\nL 265.564542 210.107216 \r\nL 265.869211 183.750852 \r\nL 266.173879 203.518124 \r\nL 266.478547 183.750852 \r\nL 266.783216 183.750852 \r\nL 267.087884 163.983578 \r\nL 267.392552 196.929033 \r\nL 267.697221 177.161759 \r\nL 268.306557 203.518124 \r\nL 268.611225 177.161759 \r\nL 268.915894 203.518124 \r\nL 269.220562 163.983578 \r\nL 269.52523 190.339943 \r\nL 269.829899 190.339943 \r\nL 270.134567 196.929033 \r\nL 270.439235 163.983578 \r\nL 270.743904 183.750852 \r\nL 271.048572 183.750852 \r\nL 271.35324 177.161759 \r\nL 271.657908 177.161759 \r\nL 271.962577 183.750852 \r\nL 272.267245 157.394487 \r\nL 272.571913 170.572668 \r\nL 272.876582 177.161759 \r\nL 273.18125 170.572668 \r\nL 273.485918 177.161759 \r\nL 273.790587 203.518124 \r\nL 274.095255 177.161759 \r\nL 274.399923 203.518124 \r\nL 274.704592 190.339943 \r\nL 275.00926 183.750852 \r\nL 275.313928 196.929033 \r\nL 275.618596 157.394487 \r\nL 275.923265 170.572668 \r\nL 276.227933 150.805397 \r\nL 276.532601 196.929033 \r\nL 276.83727 190.339943 \r\nL 277.141938 170.572668 \r\nL 277.446606 203.518124 \r\nL 277.751275 196.929033 \r\nL 278.055943 196.929033 \r\nL 278.360611 163.983578 \r\nL 278.665279 190.339943 \r\nL 278.969948 190.339943 \r\nL 279.274616 163.983578 \r\nL 279.579284 170.572668 \r\nL 279.883953 190.339943 \r\nL 280.188621 177.161759 \r\nL 280.493289 196.929033 \r\nL 280.797958 196.929033 \r\nL 281.102626 170.572668 \r\nL 281.407294 190.339943 \r\nL 281.711963 177.161759 \r\nL 282.321299 210.107216 \r\nL 282.625967 163.983578 \r\nL 282.930636 157.394487 \r\nL 283.235304 196.929033 \r\nL 283.539972 183.750852 \r\nL 283.844641 196.929033 \r\nL 284.149309 190.339943 \r\nL 284.453977 203.518124 \r\nL 284.758646 170.572668 \r\nL 285.063314 170.572668 \r\nL 285.367982 196.929033 \r\nL 285.67265 170.572668 \r\nL 286.281987 196.929033 \r\nL 286.586655 190.339943 \r\nL 286.891324 196.929033 \r\nL 287.195992 163.983578 \r\nL 287.805329 177.161759 \r\nL 288.414665 210.107216 \r\nL 288.719334 177.161759 \r\nL 289.024002 163.983578 \r\nL 289.32867 210.107216 \r\nL 289.633338 170.572668 \r\nL 290.242675 196.929033 \r\nL 290.547343 183.750852 \r\nL 290.852012 196.929033 \r\nL 291.15668 196.929033 \r\nL 291.461348 177.161759 \r\nL 291.766017 203.518124 \r\nL 292.070685 196.929033 \r\nL 292.375353 183.750852 \r\nL 292.98469 196.929033 \r\nL 293.289358 183.750852 \r\nL 293.594026 190.339943 \r\nL 294.203363 177.161759 \r\nL 294.508031 183.750852 \r\nL 294.8127 177.161759 \r\nL 295.117368 203.518124 \r\nL 295.422036 183.750852 \r\nL 295.726705 190.339943 \r\nL 296.945378 190.339943 \r\nL 297.250046 196.929033 \r\nL 297.554714 177.161759 \r\nL 298.164051 203.518124 \r\nL 298.468719 183.750852 \r\nL 298.773388 190.339943 \r\nL 299.078056 163.983578 \r\nL 299.382724 183.750852 \r\nL 299.687393 216.696306 \r\nL 300.296729 190.339943 \r\nL 300.601397 190.339943 \r\nL 300.906066 196.929033 \r\nL 301.210734 190.339943 \r\nL 301.515402 190.339943 \r\nL 301.820071 163.983578 \r\nL 302.429407 190.339943 \r\nL 302.734076 190.339943 \r\nL 303.038744 157.394487 \r\nL 303.343412 163.983578 \r\nL 303.64808 203.518124 \r\nL 303.952749 190.339943 \r\nL 304.257417 196.929033 \r\nL 304.562085 196.929033 \r\nL 304.866754 210.107216 \r\nL 305.171422 170.572668 \r\nL 305.47609 183.750852 \r\nL 306.085427 170.572668 \r\nL 306.390095 216.696306 \r\nL 306.999432 177.161759 \r\nL 307.3041 196.929033 \r\nL 307.608768 190.339943 \r\nL 308.218105 163.983578 \r\nL 308.522773 177.161759 \r\nL 308.827442 177.161759 \r\nL 309.13211 210.107216 \r\nL 309.436778 170.572668 \r\nL 309.741447 190.339943 \r\nL 310.046115 177.161759 \r\nL 310.350783 150.805397 \r\nL 310.96012 210.107216 \r\nL 311.264788 190.339943 \r\nL 311.569456 196.929033 \r\nL 311.874125 170.572668 \r\nL 312.178793 190.339943 \r\nL 312.483461 190.339943 \r\nL 312.78813 183.750852 \r\nL 313.092798 190.339943 \r\nL 313.397466 190.339943 \r\nL 313.702135 170.572668 \r\nL 314.006803 190.339943 \r\nL 314.311471 170.572668 \r\nL 314.616139 190.339943 \r\nL 314.920808 177.161759 \r\nL 315.225476 190.339943 \r\nL 315.530144 190.339943 \r\nL 315.834813 183.750852 \r\nL 316.139481 183.750852 \r\nL 316.444149 203.518124 \r\nL 316.748818 157.394487 \r\nL 317.053486 190.339943 \r\nL 317.358154 203.518124 \r\nL 317.662822 177.161759 \r\nL 317.967491 177.161759 \r\nL 318.272159 216.696306 \r\nL 318.576827 163.983578 \r\nL 318.881496 203.518124 \r\nL 319.186164 183.750852 \r\nL 319.490832 190.339943 \r\nL 319.795501 183.750852 \r\nL 320.100169 216.696306 \r\nL 320.404837 183.750852 \r\nL 320.709506 210.107216 \r\nL 321.014174 203.518124 \r\nL 321.318842 170.572668 \r\nL 321.62351 170.572668 \r\nL 321.928179 190.339943 \r\nL 322.232847 183.750852 \r\nL 322.537515 190.339943 \r\nL 322.842184 190.339943 \r\nL 323.146852 196.929033 \r\nL 323.45152 150.805397 \r\nL 323.756189 190.339943 \r\nL 324.060857 183.750852 \r\nL 324.365525 196.929033 \r\nL 324.670193 170.572668 \r\nL 324.974862 196.929033 \r\nL 325.27953 177.161759 \r\nL 325.584198 177.161759 \r\nL 325.888867 183.750852 \r\nL 326.193535 157.394487 \r\nL 326.498203 190.339943 \r\nL 326.802872 163.983578 \r\nL 327.10754 190.339943 \r\nL 327.412208 183.750852 \r\nL 327.716877 203.518124 \r\nL 328.021545 203.518124 \r\nL 328.630881 190.339943 \r\nL 328.93555 210.107216 \r\nL 329.240218 157.394487 \r\nL 329.544886 210.107216 \r\nL 329.849555 183.750852 \r\nL 330.154223 177.161759 \r\nL 330.458891 190.339943 \r\nL 330.76356 196.929033 \r\nL 331.068228 183.750852 \r\nL 331.372896 203.518124 \r\nL 332.286901 203.518124 \r\nL 332.591569 190.339943 \r\nL 332.896238 183.750852 \r\nL 333.200906 196.929033 \r\nL 333.505574 190.339943 \r\nL 333.810243 190.339943 \r\nL 334.114911 170.572668 \r\nL 334.419579 183.750852 \r\nL 335.028916 183.750852 \r\nL 335.333584 190.339943 \r\nL 335.638252 183.750852 \r\nL 335.942921 163.983578 \r\nL 336.247589 163.983578 \r\nL 336.552257 183.750852 \r\nL 336.856926 177.161759 \r\nL 337.161594 190.339943 \r\nL 337.466262 210.107216 \r\nL 337.770931 163.983578 \r\nL 338.380267 210.107216 \r\nL 338.989604 163.983578 \r\nL 339.294272 157.394487 \r\nL 339.903609 183.750852 \r\nL 340.208277 183.750852 \r\nL 340.817614 196.929033 \r\nL 341.122282 157.394487 \r\nL 341.42695 196.929033 \r\nL 341.731619 190.339943 \r\nL 342.036287 196.929033 \r\nL 342.340955 177.161759 \r\nL 342.645623 183.750852 \r\nL 342.950292 177.161759 \r\nL 343.25496 177.161759 \r\nL 343.559628 183.750852 \r\nL 343.864297 203.518124 \r\nL 344.168965 177.161759 \r\nL 344.473633 163.983578 \r\nL 344.778302 190.339943 \r\nL 345.08297 196.929033 \r\nL 345.387638 190.339943 \r\nL 345.692307 163.983578 \r\nL 345.996975 177.161759 \r\nL 346.301643 196.929033 \r\nL 346.606311 190.339943 \r\nL 346.91098 170.572668 \r\nL 347.215648 163.983578 \r\nL 347.520316 144.216306 \r\nL 347.824985 163.983578 \r\nL 348.129653 196.929033 \r\nL 348.434321 190.339943 \r\nL 348.73899 157.394487 \r\nL 349.043658 183.750852 \r\nL 349.348326 183.750852 \r\nL 349.652994 170.572668 \r\nL 349.957663 170.572668 \r\nL 350.262331 190.339943 \r\nL 350.566999 163.983578 \r\nL 350.871668 163.983578 \r\nL 351.176336 183.750852 \r\nL 351.481004 183.750852 \r\nL 351.785673 131.038125 \r\nL 352.090341 196.929033 \r\nL 352.395009 177.161759 \r\nL 352.699678 196.929033 \r\nL 353.309014 157.394487 \r\nL 353.613682 196.929033 \r\nL 353.918351 163.983578 \r\nL 354.223019 203.518124 \r\nL 354.527687 170.572668 \r\nL 354.832356 183.750852 \r\nL 355.441692 196.929033 \r\nL 355.746361 190.339943 \r\nL 356.051029 190.339943 \r\nL 356.355697 177.161759 \r\nL 356.660365 190.339943 \r\nL 357.269702 163.983578 \r\nL 357.57437 203.518124 \r\nL 357.879039 183.750852 \r\nL 358.183707 177.161759 \r\nL 358.488375 190.339943 \r\nL 358.793044 196.929033 \r\nL 359.097712 163.983578 \r\nL 359.40238 183.750852 \r\nL 359.707049 190.339943 \r\nL 360.011717 177.161759 \r\nL 360.316385 157.394487 \r\nL 360.621053 163.983578 \r\nL 360.925722 203.518124 \r\nL 361.23039 190.339943 \r\nL 361.535058 196.929033 \r\nL 361.839727 183.750852 \r\nL 362.144395 210.107216 \r\nL 362.753732 177.161759 \r\nL 363.0584 203.518124 \r\nL 363.363068 170.572668 \r\nL 363.363068 170.572668 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p833c1f4436)\" d=\"M 58.999432 190.339943 \r\nL 59.3041 229.874489 \r\nL 59.608768 210.107216 \r\nL 59.913437 229.874489 \r\nL 60.522773 190.339943 \r\nL 60.827442 229.874489 \r\nL 61.13211 190.339943 \r\nL 61.436778 131.038125 \r\nL 61.741447 150.805397 \r\nL 62.046115 111.270848 \r\nL 62.350783 91.503582 \r\nL 62.655451 229.874489 \r\nL 62.96012 229.874489 \r\nL 63.264788 170.572668 \r\nL 63.569456 190.339943 \r\nL 63.874125 170.572668 \r\nL 64.178793 190.339943 \r\nL 64.483461 229.874489 \r\nL 64.78813 190.339943 \r\nL 65.092798 91.503582 \r\nL 65.397466 229.874489 \r\nL 66.006803 229.874489 \r\nL 66.311471 111.270848 \r\nL 66.616139 170.572668 \r\nL 66.920808 131.038125 \r\nL 67.225476 190.339943 \r\nL 67.530144 210.107216 \r\nL 67.834813 131.038125 \r\nL 68.139481 190.339943 \r\nL 68.444149 150.805397 \r\nL 68.748818 229.874489 \r\nL 69.053486 210.107216 \r\nL 69.358154 229.874489 \r\nL 69.662822 190.339943 \r\nL 69.967491 229.874489 \r\nL 70.576827 229.874489 \r\nL 70.881496 210.107216 \r\nL 71.186164 229.874489 \r\nL 71.795501 229.874489 \r\nL 72.100169 170.572668 \r\nL 72.404837 170.572668 \r\nL 72.709506 229.874489 \r\nL 73.014174 190.339943 \r\nL 73.318842 229.874489 \r\nL 73.928179 229.874489 \r\nL 74.232847 190.339943 \r\nL 74.537515 229.874489 \r\nL 75.146852 229.874489 \r\nL 75.45152 210.107216 \r\nL 75.756189 229.874489 \r\nL 76.365525 111.270848 \r\nL 76.670193 229.874489 \r\nL 80.326213 229.874489 \r\nL 80.630881 150.805397 \r\nL 80.93555 190.339943 \r\nL 81.544886 229.874489 \r\nL 81.849555 229.874489 \r\nL 82.154223 190.339943 \r\nL 82.458891 190.339943 \r\nL 82.76356 229.874489 \r\nL 83.068228 229.874489 \r\nL 83.372896 32.201761 \r\nL 83.677564 210.107216 \r\nL 83.982233 210.107216 \r\nL 84.286901 229.874489 \r\nL 84.591569 190.339943 \r\nL 84.896238 229.874489 \r\nL 85.200906 229.874489 \r\nL 85.505574 131.038125 \r\nL 85.810243 210.107216 \r\nL 86.419579 210.107216 \r\nL 86.724248 229.874489 \r\nL 87.333584 190.339943 \r\nL 87.638252 210.107216 \r\nL 87.942921 210.107216 \r\nL 88.247589 229.874489 \r\nL 89.770931 229.874489 \r\nL 90.075599 190.339943 \r\nL 90.380267 190.339943 \r\nL 90.684936 229.874489 \r\nL 90.989604 111.270848 \r\nL 91.294272 91.503582 \r\nL 91.59894 229.874489 \r\nL 92.208277 229.874489 \r\nL 92.512945 210.107216 \r\nL 92.817614 131.038125 \r\nL 93.122282 170.572668 \r\nL 93.42695 170.572668 \r\nL 93.731619 229.874489 \r\nL 94.340955 229.874489 \r\nL 94.645623 210.107216 \r\nL 94.950292 150.805397 \r\nL 95.25496 210.107216 \r\nL 95.559628 150.805397 \r\nL 95.864297 150.805397 \r\nL 96.168965 229.874489 \r\nL 96.473633 229.874489 \r\nL 96.778302 210.107216 \r\nL 97.08297 229.874489 \r\nL 97.387638 170.572668 \r\nL 97.692307 210.107216 \r\nL 97.996975 229.874489 \r\nL 99.520316 229.874489 \r\nL 99.824985 150.805397 \r\nL 100.129653 170.572668 \r\nL 100.434321 229.874489 \r\nL 100.73899 32.201761 \r\nL 101.043658 150.805397 \r\nL 101.348326 170.572668 \r\nL 101.652994 150.805397 \r\nL 101.957663 170.572668 \r\nL 102.262331 229.874489 \r\nL 102.566999 150.805397 \r\nL 102.871668 190.339943 \r\nL 103.176336 170.572668 \r\nL 103.481004 229.874489 \r\nL 103.785673 131.038125 \r\nL 104.090341 131.038125 \r\nL 104.395009 111.270848 \r\nL 104.699678 229.874489 \r\nL 105.004346 229.874489 \r\nL 105.309014 210.107216 \r\nL 105.613682 150.805397 \r\nL 105.918351 32.201761 \r\nL 106.223019 170.572668 \r\nL 106.527687 229.874489 \r\nL 106.832356 229.874489 \r\nL 107.137024 210.107216 \r\nL 107.746361 210.107216 \r\nL 108.051029 229.874489 \r\nL 108.355697 210.107216 \r\nL 108.660365 229.874489 \r\nL 108.965034 229.874489 \r\nL 109.269702 210.107216 \r\nL 109.57437 131.038125 \r\nL 109.879039 210.107216 \r\nL 110.183707 229.874489 \r\nL 110.793044 229.874489 \r\nL 111.097712 190.339943 \r\nL 111.40238 229.874489 \r\nL 111.707049 229.874489 \r\nL 112.011717 190.339943 \r\nL 112.316385 170.572668 \r\nL 112.621053 111.270848 \r\nL 112.925722 229.874489 \r\nL 113.23039 229.874489 \r\nL 113.535058 210.107216 \r\nL 114.144395 71.736304 \r\nL 114.449063 131.038125 \r\nL 114.753732 229.874489 \r\nL 115.0584 131.038125 \r\nL 115.363068 91.503582 \r\nL 115.667736 229.874489 \r\nL 115.972405 229.874489 \r\nL 116.277073 190.339943 \r\nL 116.581741 229.874489 \r\nL 117.191078 229.874489 \r\nL 117.495746 210.107216 \r\nL 117.800415 229.874489 \r\nL 118.105083 131.038125 \r\nL 118.409751 91.503582 \r\nL 119.019088 229.874489 \r\nL 119.323756 229.874489 \r\nL 119.628424 210.107216 \r\nL 119.933093 170.572668 \r\nL 120.237761 111.270848 \r\nL 120.542429 190.339943 \r\nL 120.847098 190.339943 \r\nL 121.151766 229.874489 \r\nL 121.456434 210.107216 \r\nL 121.761103 229.874489 \r\nL 122.065771 190.339943 \r\nL 122.675107 190.339943 \r\nL 122.979776 170.572668 \r\nL 123.284444 190.339943 \r\nL 123.589112 131.038125 \r\nL 123.893781 190.339943 \r\nL 124.198449 131.038125 \r\nL 124.503117 150.805397 \r\nL 124.807786 131.038125 \r\nL 125.112454 170.572668 \r\nL 125.417122 111.270848 \r\nL 125.721791 190.339943 \r\nL 126.026459 170.572668 \r\nL 126.331127 170.572668 \r\nL 126.635795 229.874489 \r\nL 126.940464 150.805397 \r\nL 127.245132 150.805397 \r\nL 127.854469 71.736304 \r\nL 128.159137 91.503582 \r\nL 128.463805 229.874489 \r\nL 128.768474 190.339943 \r\nL 129.073142 51.969039 \r\nL 129.37781 190.339943 \r\nL 129.682479 131.038125 \r\nL 129.987147 229.874489 \r\nL 130.291815 210.107216 \r\nL 130.596483 111.270848 \r\nL 130.901152 229.874489 \r\nL 131.510488 229.874489 \r\nL 132.119825 190.339943 \r\nL 132.424493 190.339943 \r\nL 132.729162 150.805397 \r\nL 133.03383 150.805397 \r\nL 133.338498 131.038125 \r\nL 133.643166 210.107216 \r\nL 133.947835 111.270848 \r\nL 134.252503 170.572668 \r\nL 134.557171 71.736304 \r\nL 135.166508 229.874489 \r\nL 135.471176 210.107216 \r\nL 135.775845 229.874489 \r\nL 136.080513 229.874489 \r\nL 136.385181 150.805397 \r\nL 136.68985 170.572668 \r\nL 136.994518 229.874489 \r\nL 137.603854 229.874489 \r\nL 137.908523 210.107216 \r\nL 138.213191 131.038125 \r\nL 138.517859 229.874489 \r\nL 138.822528 131.038125 \r\nL 139.127196 71.736304 \r\nL 139.431864 131.038125 \r\nL 139.736533 131.038125 \r\nL 140.041201 210.107216 \r\nL 140.345869 190.339943 \r\nL 140.650537 229.874489 \r\nL 141.869211 229.874489 \r\nL 142.173879 210.107216 \r\nL 142.478547 131.038125 \r\nL 142.783216 210.107216 \r\nL 143.087884 229.874489 \r\nL 143.392552 131.038125 \r\nL 143.697221 150.805397 \r\nL 144.001889 210.107216 \r\nL 144.306557 229.874489 \r\nL 144.611225 229.874489 \r\nL 144.915894 170.572668 \r\nL 145.220562 210.107216 \r\nL 145.52523 229.874489 \r\nL 145.829899 91.503582 \r\nL 146.439235 91.503582 \r\nL 146.743904 111.270848 \r\nL 147.048572 111.270848 \r\nL 147.35324 229.874489 \r\nL 147.657908 131.038125 \r\nL 147.962577 229.874489 \r\nL 148.267245 190.339943 \r\nL 148.571913 190.339943 \r\nL 148.876582 229.874489 \r\nL 149.18125 210.107216 \r\nL 149.485918 229.874489 \r\nL 150.095255 190.339943 \r\nL 150.399923 229.874489 \r\nL 150.704592 111.270848 \r\nL 151.00926 210.107216 \r\nL 151.313928 210.107216 \r\nL 151.618596 111.270848 \r\nL 151.923265 150.805397 \r\nL 152.227933 210.107216 \r\nL 152.532601 170.572668 \r\nL 152.83727 190.339943 \r\nL 153.141938 150.805397 \r\nL 153.446606 190.339943 \r\nL 153.751275 51.969039 \r\nL 154.055943 229.874489 \r\nL 154.360611 229.874489 \r\nL 154.665279 190.339943 \r\nL 154.969948 229.874489 \r\nL 155.274616 229.874489 \r\nL 155.579284 210.107216 \r\nL 155.883953 229.874489 \r\nL 156.493289 229.874489 \r\nL 156.797958 210.107216 \r\nL 157.102626 229.874489 \r\nL 157.407294 150.805397 \r\nL 157.711963 150.805397 \r\nL 158.016631 111.270848 \r\nL 158.321299 131.038125 \r\nL 158.625967 111.270848 \r\nL 158.930636 150.805397 \r\nL 159.235304 170.572668 \r\nL 159.539972 170.572668 \r\nL 159.844641 91.503582 \r\nL 160.149309 210.107216 \r\nL 160.453977 229.874489 \r\nL 160.758646 210.107216 \r\nL 161.063314 229.874489 \r\nL 161.367982 229.874489 \r\nL 161.67265 150.805397 \r\nL 161.977319 170.572668 \r\nL 162.281987 71.736304 \r\nL 162.586655 111.270848 \r\nL 162.891324 170.572668 \r\nL 163.195992 131.038125 \r\nL 163.50066 71.736304 \r\nL 163.805329 229.874489 \r\nL 164.109997 210.107216 \r\nL 164.414665 170.572668 \r\nL 164.719334 170.572668 \r\nL 165.024002 150.805397 \r\nL 165.32867 190.339943 \r\nL 165.633338 210.107216 \r\nL 166.242675 170.572668 \r\nL 166.547343 229.874489 \r\nL 166.852012 229.874489 \r\nL 167.15668 210.107216 \r\nL 167.461348 150.805397 \r\nL 167.766017 131.038125 \r\nL 168.070685 131.038125 \r\nL 168.375353 150.805397 \r\nL 168.680021 190.339943 \r\nL 168.98469 131.038125 \r\nL 169.289358 150.805397 \r\nL 169.594026 131.038125 \r\nL 169.898695 170.572668 \r\nL 170.203363 190.339943 \r\nL 170.8127 150.805397 \r\nL 171.117368 229.874489 \r\nL 171.726705 229.874489 \r\nL 172.031373 210.107216 \r\nL 172.336041 229.874489 \r\nL 172.640709 111.270848 \r\nL 172.945378 229.874489 \r\nL 173.859383 229.874489 \r\nL 174.164051 210.107216 \r\nL 174.468719 229.874489 \r\nL 175.078056 229.874489 \r\nL 175.382724 210.107216 \r\nL 175.687393 229.874489 \r\nL 176.296729 229.874489 \r\nL 176.601397 210.107216 \r\nL 176.906066 131.038125 \r\nL 177.210734 170.572668 \r\nL 177.515402 190.339943 \r\nL 177.820071 229.874489 \r\nL 179.952749 229.874489 \r\nL 180.257417 190.339943 \r\nL 180.562085 229.874489 \r\nL 181.47609 229.874489 \r\nL 181.780759 131.038125 \r\nL 182.390095 170.572668 \r\nL 182.694764 170.572668 \r\nL 182.999432 229.874489 \r\nL 183.3041 190.339943 \r\nL 183.608768 170.572668 \r\nL 183.913437 210.107216 \r\nL 184.218105 190.339943 \r\nL 184.522773 51.969039 \r\nL 184.827442 210.107216 \r\nL 185.436778 170.572668 \r\nL 185.741447 111.270848 \r\nL 186.046115 229.874489 \r\nL 186.350783 229.874489 \r\nL 186.655451 71.736304 \r\nL 186.96012 210.107216 \r\nL 187.264788 210.107216 \r\nL 187.569456 190.339943 \r\nL 187.874125 229.874489 \r\nL 188.78813 170.572668 \r\nL 189.092798 131.038125 \r\nL 189.397466 229.874489 \r\nL 190.616139 229.874489 \r\nL 191.225476 150.805397 \r\nL 191.530144 190.339943 \r\nL 191.834813 190.339943 \r\nL 192.139481 170.572668 \r\nL 192.444149 91.503582 \r\nL 192.748818 229.874489 \r\nL 193.358154 229.874489 \r\nL 193.662822 190.339943 \r\nL 193.967491 170.572668 \r\nL 194.881496 229.874489 \r\nL 195.490832 229.874489 \r\nL 195.795501 210.107216 \r\nL 196.100169 150.805397 \r\nL 196.404837 229.874489 \r\nL 196.709506 150.805397 \r\nL 197.014174 150.805397 \r\nL 197.318842 229.874489 \r\nL 197.62351 229.874489 \r\nL 197.928179 210.107216 \r\nL 198.232847 210.107216 \r\nL 198.537515 190.339943 \r\nL 198.842184 190.339943 \r\nL 199.146852 210.107216 \r\nL 199.45152 210.107216 \r\nL 199.756189 229.874489 \r\nL 200.670193 229.874489 \r\nL 200.974862 190.339943 \r\nL 201.27953 170.572668 \r\nL 201.584198 229.874489 \r\nL 201.888867 32.201761 \r\nL 202.193535 131.038125 \r\nL 202.802872 131.038125 \r\nL 203.10754 111.270848 \r\nL 203.412208 229.874489 \r\nL 203.716877 131.038125 \r\nL 204.021545 170.572668 \r\nL 204.326213 170.572668 \r\nL 204.630881 229.874489 \r\nL 204.93555 229.874489 \r\nL 205.240218 150.805397 \r\nL 205.544886 229.874489 \r\nL 206.458891 229.874489 \r\nL 207.068228 111.270848 \r\nL 207.372896 131.038125 \r\nL 207.677564 229.874489 \r\nL 208.286901 229.874489 \r\nL 208.591569 210.107216 \r\nL 208.896238 229.874489 \r\nL 209.200906 229.874489 \r\nL 209.505574 210.107216 \r\nL 209.810243 229.874489 \r\nL 210.114911 229.874489 \r\nL 210.724248 150.805397 \r\nL 211.028916 150.805397 \r\nL 211.333584 229.874489 \r\nL 211.942921 229.874489 \r\nL 212.247589 210.107216 \r\nL 212.552257 229.874489 \r\nL 213.161594 229.874489 \r\nL 213.466262 190.339943 \r\nL 213.770931 170.572668 \r\nL 214.075599 229.874489 \r\nL 214.684936 229.874489 \r\nL 214.989604 131.038125 \r\nL 215.294272 91.503582 \r\nL 215.903609 229.874489 \r\nL 216.208277 170.572668 \r\nL 216.512945 170.572668 \r\nL 216.817614 229.874489 \r\nL 217.122282 229.874489 \r\nL 217.42695 170.572668 \r\nL 217.731619 229.874489 \r\nL 218.340955 229.874489 \r\nL 218.645623 190.339943 \r\nL 218.950292 229.874489 \r\nL 219.25496 91.503582 \r\nL 219.559628 190.339943 \r\nL 219.864297 131.038125 \r\nL 220.168965 210.107216 \r\nL 220.473633 229.874489 \r\nL 220.778302 229.874489 \r\nL 221.08297 190.339943 \r\nL 221.387638 111.270848 \r\nL 221.692307 229.874489 \r\nL 222.301643 150.805397 \r\nL 222.606311 210.107216 \r\nL 222.91098 150.805397 \r\nL 223.215648 170.572668 \r\nL 223.520316 150.805397 \r\nL 224.129653 190.339943 \r\nL 224.434321 190.339943 \r\nL 224.73899 71.736304 \r\nL 225.348326 111.270848 \r\nL 225.957663 229.874489 \r\nL 226.262331 210.107216 \r\nL 226.871668 71.736304 \r\nL 227.176336 111.270848 \r\nL 227.481004 170.572668 \r\nL 227.785673 91.503582 \r\nL 228.395009 131.038125 \r\nL 228.699678 131.038125 \r\nL 229.004346 91.503582 \r\nL 229.309014 91.503582 \r\nL 229.613682 210.107216 \r\nL 229.918351 210.107216 \r\nL 230.223019 131.038125 \r\nL 230.527687 210.107216 \r\nL 230.832356 150.805397 \r\nL 231.137024 229.874489 \r\nL 231.441692 190.339943 \r\nL 231.746361 91.503582 \r\nL 232.051029 229.874489 \r\nL 232.660365 229.874489 \r\nL 233.269702 111.270848 \r\nL 233.57437 91.503582 \r\nL 233.879039 150.805397 \r\nL 234.183707 91.503582 \r\nL 234.488375 210.107216 \r\nL 234.793044 210.107216 \r\nL 235.097712 131.038125 \r\nL 235.40238 131.038125 \r\nL 235.707049 111.270848 \r\nL 236.011717 131.038125 \r\nL 236.316385 229.874489 \r\nL 236.621053 210.107216 \r\nL 236.925722 210.107216 \r\nL 237.23039 229.874489 \r\nL 237.535058 131.038125 \r\nL 237.839727 150.805397 \r\nL 238.144395 229.874489 \r\nL 239.0584 229.874489 \r\nL 239.363068 190.339943 \r\nL 239.667736 229.874489 \r\nL 239.972405 111.270848 \r\nL 240.277073 131.038125 \r\nL 240.581741 32.201761 \r\nL 240.88641 229.874489 \r\nL 242.71442 229.874489 \r\nL 243.019088 190.339943 \r\nL 243.323756 229.874489 \r\nL 243.628424 210.107216 \r\nL 243.933093 210.107216 \r\nL 244.237761 229.874489 \r\nL 244.542429 150.805397 \r\nL 244.847098 170.572668 \r\nL 245.151766 111.270848 \r\nL 245.456434 229.874489 \r\nL 246.065771 229.874489 \r\nL 246.370439 210.107216 \r\nL 246.675107 229.874489 \r\nL 246.979776 190.339943 \r\nL 247.284444 190.339943 \r\nL 247.589112 111.270848 \r\nL 247.893781 111.270848 \r\nL 248.198449 32.201761 \r\nL 248.503117 229.874489 \r\nL 248.807786 111.270848 \r\nL 249.112454 111.270848 \r\nL 249.721791 229.874489 \r\nL 250.331127 229.874489 \r\nL 250.635795 170.572668 \r\nL 250.940464 229.874489 \r\nL 251.5498 229.874489 \r\nL 251.854469 111.270848 \r\nL 252.159137 170.572668 \r\nL 252.463805 190.339943 \r\nL 252.768474 170.572668 \r\nL 253.073142 91.503582 \r\nL 253.37781 210.107216 \r\nL 253.682479 111.270848 \r\nL 253.987147 150.805397 \r\nL 254.291815 111.270848 \r\nL 254.596483 131.038125 \r\nL 254.901152 131.038125 \r\nL 255.20582 210.107216 \r\nL 255.510488 229.874489 \r\nL 255.815157 229.874489 \r\nL 256.119825 210.107216 \r\nL 256.424493 229.874489 \r\nL 256.729162 131.038125 \r\nL 257.03383 91.503582 \r\nL 257.338498 229.874489 \r\nL 257.643166 170.572668 \r\nL 257.947835 210.107216 \r\nL 258.252503 229.874489 \r\nL 258.557171 150.805397 \r\nL 258.86184 131.038125 \r\nL 259.166508 170.572668 \r\nL 259.471176 131.038125 \r\nL 260.080513 170.572668 \r\nL 260.68985 71.736304 \r\nL 260.994518 190.339943 \r\nL 261.299186 131.038125 \r\nL 261.603854 229.874489 \r\nL 261.908523 150.805397 \r\nL 262.213191 210.107216 \r\nL 262.517859 210.107216 \r\nL 262.822528 150.805397 \r\nL 263.127196 131.038125 \r\nL 263.431864 71.736304 \r\nL 264.041201 170.572668 \r\nL 264.345869 150.805397 \r\nL 264.650537 150.805397 \r\nL 264.955206 229.874489 \r\nL 265.259874 210.107216 \r\nL 265.564542 170.572668 \r\nL 265.869211 210.107216 \r\nL 266.173879 131.038125 \r\nL 266.478547 170.572668 \r\nL 266.783216 229.874489 \r\nL 268.001889 229.874489 \r\nL 268.306557 210.107216 \r\nL 268.611225 111.270848 \r\nL 268.915894 150.805397 \r\nL 269.220562 111.270848 \r\nL 269.52523 170.572668 \r\nL 269.829899 131.038125 \r\nL 270.439235 210.107216 \r\nL 270.743904 229.874489 \r\nL 271.048572 150.805397 \r\nL 271.35324 170.572668 \r\nL 271.657908 210.107216 \r\nL 271.962577 190.339943 \r\nL 272.267245 229.874489 \r\nL 273.18125 229.874489 \r\nL 273.485918 210.107216 \r\nL 274.095255 210.107216 \r\nL 274.399923 190.339943 \r\nL 274.704592 229.874489 \r\nL 275.00926 229.874489 \r\nL 275.313928 210.107216 \r\nL 275.618596 229.874489 \r\nL 276.227933 229.874489 \r\nL 276.532601 190.339943 \r\nL 276.83727 190.339943 \r\nL 277.141938 229.874489 \r\nL 277.446606 229.874489 \r\nL 277.751275 131.038125 \r\nL 278.055943 91.503582 \r\nL 278.360611 190.339943 \r\nL 278.665279 91.503582 \r\nL 278.969948 229.874489 \r\nL 279.274616 210.107216 \r\nL 279.579284 229.874489 \r\nL 280.493289 229.874489 \r\nL 281.102626 190.339943 \r\nL 281.407294 229.874489 \r\nL 282.625967 229.874489 \r\nL 282.930636 131.038125 \r\nL 283.539972 210.107216 \r\nL 283.844641 190.339943 \r\nL 284.149309 229.874489 \r\nL 284.453977 170.572668 \r\nL 284.758646 190.339943 \r\nL 285.367982 190.339943 \r\nL 285.67265 32.201761 \r\nL 285.977319 210.107216 \r\nL 286.281987 170.572668 \r\nL 286.586655 150.805397 \r\nL 286.891324 150.805397 \r\nL 287.195992 229.874489 \r\nL 287.50066 229.874489 \r\nL 287.805329 91.503582 \r\nL 288.109997 190.339943 \r\nL 288.414665 190.339943 \r\nL 289.024002 229.874489 \r\nL 289.32867 229.874489 \r\nL 289.633338 170.572668 \r\nL 289.938007 170.572668 \r\nL 290.242675 131.038125 \r\nL 290.547343 210.107216 \r\nL 291.15668 210.107216 \r\nL 291.461348 229.874489 \r\nL 291.766017 229.874489 \r\nL 292.375353 111.270848 \r\nL 292.680021 190.339943 \r\nL 292.98469 229.874489 \r\nL 293.289358 150.805397 \r\nL 293.594026 111.270848 \r\nL 293.898695 229.874489 \r\nL 294.203363 229.874489 \r\nL 294.508031 210.107216 \r\nL 294.8127 210.107216 \r\nL 295.117368 170.572668 \r\nL 295.422036 229.874489 \r\nL 295.726705 190.339943 \r\nL 296.031373 229.874489 \r\nL 296.336041 210.107216 \r\nL 296.640709 229.874489 \r\nL 296.945378 190.339943 \r\nL 297.250046 131.038125 \r\nL 297.554714 190.339943 \r\nL 297.859383 111.270848 \r\nL 298.164051 131.038125 \r\nL 298.468719 229.874489 \r\nL 299.078056 229.874489 \r\nL 299.382724 210.107216 \r\nL 299.687393 170.572668 \r\nL 300.601397 229.874489 \r\nL 300.906066 229.874489 \r\nL 301.515402 190.339943 \r\nL 301.820071 150.805397 \r\nL 302.429407 111.270848 \r\nL 302.734076 229.874489 \r\nL 303.038744 32.201761 \r\nL 303.343412 150.805397 \r\nL 303.64808 131.038125 \r\nL 304.257417 131.038125 \r\nL 304.562085 229.874489 \r\nL 304.866754 150.805397 \r\nL 305.171422 190.339943 \r\nL 305.47609 170.572668 \r\nL 305.780759 229.874489 \r\nL 306.085427 131.038125 \r\nL 306.390095 91.503582 \r\nL 306.694764 131.038125 \r\nL 306.999432 229.874489 \r\nL 307.3041 210.107216 \r\nL 307.608768 229.874489 \r\nL 307.913437 190.339943 \r\nL 308.218105 71.736304 \r\nL 308.522773 111.270848 \r\nL 308.827442 229.874489 \r\nL 309.436778 229.874489 \r\nL 309.741447 131.038125 \r\nL 310.046115 150.805397 \r\nL 310.350783 229.874489 \r\nL 311.264788 229.874489 \r\nL 311.569456 170.572668 \r\nL 311.874125 150.805397 \r\nL 312.178793 170.572668 \r\nL 312.483461 229.874489 \r\nL 312.78813 210.107216 \r\nL 313.092798 229.874489 \r\nL 313.397466 229.874489 \r\nL 313.702135 210.107216 \r\nL 314.006803 229.874489 \r\nL 314.311471 190.339943 \r\nL 314.616139 170.572668 \r\nL 314.920808 170.572668 \r\nL 315.225476 229.874489 \r\nL 315.530144 210.107216 \r\nL 315.834813 229.874489 \r\nL 316.139481 170.572668 \r\nL 316.444149 150.805397 \r\nL 317.053486 229.874489 \r\nL 317.358154 150.805397 \r\nL 317.662822 111.270848 \r\nL 317.967491 229.874489 \r\nL 318.272159 229.874489 \r\nL 318.576827 210.107216 \r\nL 318.881496 229.874489 \r\nL 320.100169 229.874489 \r\nL 320.404837 131.038125 \r\nL 320.709506 131.038125 \r\nL 321.014174 91.503582 \r\nL 321.318842 229.874489 \r\nL 321.62351 229.874489 \r\nL 321.928179 210.107216 \r\nL 322.232847 150.805397 \r\nL 322.537515 131.038125 \r\nL 322.842184 229.874489 \r\nL 323.146852 210.107216 \r\nL 323.45152 150.805397 \r\nL 323.756189 210.107216 \r\nL 324.365525 170.572668 \r\nL 324.670193 190.339943 \r\nL 324.974862 190.339943 \r\nL 325.27953 210.107216 \r\nL 325.584198 190.339943 \r\nL 325.888867 91.503582 \r\nL 326.193535 91.503582 \r\nL 326.498203 170.572668 \r\nL 326.802872 190.339943 \r\nL 327.10754 229.874489 \r\nL 327.412208 210.107216 \r\nL 327.716877 131.038125 \r\nL 328.021545 170.572668 \r\nL 328.630881 91.503582 \r\nL 328.93555 170.572668 \r\nL 329.240218 131.038125 \r\nL 329.544886 150.805397 \r\nL 329.849555 131.038125 \r\nL 330.154223 91.503582 \r\nL 330.458891 91.503582 \r\nL 330.76356 190.339943 \r\nL 331.068228 210.107216 \r\nL 331.372896 111.270848 \r\nL 331.677564 170.572668 \r\nL 331.982233 131.038125 \r\nL 332.286901 229.874489 \r\nL 332.591569 210.107216 \r\nL 332.896238 71.736304 \r\nL 333.200906 229.874489 \r\nL 333.505574 229.874489 \r\nL 333.810243 170.572668 \r\nL 334.114911 150.805397 \r\nL 334.419579 210.107216 \r\nL 334.724248 111.270848 \r\nL 335.028916 190.339943 \r\nL 335.333584 111.270848 \r\nL 335.942921 190.339943 \r\nL 336.247589 131.038125 \r\nL 336.552257 150.805397 \r\nL 336.856926 91.503582 \r\nL 337.161594 170.572668 \r\nL 337.466262 210.107216 \r\nL 337.770931 210.107216 \r\nL 338.075599 229.874489 \r\nL 338.380267 229.874489 \r\nL 338.684936 131.038125 \r\nL 338.989604 111.270848 \r\nL 339.294272 229.874489 \r\nL 339.59894 210.107216 \r\nL 339.903609 229.874489 \r\nL 340.208277 210.107216 \r\nL 340.512945 150.805397 \r\nL 340.817614 229.874489 \r\nL 341.122282 111.270848 \r\nL 341.42695 111.270848 \r\nL 341.731619 32.201761 \r\nL 342.036287 131.038125 \r\nL 342.340955 150.805397 \r\nL 342.645623 229.874489 \r\nL 342.950292 229.874489 \r\nL 343.25496 190.339943 \r\nL 343.559628 210.107216 \r\nL 343.864297 170.572668 \r\nL 344.473633 210.107216 \r\nL 344.778302 111.270848 \r\nL 345.08297 150.805397 \r\nL 345.387638 229.874489 \r\nL 345.996975 91.503582 \r\nL 346.301643 229.874489 \r\nL 347.824985 229.874489 \r\nL 348.129653 131.038125 \r\nL 348.434321 190.339943 \r\nL 348.73899 91.503582 \r\nL 349.043658 111.270848 \r\nL 349.348326 51.969039 \r\nL 349.652994 229.874489 \r\nL 350.262331 91.503582 \r\nL 350.566999 150.805397 \r\nL 350.871668 170.572668 \r\nL 351.176336 150.805397 \r\nL 351.481004 229.874489 \r\nL 351.785673 190.339943 \r\nL 352.090341 229.874489 \r\nL 352.395009 170.572668 \r\nL 352.699678 229.874489 \r\nL 353.004346 170.572668 \r\nL 353.613682 170.572668 \r\nL 353.918351 190.339943 \r\nL 354.223019 91.503582 \r\nL 354.527687 210.107216 \r\nL 354.832356 131.038125 \r\nL 355.137024 190.339943 \r\nL 355.441692 131.038125 \r\nL 355.746361 150.805397 \r\nL 356.051029 71.736304 \r\nL 356.355697 210.107216 \r\nL 356.660365 229.874489 \r\nL 356.965034 190.339943 \r\nL 357.57437 229.874489 \r\nL 357.879039 91.503582 \r\nL 358.183707 111.270848 \r\nL 358.488375 190.339943 \r\nL 358.793044 170.572668 \r\nL 359.097712 210.107216 \r\nL 359.40238 229.874489 \r\nL 359.707049 111.270848 \r\nL 360.011717 170.572668 \r\nL 360.316385 131.038125 \r\nL 360.621053 170.572668 \r\nL 360.925722 150.805397 \r\nL 361.23039 190.339943 \r\nL 361.535058 210.107216 \r\nL 361.839727 190.339943 \r\nL 362.144395 71.736304 \r\nL 362.449063 131.038125 \r\nL 362.753732 229.874489 \r\nL 363.0584 150.805397 \r\nL 363.363068 229.874489 \r\nL 363.363068 229.874489 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 43.78125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 22.318125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_15\">\r\n    <!-- model accuracy -->\r\n    <g transform=\"translate(163.519688 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-109\"/>\r\n     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"158.59375\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"222.070312\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"283.59375\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"311.376953\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"343.164062\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"404.443359\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"459.423828\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"514.404297\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"577.783203\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"618.896484\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"680.175781\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"735.15625\" xlink:href=\"#DejaVuSans-121\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 50.78125 59.674375 \r\nL 106.05625 59.674375 \r\nQ 108.05625 59.674375 108.05625 57.674375 \r\nL 108.05625 29.318125 \r\nQ 108.05625 27.318125 106.05625 27.318125 \r\nL 50.78125 27.318125 \r\nQ 48.78125 27.318125 48.78125 29.318125 \r\nL 48.78125 57.674375 \r\nQ 48.78125 59.674375 50.78125 59.674375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 52.78125 35.416562 \r\nL 72.78125 35.416562 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_16\">\r\n     <!-- train -->\r\n     <g transform=\"translate(80.78125 38.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 52.78125 50.094687 \r\nL 72.78125 50.094687 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_17\">\r\n     <!-- test -->\r\n     <g transform=\"translate(80.78125 53.594687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p833c1f4436\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 376.240625 277.314375\" width=\"376.240625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-01-07T12:21:26.930076</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 277.314375 \r\nL 376.240625 277.314375 \r\nL 376.240625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 34.240625 239.758125 \r\nL 369.040625 239.758125 \r\nL 369.040625 22.318125 \r\nL 34.240625 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m83091e7396\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.458807\" xlink:href=\"#m83091e7396\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(46.277557 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"110.392468\" xlink:href=\"#m83091e7396\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(100.848718 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.326129\" xlink:href=\"#m83091e7396\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(161.782379 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"232.25979\" xlink:href=\"#m83091e7396\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(222.71604 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.193451\" xlink:href=\"#m83091e7396\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(283.649701 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"354.127111\" xlink:href=\"#m83091e7396\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(341.402111 254.356562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- epoch -->\r\n     <g transform=\"translate(186.4125 268.034687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mca256261f8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca256261f8\" y=\"233.200147\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(20.878125 236.999365)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca256261f8\" y=\"188.724012\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(20.878125 192.523231)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca256261f8\" y=\"144.247878\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(20.878125 148.047096)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca256261f8\" y=\"99.771743\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(20.878125 103.570962)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.240625\" xlink:href=\"#mca256261f8\" y=\"55.295609\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 8 -->\r\n      <g transform=\"translate(20.878125 59.094827)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_13\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(14.798437 140.695937)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#p4a85c7a2d8)\" d=\"M 49.458807 134.458525 \r\nL 49.763475 147.653852 \r\nL 50.068143 139.794797 \r\nL 50.372812 141.405249 \r\nL 50.67748 142.163791 \r\nL 50.982148 136.760449 \r\nL 51.286817 152.085491 \r\nL 51.591485 149.07909 \r\nL 51.896153 137.822645 \r\nL 52.200822 154.010466 \r\nL 52.50549 144.972508 \r\nL 52.810158 142.250785 \r\nL 53.114826 150.017936 \r\nL 53.419495 132.107314 \r\nL 53.724163 149.300214 \r\nL 54.028831 155.409725 \r\nL 54.3335 157.67023 \r\nL 54.638168 139.707792 \r\nL 54.942836 141.637231 \r\nL 55.856841 155.811545 \r\nL 56.16151 156.887908 \r\nL 56.466178 144.988902 \r\nL 56.770846 152.098767 \r\nL 57.380183 141.892658 \r\nL 57.989519 155.404608 \r\nL 58.294188 156.457648 \r\nL 58.598856 142.143537 \r\nL 58.903524 148.640309 \r\nL 59.208193 144.833348 \r\nL 59.512861 137.768172 \r\nL 59.817529 151.897404 \r\nL 60.122197 140.377706 \r\nL 60.426866 147.126719 \r\nL 60.731534 140.173336 \r\nL 61.036202 150.244298 \r\nL 61.340871 133.897831 \r\nL 61.645539 148.062289 \r\nL 61.950207 145.800469 \r\nL 62.254876 134.70672 \r\nL 62.559544 148.091683 \r\nL 62.864212 145.837222 \r\nL 63.168881 150.43817 \r\nL 63.473549 153.00875 \r\nL 63.778217 147.920891 \r\nL 64.082885 145.061581 \r\nL 64.387554 153.563648 \r\nL 64.692222 144.071099 \r\nL 64.99689 141.986058 \r\nL 65.301559 146.84686 \r\nL 65.606227 141.867729 \r\nL 65.910895 157.619675 \r\nL 66.215564 156.156443 \r\nL 66.520232 144.032946 \r\nL 66.8249 154.996532 \r\nL 67.129568 136.94993 \r\nL 67.738905 149.02888 \r\nL 68.043573 152.030356 \r\nL 68.348242 143.877419 \r\nL 68.65291 149.109502 \r\nL 68.957578 144.483168 \r\nL 69.262247 142.102277 \r\nL 69.566915 149.020545 \r\nL 69.871583 150.098091 \r\nL 70.176252 142.056447 \r\nL 70.48092 141.730291 \r\nL 70.785588 153.908 \r\nL 71.090256 142.763464 \r\nL 71.394925 135.903269 \r\nL 72.004261 152.692991 \r\nL 72.30893 150.746739 \r\nL 72.613598 154.449294 \r\nL 72.918266 149.742944 \r\nL 73.222935 142.794555 \r\nL 73.527603 146.468241 \r\nL 73.832271 138.726196 \r\nL 74.441608 151.242664 \r\nL 75.050944 146.374571 \r\nL 75.355613 155.133116 \r\nL 75.660281 146.648768 \r\nL 75.964949 132.395179 \r\nL 76.269618 150.945828 \r\nL 76.574286 148.254575 \r\nL 77.183623 160.64711 \r\nL 77.488291 148.115446 \r\nL 77.792959 150.610229 \r\nL 78.097627 149.835399 \r\nL 78.402296 156.314198 \r\nL 79.011632 139.379144 \r\nL 79.316301 149.219714 \r\nL 79.620969 144.960202 \r\nL 79.925637 127.591479 \r\nL 80.230306 148.513937 \r\nL 80.534974 148.991994 \r\nL 80.839642 145.221293 \r\nL 81.144311 130.961749 \r\nL 81.448979 164.074546 \r\nL 81.753647 142.137291 \r\nL 82.058315 155.487574 \r\nL 82.362984 134.243021 \r\nL 82.667652 145.241626 \r\nL 82.97232 149.589436 \r\nL 83.276989 149.030349 \r\nL 83.581657 151.735238 \r\nL 83.886325 149.326676 \r\nL 84.190994 161.645009 \r\nL 84.495662 158.442382 \r\nL 84.80033 152.084457 \r\nL 85.104998 128.662657 \r\nL 85.409667 151.376417 \r\nL 85.714335 161.306166 \r\nL 86.323672 137.384289 \r\nL 86.62834 155.375803 \r\nL 86.933008 155.640429 \r\nL 87.237677 147.831605 \r\nL 87.542345 149.102567 \r\nL 87.847013 146.162699 \r\nL 88.151682 154.869401 \r\nL 88.45635 147.948885 \r\nL 88.761018 147.888342 \r\nL 89.065686 152.349816 \r\nL 89.370355 150.788741 \r\nL 89.675023 153.87235 \r\nL 90.28436 156.266295 \r\nL 90.589028 133.692867 \r\nL 90.893696 139.621984 \r\nL 91.198365 137.251178 \r\nL 91.503033 144.542789 \r\nL 91.807701 140.230523 \r\nL 92.112369 151.666811 \r\nL 92.417038 152.35182 \r\nL 92.721706 148.411498 \r\nL 93.026374 149.158184 \r\nL 93.635711 142.883459 \r\nL 93.940379 148.506159 \r\nL 94.245048 150.889945 \r\nL 94.549716 160.317767 \r\nL 94.854384 139.462215 \r\nL 95.159053 148.500555 \r\nL 95.463721 148.449921 \r\nL 95.768389 141.752952 \r\nL 96.073057 149.313622 \r\nL 96.377726 145.817091 \r\nL 96.987062 150.175722 \r\nL 97.291731 153.438548 \r\nL 97.596399 142.376346 \r\nL 97.901067 146.782675 \r\nL 98.205736 159.675588 \r\nL 98.510404 156.021832 \r\nL 98.815072 142.560123 \r\nL 99.11974 156.821066 \r\nL 99.424409 151.251359 \r\nL 99.729077 148.565212 \r\nL 100.033745 154.360502 \r\nL 100.338414 147.051749 \r\nL 100.643082 148.00191 \r\nL 100.94775 141.378972 \r\nL 101.252419 145.552851 \r\nL 101.861755 160.015417 \r\nL 102.166424 134.495405 \r\nL 102.471092 144.938814 \r\nL 102.77576 133.788822 \r\nL 103.080428 148.673219 \r\nL 103.385097 141.512168 \r\nL 103.689765 139.485554 \r\nL 104.299102 165.653403 \r\nL 104.60377 147.575865 \r\nL 104.908438 142.334768 \r\nL 105.213107 146.534786 \r\nL 105.517775 154.076974 \r\nL 105.822443 124.522297 \r\nL 106.127111 142.897127 \r\nL 106.43178 145.526437 \r\nL 106.736448 152.563676 \r\nL 107.041116 146.779483 \r\nL 107.345785 133.803488 \r\nL 107.650453 136.025097 \r\nL 107.955121 152.874249 \r\nL 108.25979 151.242457 \r\nL 108.564458 154.340572 \r\nL 108.869126 148.27503 \r\nL 109.173795 138.131092 \r\nL 109.478463 150.538541 \r\nL 109.783131 147.482423 \r\nL 110.087799 150.012909 \r\nL 110.392468 147.00883 \r\nL 110.697136 151.325793 \r\nL 111.306473 144.341431 \r\nL 111.611141 145.224877 \r\nL 112.220478 139.576377 \r\nL 112.525146 144.959799 \r\nL 112.829814 152.368765 \r\nL 113.134482 153.967637 \r\nL 113.439151 143.570212 \r\nL 113.743819 148.785355 \r\nL 114.048487 128.568059 \r\nL 114.353156 144.054048 \r\nL 114.657824 145.687044 \r\nL 114.962492 156.205863 \r\nL 115.571829 143.77209 \r\nL 115.876497 141.072751 \r\nL 116.181166 155.654326 \r\nL 116.485834 141.730302 \r\nL 116.790502 152.519664 \r\nL 117.09517 152.255658 \r\nL 117.399839 147.437881 \r\nL 117.704507 155.575109 \r\nL 118.009175 144.670243 \r\nL 118.313844 148.795843 \r\nL 118.618512 145.935781 \r\nL 118.92318 154.085585 \r\nL 119.227849 137.534748 \r\nL 119.532517 157.711182 \r\nL 119.837185 155.950801 \r\nL 120.141854 143.117965 \r\nL 120.446522 154.058131 \r\nL 120.75119 138.386011 \r\nL 121.055858 157.175641 \r\nL 121.360527 150.311309 \r\nL 121.665195 147.105538 \r\nL 121.969863 166.579328 \r\nL 122.274532 139.553759 \r\nL 122.5792 144.877004 \r\nL 123.188537 148.524631 \r\nL 123.493205 141.357425 \r\nL 123.797873 143.21288 \r\nL 124.102541 154.363137 \r\nL 124.40721 155.395224 \r\nL 124.711878 134.363217 \r\nL 125.016546 142.618212 \r\nL 125.321215 154.338245 \r\nL 125.930551 144.522276 \r\nL 126.23522 151.955503 \r\nL 126.539888 147.483812 \r\nL 126.844556 159.100992 \r\nL 127.149225 142.789168 \r\nL 127.453893 146.356635 \r\nL 127.758561 155.694472 \r\nL 128.063229 131.769684 \r\nL 128.367898 150.492594 \r\nL 128.672566 148.682624 \r\nL 128.977234 144.837266 \r\nL 129.281903 157.582052 \r\nL 129.586571 147.381585 \r\nL 129.891239 143.258711 \r\nL 130.195908 143.524467 \r\nL 130.500576 148.028144 \r\nL 130.805244 158.633868 \r\nL 131.109912 143.697777 \r\nL 131.414581 164.665949 \r\nL 131.719249 146.875019 \r\nL 132.023917 151.828542 \r\nL 132.328586 161.811862 \r\nL 132.633254 153.798934 \r\nL 132.937922 152.951759 \r\nL 133.242591 150.128296 \r\nL 133.547259 156.315121 \r\nL 133.851927 129.451452 \r\nL 134.156596 156.908278 \r\nL 134.461264 155.947816 \r\nL 134.765932 158.01894 \r\nL 135.0706 151.459604 \r\nL 135.375269 167.641938 \r\nL 135.679937 148.76393 \r\nL 135.984605 169.870112 \r\nL 136.593942 138.828725 \r\nL 136.89861 135.461498 \r\nL 137.203279 156.300551 \r\nL 137.507947 160.992125 \r\nL 137.812615 158.209053 \r\nL 138.117283 156.625917 \r\nL 138.421952 158.170873 \r\nL 138.72662 149.647773 \r\nL 139.031288 153.079632 \r\nL 139.335957 154.448674 \r\nL 139.640625 157.015845 \r\nL 139.945293 145.534999 \r\nL 140.249962 146.637989 \r\nL 140.55463 160.468577 \r\nL 140.859298 142.964865 \r\nL 141.468635 156.829995 \r\nL 141.773303 159.568886 \r\nL 142.077971 153.557965 \r\nL 142.687308 158.284775 \r\nL 142.991976 159.125365 \r\nL 143.296645 168.214562 \r\nL 143.601313 142.790526 \r\nL 143.905981 151.232309 \r\nL 144.21065 164.361446 \r\nL 144.515318 138.697406 \r\nL 144.819986 152.943058 \r\nL 145.124654 143.050174 \r\nL 145.429323 147.219827 \r\nL 145.733991 147.781687 \r\nL 146.038659 146.669706 \r\nL 146.343328 142.81729 \r\nL 146.647996 153.936493 \r\nL 146.952664 155.544697 \r\nL 147.257333 170.113329 \r\nL 147.562001 164.727547 \r\nL 147.866669 144.162654 \r\nL 148.171338 154.679506 \r\nL 148.476006 150.125863 \r\nL 148.780674 140.465803 \r\nL 149.390011 152.568458 \r\nL 149.999347 143.547201 \r\nL 150.304016 167.142684 \r\nL 150.608684 161.617704 \r\nL 150.913352 166.060663 \r\nL 151.218021 150.020083 \r\nL 151.522689 165.02081 \r\nL 151.827357 166.430264 \r\nL 152.132025 142.020256 \r\nL 152.436694 146.192587 \r\nL 152.741362 155.27637 \r\nL 153.04603 137.753062 \r\nL 153.350699 151.632995 \r\nL 153.655367 152.585027 \r\nL 153.960035 150.702897 \r\nL 154.264704 163.126697 \r\nL 154.569372 169.316008 \r\nL 154.87404 151.372164 \r\nL 155.178709 149.300972 \r\nL 155.483377 140.132384 \r\nL 155.788045 147.545087 \r\nL 156.092713 144.712404 \r\nL 156.397382 143.541072 \r\nL 156.70205 153.502803 \r\nL 157.006718 150.634703 \r\nL 157.311387 162.375043 \r\nL 157.616055 157.666364 \r\nL 157.920723 145.373215 \r\nL 158.225392 165.860223 \r\nL 158.53006 157.960915 \r\nL 158.834728 156.823277 \r\nL 159.139396 146.640656 \r\nL 159.444065 149.408183 \r\nL 159.748733 159.109603 \r\nL 160.053401 156.722057 \r\nL 160.35807 146.729549 \r\nL 160.662738 159.686801 \r\nL 160.967406 155.146402 \r\nL 161.272075 140.55968 \r\nL 161.576743 151.555066 \r\nL 161.881411 149.787464 \r\nL 162.18608 149.837886 \r\nL 162.490748 148.537027 \r\nL 162.795416 159.138133 \r\nL 163.100084 159.668658 \r\nL 163.404753 140.553752 \r\nL 164.014089 172.800202 \r\nL 164.318758 142.396812 \r\nL 164.623426 153.816722 \r\nL 165.232763 145.656908 \r\nL 165.842099 161.879059 \r\nL 166.146768 154.522467 \r\nL 166.451436 154.130699 \r\nL 166.756104 162.364481 \r\nL 167.060772 139.675439 \r\nL 167.365441 145.658806 \r\nL 167.670109 162.413476 \r\nL 167.974777 156.747931 \r\nL 168.279446 146.407926 \r\nL 168.584114 156.35339 \r\nL 168.888782 146.968509 \r\nL 169.193451 151.552389 \r\nL 169.498119 159.481265 \r\nL 170.107455 146.75459 \r\nL 170.412124 154.897502 \r\nL 170.716792 154.724477 \r\nL 171.02146 161.564043 \r\nL 171.326129 145.960933 \r\nL 171.630797 156.356264 \r\nL 171.935465 155.047298 \r\nL 172.240134 146.130845 \r\nL 172.544802 154.088941 \r\nL 172.84947 155.845923 \r\nL 173.154139 154.042617 \r\nL 173.763475 166.015459 \r\nL 174.068143 145.126255 \r\nL 174.372812 144.976426 \r\nL 174.982148 155.195642 \r\nL 175.286817 148.750664 \r\nL 175.591485 159.475216 \r\nL 175.896153 153.123829 \r\nL 176.50549 152.524356 \r\nL 176.810158 154.012333 \r\nL 177.114826 169.372018 \r\nL 177.419495 148.771172 \r\nL 177.724163 143.007578 \r\nL 178.028831 160.324114 \r\nL 178.3335 159.518893 \r\nL 178.638168 161.961224 \r\nL 179.247505 137.176727 \r\nL 179.552173 171.6251 \r\nL 179.856841 154.483073 \r\nL 180.16151 159.280257 \r\nL 180.466178 159.804023 \r\nL 180.770846 147.582715 \r\nL 181.075514 148.778701 \r\nL 181.380183 147.382767 \r\nL 181.684851 155.486206 \r\nL 181.989519 145.48515 \r\nL 182.294188 156.341376 \r\nL 182.598856 144.989909 \r\nL 182.903524 142.072427 \r\nL 183.208193 148.288338 \r\nL 183.512861 157.42688 \r\nL 183.817529 134.363768 \r\nL 184.122197 143.974487 \r\nL 184.426866 137.962649 \r\nL 184.731534 147.102776 \r\nL 185.036202 144.60768 \r\nL 185.340871 167.026178 \r\nL 185.645539 150.41157 \r\nL 185.950207 155.053093 \r\nL 186.254876 142.454402 \r\nL 186.559544 158.731312 \r\nL 187.168881 135.222369 \r\nL 187.473549 137.408953 \r\nL 187.778217 149.430822 \r\nL 188.082885 143.939409 \r\nL 188.387554 154.099126 \r\nL 188.692222 156.421637 \r\nL 188.99689 146.091823 \r\nL 189.301559 145.679711 \r\nL 189.606227 152.476193 \r\nL 189.910895 152.222134 \r\nL 190.215564 145.920347 \r\nL 190.520232 156.247187 \r\nL 190.8249 137.775001 \r\nL 191.129568 151.797494 \r\nL 191.434237 159.295627 \r\nL 191.738905 150.117756 \r\nL 192.043573 151.334526 \r\nL 192.348242 147.916962 \r\nL 192.65291 147.199918 \r\nL 192.957578 143.830984 \r\nL 193.566915 158.401795 \r\nL 193.871583 148.111915 \r\nL 194.176252 150.350194 \r\nL 194.785588 174.051891 \r\nL 195.090256 152.942957 \r\nL 195.394925 151.86977 \r\nL 195.699593 160.948861 \r\nL 196.004261 161.542522 \r\nL 196.30893 163.668304 \r\nL 196.613598 142.419494 \r\nL 196.918266 159.983553 \r\nL 197.222935 156.1635 \r\nL 197.527603 154.112254 \r\nL 197.832271 156.312152 \r\nL 198.136939 146.158681 \r\nL 198.746276 157.639234 \r\nL 199.050944 146.721707 \r\nL 199.355613 143.399998 \r\nL 199.660281 145.247145 \r\nL 199.964949 162.065657 \r\nL 200.574286 143.238521 \r\nL 201.183623 159.880906 \r\nL 201.488291 156.352733 \r\nL 201.792959 155.680358 \r\nL 202.097627 155.705076 \r\nL 202.402296 145.982782 \r\nL 202.706964 149.867206 \r\nL 203.011632 140.367134 \r\nL 203.316301 153.980436 \r\nL 203.620969 151.260457 \r\nL 203.925637 146.849463 \r\nL 204.230306 150.12275 \r\nL 204.534974 138.670695 \r\nL 204.839642 142.679121 \r\nL 205.144311 152.466857 \r\nL 205.448979 154.333245 \r\nL 205.753647 145.556239 \r\nL 206.058315 171.36725 \r\nL 206.667652 136.294342 \r\nL 206.97232 165.236112 \r\nL 207.276989 146.392068 \r\nL 207.886325 159.554581 \r\nL 208.190994 150.633176 \r\nL 208.495662 145.129245 \r\nL 208.80033 134.591381 \r\nL 209.409667 150.655031 \r\nL 209.714335 152.773263 \r\nL 210.019003 150.98695 \r\nL 210.323672 144.16628 \r\nL 210.933008 158.766348 \r\nL 211.237677 153.037168 \r\nL 211.542345 152.86759 \r\nL 211.847013 152.474894 \r\nL 212.151682 149.102991 \r\nL 212.45635 151.34593 \r\nL 212.761018 162.101864 \r\nL 213.065686 160.025666 \r\nL 213.370355 149.352008 \r\nL 213.675023 154.653929 \r\nL 213.979691 148.515098 \r\nL 214.28436 145.708072 \r\nL 214.589028 158.20496 \r\nL 215.198365 154.072478 \r\nL 215.503033 139.924791 \r\nL 216.112369 166.733854 \r\nL 216.721706 146.16077 \r\nL 217.026374 156.106881 \r\nL 217.331043 160.425233 \r\nL 217.635711 158.542573 \r\nL 217.940379 151.22521 \r\nL 218.245048 157.448734 \r\nL 218.549716 153.877615 \r\nL 218.854384 153.330452 \r\nL 219.159053 160.372341 \r\nL 219.463721 161.064115 \r\nL 219.768389 156.699185 \r\nL 220.073057 129.303771 \r\nL 220.377726 158.480519 \r\nL 220.682394 149.50989 \r\nL 220.987062 147.019397 \r\nL 221.291731 151.656387 \r\nL 221.596399 143.325431 \r\nL 221.901067 157.801289 \r\nL 222.205736 153.869042 \r\nL 222.510404 145.704366 \r\nL 222.815072 155.365273 \r\nL 223.11974 153.804506 \r\nL 223.424409 160.924281 \r\nL 223.729077 158.5462 \r\nL 224.033745 154.381747 \r\nL 224.338414 164.945331 \r\nL 224.643082 154.142114 \r\nL 224.94775 150.763461 \r\nL 225.252419 148.745612 \r\nL 225.557087 154.926774 \r\nL 225.861755 148.466384 \r\nL 226.166424 161.909784 \r\nL 226.471092 149.678281 \r\nL 226.77576 154.550991 \r\nL 227.080428 141.029869 \r\nL 227.385097 140.630302 \r\nL 227.689765 141.80595 \r\nL 227.994433 161.444298 \r\nL 228.299102 157.845258 \r\nL 228.60377 147.532352 \r\nL 228.908438 155.751537 \r\nL 229.213107 142.13271 \r\nL 229.517775 157.68929 \r\nL 229.822443 161.077211 \r\nL 230.127111 166.99978 \r\nL 230.43178 158.490995 \r\nL 230.736448 157.823022 \r\nL 231.041116 150.596778 \r\nL 231.345785 157.983783 \r\nL 231.955121 161.882893 \r\nL 232.25979 142.320559 \r\nL 232.564458 161.077126 \r\nL 232.869126 154.497876 \r\nL 233.173795 167.554302 \r\nL 233.478463 157.152773 \r\nL 233.783131 158.243738 \r\nL 234.087799 162.174411 \r\nL 234.392468 148.282681 \r\nL 234.697136 151.134249 \r\nL 235.001804 161.641987 \r\nL 235.306473 161.260632 \r\nL 235.611141 170.220249 \r\nL 236.220478 145.812711 \r\nL 236.525146 156.282005 \r\nL 236.829814 154.554305 \r\nL 237.134482 158.745739 \r\nL 237.439151 156.959029 \r\nL 237.743819 165.982645 \r\nL 238.048487 154.035115 \r\nL 238.353156 160.037755 \r\nL 238.657824 157.378367 \r\nL 238.962492 161.394359 \r\nL 239.267161 162.782377 \r\nL 239.571829 160.335837 \r\nL 239.876497 147.239683 \r\nL 240.181166 160.040008 \r\nL 240.485834 155.61332 \r\nL 240.790502 166.256783 \r\nL 241.09517 155.145379 \r\nL 241.399839 170.188617 \r\nL 241.704507 161.329404 \r\nL 242.009175 147.839383 \r\nL 242.313844 157.780966 \r\nL 242.618512 161.691926 \r\nL 243.227849 155.725441 \r\nL 243.532517 134.652089 \r\nL 243.837185 150.887909 \r\nL 244.141854 157.585743 \r\nL 244.446522 149.188984 \r\nL 244.75119 149.807744 \r\nL 245.055858 168.735119 \r\nL 245.360527 160.802823 \r\nL 245.665195 159.355746 \r\nL 245.969863 144.650907 \r\nL 246.274532 156.778322 \r\nL 246.5792 160.210599 \r\nL 246.883868 135.273925 \r\nL 247.188537 157.207765 \r\nL 247.493205 154.256329 \r\nL 247.797873 148.340902 \r\nL 248.40721 169.459766 \r\nL 248.711878 161.364821 \r\nL 249.016546 166.642957 \r\nL 249.321215 147.875585 \r\nL 249.625883 149.455116 \r\nL 249.930551 156.115024 \r\nL 250.23522 147.787228 \r\nL 250.844556 166.078414 \r\nL 251.453893 153.620454 \r\nL 251.758561 142.406387 \r\nL 252.063229 163.933275 \r\nL 252.367898 163.420034 \r\nL 252.672566 154.373105 \r\nL 252.977234 153.561236 \r\nL 253.281903 158.884672 \r\nL 253.586571 160.973292 \r\nL 253.891239 149.833809 \r\nL 254.195908 162.787992 \r\nL 254.500576 151.457977 \r\nL 254.805244 160.219474 \r\nL 255.109912 157.661762 \r\nL 255.414581 137.079892 \r\nL 255.719249 157.641275 \r\nL 256.023917 157.19495 \r\nL 256.328586 140.914869 \r\nL 256.633254 150.856468 \r\nL 256.937922 168.919411 \r\nL 257.242591 147.530576 \r\nL 257.547259 175.720314 \r\nL 257.851927 144.534767 \r\nL 258.156596 155.513633 \r\nL 258.461264 160.252755 \r\nL 258.765932 131.754595 \r\nL 259.0706 164.289599 \r\nL 259.375269 146.603988 \r\nL 259.679937 159.394992 \r\nL 259.984605 159.461706 \r\nL 260.289274 147.454063 \r\nL 260.593942 159.934398 \r\nL 260.89861 167.352864 \r\nL 261.203279 157.376172 \r\nL 261.507947 151.368612 \r\nL 261.812615 158.699034 \r\nL 262.117283 159.095038 \r\nL 262.421952 159.709229 \r\nL 262.72662 178.002716 \r\nL 263.031288 152.594295 \r\nL 263.335957 161.428106 \r\nL 263.640625 158.626413 \r\nL 263.945293 170.171396 \r\nL 264.249962 148.466977 \r\nL 264.55463 155.221203 \r\nL 264.859298 144.623914 \r\nL 265.163967 149.169234 \r\nL 265.468635 163.898128 \r\nL 265.773303 145.498034 \r\nL 266.077971 161.890734 \r\nL 266.38264 166.189766 \r\nL 266.687308 173.290496 \r\nL 266.991976 149.591822 \r\nL 267.601313 167.043579 \r\nL 267.905981 144.190627 \r\nL 268.21065 154.413686 \r\nL 268.819986 160.033863 \r\nL 269.124654 147.907227 \r\nL 269.429323 160.228684 \r\nL 269.733991 158.967049 \r\nL 270.038659 163.438999 \r\nL 270.343328 158.210808 \r\nL 270.647996 155.715919 \r\nL 270.952664 154.525107 \r\nL 271.257333 156.364668 \r\nL 271.562001 164.476801 \r\nL 271.866669 151.218869 \r\nL 272.171338 144.402282 \r\nL 272.780674 157.838721 \r\nL 273.085342 160.465904 \r\nL 273.390011 167.238692 \r\nL 273.694679 147.143516 \r\nL 273.999347 147.745867 \r\nL 274.304016 159.759285 \r\nL 274.608684 145.782241 \r\nL 274.913352 149.941609 \r\nL 275.218021 165.814864 \r\nL 275.522689 167.858927 \r\nL 275.827357 159.612319 \r\nL 276.132025 163.852192 \r\nL 276.436694 151.981217 \r\nL 276.741362 144.915369 \r\nL 277.04603 162.995845 \r\nL 277.350699 145.761971 \r\nL 277.655367 164.376578 \r\nL 278.264704 161.94184 \r\nL 278.569372 150.719651 \r\nL 278.87404 145.337639 \r\nL 279.178709 157.808044 \r\nL 279.483377 158.535718 \r\nL 279.788045 156.982522 \r\nL 280.092713 161.294406 \r\nL 280.397382 160.746575 \r\nL 280.70205 159.224846 \r\nL 281.006718 149.308607 \r\nL 281.311387 150.01423 \r\nL 281.616055 144.421872 \r\nL 281.920723 165.854719 \r\nL 282.225392 135.501581 \r\nL 282.53006 163.521773 \r\nL 282.834728 151.160695 \r\nL 283.139396 154.488152 \r\nL 283.444065 151.274529 \r\nL 283.748733 151.907451 \r\nL 284.053401 151.086404 \r\nL 284.35807 162.015855 \r\nL 284.662738 152.270684 \r\nL 284.967406 152.345245 \r\nL 285.272075 154.877365 \r\nL 285.576743 147.760061 \r\nL 285.881411 161.425433 \r\nL 286.18608 162.67743 \r\nL 286.490748 155.458248 \r\nL 286.795416 154.196682 \r\nL 287.100084 154.63103 \r\nL 287.404753 150.602589 \r\nL 287.709421 158.895891 \r\nL 288.014089 156.288192 \r\nL 288.318758 160.914032 \r\nL 288.623426 160.53202 \r\nL 288.928094 154.627297 \r\nL 289.232763 154.785444 \r\nL 289.537431 151.524363 \r\nL 289.842099 146.294114 \r\nL 290.146768 150.748176 \r\nL 290.451436 133.234788 \r\nL 290.756104 155.227289 \r\nL 291.060772 160.522636 \r\nL 291.365441 139.995073 \r\nL 291.670109 151.937036 \r\nL 291.974777 152.643242 \r\nL 292.279446 168.927639 \r\nL 292.888782 148.814028 \r\nL 293.193451 151.394963 \r\nL 293.498119 163.222477 \r\nL 293.802787 162.415359 \r\nL 294.107455 142.717624 \r\nL 294.412124 164.205489 \r\nL 294.716792 144.927123 \r\nL 295.02146 159.100807 \r\nL 295.326129 149.306412 \r\nL 295.630797 159.003717 \r\nL 295.935465 159.106952 \r\nL 296.240134 151.124806 \r\nL 296.544802 164.694961 \r\nL 296.84947 148.77359 \r\nL 297.154139 155.396629 \r\nL 297.458807 145.970747 \r\nL 297.763475 152.212373 \r\nL 298.068143 167.223958 \r\nL 298.372812 159.054807 \r\nL 298.67748 167.61861 \r\nL 298.982148 154.047813 \r\nL 299.286817 150.38365 \r\nL 299.591485 144.42034 \r\nL 299.896153 162.561439 \r\nL 300.200822 163.450722 \r\nL 300.50549 148.352545 \r\nL 300.810158 168.328294 \r\nL 301.114826 162.140526 \r\nL 301.419495 146.213005 \r\nL 301.724163 156.351174 \r\nL 302.028831 137.805032 \r\nL 302.3335 153.022529 \r\nL 302.638168 144.11412 \r\nL 302.942836 159.028986 \r\nL 303.247505 144.153587 \r\nL 303.856841 164.147305 \r\nL 304.16151 165.458588 \r\nL 304.466178 154.843162 \r\nL 304.770846 165.339161 \r\nL 305.075514 149.442349 \r\nL 305.380183 159.290755 \r\nL 305.684851 154.455461 \r\nL 305.989519 154.328722 \r\nL 306.294188 141.177009 \r\nL 306.598856 157.058223 \r\nL 306.903524 149.335551 \r\nL 307.208193 160.393808 \r\nL 307.512861 152.987266 \r\nL 307.817529 152.108952 \r\nL 308.122197 156.547877 \r\nL 308.426866 154.195304 \r\nL 308.731534 142.537919 \r\nL 309.036202 158.242312 \r\nL 309.340871 149.327683 \r\nL 309.645539 157.766338 \r\nL 309.950207 155.801079 \r\nL 310.254876 154.565683 \r\nL 310.559544 147.732215 \r\nL 310.864212 159.876654 \r\nL 311.168881 141.417676 \r\nL 311.473549 148.16828 \r\nL 311.778217 169.816392 \r\nL 312.082885 166.898645 \r\nL 312.387554 158.74804 \r\nL 312.692222 159.271169 \r\nL 312.99689 143.681511 \r\nL 313.301559 160.004973 \r\nL 313.606227 151.251046 \r\nL 313.910895 167.784514 \r\nL 314.215564 155.860604 \r\nL 314.520232 165.121536 \r\nL 314.8249 147.792975 \r\nL 315.129568 156.827572 \r\nL 315.434237 148.066451 \r\nL 315.738905 155.553212 \r\nL 316.043573 166.448863 \r\nL 316.348242 160.850255 \r\nL 316.65291 147.90754 \r\nL 316.957578 156.75661 \r\nL 317.262247 160.304072 \r\nL 317.566915 153.625353 \r\nL 317.871583 165.836777 \r\nL 318.176252 149.225329 \r\nL 318.48092 142.500126 \r\nL 318.785588 141.574943 \r\nL 319.090256 160.551521 \r\nL 319.394925 155.488109 \r\nL 319.699593 159.456977 \r\nL 320.004261 146.950455 \r\nL 320.30893 157.287173 \r\nL 320.613598 159.558372 \r\nL 320.918266 154.580094 \r\nL 321.222935 143.895679 \r\nL 321.527603 152.80517 \r\nL 321.832271 155.429952 \r\nL 322.136939 149.711869 \r\nL 322.441608 149.403644 \r\nL 322.746276 148.383779 \r\nL 323.050944 150.220561 \r\nL 323.355613 150.734804 \r\nL 323.660281 160.70619 \r\nL 323.964949 148.902889 \r\nL 324.269618 148.31646 \r\nL 324.574286 154.869852 \r\nL 324.878954 147.558114 \r\nL 325.183623 163.461904 \r\nL 325.488291 153.592555 \r\nL 325.792959 153.278249 \r\nL 326.097627 158.237286 \r\nL 326.402296 173.911723 \r\nL 326.706964 167.541657 \r\nL 327.011632 167.565786 \r\nL 327.620969 144.433409 \r\nL 327.925637 155.096606 \r\nL 328.534974 160.260421 \r\nL 328.839642 147.816049 \r\nL 329.144311 151.015724 \r\nL 329.448979 165.045777 \r\nL 329.753647 171.129123 \r\nL 330.058315 154.204518 \r\nL 330.362984 163.174744 \r\nL 330.667652 158.018898 \r\nL 330.97232 157.499883 \r\nL 331.276989 156.384189 \r\nL 331.581657 168.101714 \r\nL 331.886325 147.686437 \r\nL 332.190994 168.57272 \r\nL 332.495662 137.404542 \r\nL 332.80033 162.377375 \r\nL 333.104998 159.239241 \r\nL 333.409667 160.149881 \r\nL 333.714335 164.551559 \r\nL 334.019003 166.686577 \r\nL 334.323672 150.78609 \r\nL 334.62834 161.889112 \r\nL 334.933008 153.401429 \r\nL 335.237677 157.824851 \r\nL 335.542345 153.536963 \r\nL 335.847013 154.17813 \r\nL 336.151682 168.505772 \r\nL 336.45635 148.465079 \r\nL 336.761018 151.337124 \r\nL 337.065686 149.138403 \r\nL 337.370355 155.284068 \r\nL 337.979691 175.735473 \r\nL 338.589028 154.881675 \r\nL 338.893696 160.033821 \r\nL 339.198365 156.623064 \r\nL 339.503033 151.663073 \r\nL 339.807701 159.693365 \r\nL 340.112369 160.693677 \r\nL 340.417038 164.234459 \r\nL 340.721706 163.970002 \r\nL 341.026374 174.477353 \r\nL 341.331043 171.361662 \r\nL 341.635711 158.213687 \r\nL 341.940379 154.921175 \r\nL 342.245048 183.467927 \r\nL 342.549716 150.595877 \r\nL 342.854384 158.568017 \r\nL 343.159053 160.455422 \r\nL 343.463721 171.039334 \r\nL 343.768389 167.15877 \r\nL 344.073057 140.44432 \r\nL 344.377726 155.371185 \r\nL 344.682394 145.598257 \r\nL 344.987062 163.216884 \r\nL 345.291731 153.287898 \r\nL 345.596399 154.492352 \r\nL 345.901067 154.117943 \r\nL 346.205736 156.399645 \r\nL 346.510404 159.544003 \r\nL 346.815072 149.807495 \r\nL 347.11974 165.915607 \r\nL 347.424409 163.066096 \r\nL 347.729077 167.939967 \r\nL 348.033745 148.838889 \r\nL 348.338414 157.637622 \r\nL 348.643082 157.293085 \r\nL 348.94775 157.997695 \r\nL 349.252419 150.897617 \r\nL 349.557087 164.583243 \r\nL 349.861755 163.872424 \r\nL 350.166424 150.180982 \r\nL 350.471092 151.69288 \r\nL 350.77576 175.328255 \r\nL 351.385097 155.635058 \r\nL 351.689765 161.28263 \r\nL 351.994433 149.854667 \r\nL 352.299102 153.507962 \r\nL 352.60377 153.25888 \r\nL 352.908438 163.717962 \r\nL 353.213107 160.790989 \r\nL 353.517775 152.727745 \r\nL 353.822443 158.545049 \r\nL 353.822443 158.545049 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p4a85c7a2d8)\" d=\"M 49.458807 179.869046 \r\nL 49.763475 129.702161 \r\nL 50.068143 146.418276 \r\nL 50.67748 129.918545 \r\nL 50.982148 167.164459 \r\nL 51.286817 162.613302 \r\nL 51.591485 163.062013 \r\nL 51.896153 177.353697 \r\nL 52.200822 169.449432 \r\nL 52.50549 186.933537 \r\nL 52.810158 184.65797 \r\nL 53.114826 105.777707 \r\nL 53.419495 128.316353 \r\nL 53.724163 170.745302 \r\nL 54.028831 191.102915 \r\nL 54.3335 184.192918 \r\nL 54.638168 163.364538 \r\nL 54.942836 105.688008 \r\nL 55.247505 161.98696 \r\nL 55.552173 195.349089 \r\nL 55.856841 128.379966 \r\nL 56.16151 160.186358 \r\nL 56.466178 165.222926 \r\nL 56.770846 193.282764 \r\nL 57.075514 172.857702 \r\nL 57.380183 183.233113 \r\nL 57.684851 179.534036 \r\nL 57.989519 177.330045 \r\nL 58.294188 162.744998 \r\nL 58.598856 153.995441 \r\nL 58.903524 176.851129 \r\nL 59.208193 152.807026 \r\nL 59.512861 156.904652 \r\nL 59.817529 155.925002 \r\nL 60.122197 168.514118 \r\nL 60.426866 113.879496 \r\nL 60.731534 143.829043 \r\nL 61.036202 158.968872 \r\nL 61.340871 152.944707 \r\nL 61.645539 133.980945 \r\nL 61.950207 154.012333 \r\nL 62.254876 131.973142 \r\nL 62.864212 165.113196 \r\nL 63.168881 117.144353 \r\nL 63.473549 158.669348 \r\nL 63.778217 102.670584 \r\nL 64.082885 98.041923 \r\nL 64.387554 107.853417 \r\nL 64.692222 177.491628 \r\nL 64.99689 135.694201 \r\nL 65.301559 131.901884 \r\nL 65.606227 105.383718 \r\nL 65.910895 142.122573 \r\nL 66.215564 137.054019 \r\nL 66.520232 174.536952 \r\nL 66.8249 186.579525 \r\nL 67.129568 152.282767 \r\nL 67.434237 131.137255 \r\nL 67.738905 129.941163 \r\nL 68.043573 115.689292 \r\nL 68.348242 118.764147 \r\nL 68.957578 80.264677 \r\nL 69.262247 131.830487 \r\nL 69.566915 117.7325 \r\nL 69.871583 121.458088 \r\nL 70.176252 146.450273 \r\nL 70.48092 112.20119 \r\nL 70.785588 128.788218 \r\nL 71.090256 188.339906 \r\nL 71.394925 157.182883 \r\nL 71.699593 144.299402 \r\nL 72.004261 138.566066 \r\nL 72.30893 97.931069 \r\nL 72.613598 133.32331 \r\nL 72.918266 153.914776 \r\nL 73.222935 129.273804 \r\nL 73.527603 150.636485 \r\nL 73.832271 221.81231 \r\nL 74.136939 151.208922 \r\nL 74.441608 171.177646 \r\nL 74.746276 176.880974 \r\nL 75.050944 165.070436 \r\nL 75.355613 94.552273 \r\nL 75.660281 88.43429 \r\nL 75.964949 167.161845 \r\nL 76.269618 132.469163 \r\nL 76.574286 121.573925 \r\nL 76.878954 151.943308 \r\nL 77.183623 121.502073 \r\nL 77.488291 115.135639 \r\nL 77.792959 170.767157 \r\nL 78.097627 132.008644 \r\nL 78.402296 146.761976 \r\nL 78.706964 105.145288 \r\nL 79.011632 84.751988 \r\nL 79.316301 100.076002 \r\nL 79.620969 99.72657 \r\nL 79.925637 100.482016 \r\nL 80.230306 146.960635 \r\nL 80.839642 156.303631 \r\nL 81.144311 143.289791 \r\nL 81.753647 185.569394 \r\nL 82.058315 88.928889 \r\nL 82.362984 142.154205 \r\nL 82.667652 143.581028 \r\nL 83.276989 170.182127 \r\nL 83.581657 167.622051 \r\nL 83.886325 145.725107 \r\nL 84.190994 69.145845 \r\nL 84.80033 119.913592 \r\nL 85.104998 160.426235 \r\nL 85.409667 166.697546 \r\nL 85.714335 141.193753 \r\nL 86.019003 165.985073 \r\nL 86.323672 162.013883 \r\nL 86.62834 139.409121 \r\nL 86.933008 32.201761 \r\nL 87.237677 152.532267 \r\nL 87.542345 128.878691 \r\nL 87.847013 160.139346 \r\nL 88.151682 135.19641 \r\nL 88.45635 136.168802 \r\nL 88.761018 144.09745 \r\nL 89.370355 131.703866 \r\nL 89.675023 156.269805 \r\nL 89.979691 152.780457 \r\nL 90.28436 172.748153 \r\nL 90.589028 150.919398 \r\nL 90.893696 120.214755 \r\nL 91.198365 224.016508 \r\nL 91.503033 168.579173 \r\nL 91.807701 164.45746 \r\nL 92.112369 171.635031 \r\nL 92.417038 158.020679 \r\nL 92.721706 100.595149 \r\nL 93.026374 176.745498 \r\nL 93.331043 142.817682 \r\nL 93.635711 156.054571 \r\nL 93.940379 124.002905 \r\nL 94.245048 166.563863 \r\nL 94.549716 187.98285 \r\nL 94.854384 197.005772 \r\nL 95.159053 113.788631 \r\nL 95.463721 151.136481 \r\nL 95.768389 150.453683 \r\nL 96.073057 172.708913 \r\nL 96.377726 219.008321 \r\nL 96.682394 165.63632 \r\nL 96.987062 77.80327 \r\nL 97.291731 123.888468 \r\nL 97.596399 136.759314 \r\nL 97.901067 164.108998 \r\nL 98.205736 146.00802 \r\nL 98.510404 158.190708 \r\nL 98.815072 153.68132 \r\nL 99.11974 108.234406 \r\nL 99.424409 94.532105 \r\nL 99.729077 157.724559 \r\nL 100.033745 175.680502 \r\nL 100.338414 163.159416 \r\nL 100.643082 114.192959 \r\nL 100.94775 142.079701 \r\nL 101.252419 95.340602 \r\nL 101.557087 145.907293 \r\nL 101.861755 136.83948 \r\nL 102.166424 96.905117 \r\nL 102.471092 157.222293 \r\nL 102.77576 159.0158 \r\nL 103.080428 185.265612 \r\nL 103.385097 118.660918 \r\nL 103.689765 113.577963 \r\nL 104.60377 207.116548 \r\nL 104.908438 168.072861 \r\nL 105.213107 99.96149 \r\nL 105.517775 171.97695 \r\nL 105.822443 186.513903 \r\nL 106.127111 110.892876 \r\nL 106.43178 119.267113 \r\nL 106.736448 139.694812 \r\nL 107.041116 80.400217 \r\nL 107.345785 80.350739 \r\nL 107.650453 123.992344 \r\nL 107.955121 142.464126 \r\nL 108.25979 118.689347 \r\nL 108.564458 193.280142 \r\nL 108.869126 206.209087 \r\nL 109.173795 174.954466 \r\nL 109.478463 123.987158 \r\nL 109.783131 135.989341 \r\nL 110.087799 143.409594 \r\nL 110.392468 156.68611 \r\nL 110.697136 178.028383 \r\nL 111.001804 170.649485 \r\nL 111.306473 156.06964 \r\nL 111.611141 154.76158 \r\nL 111.915809 161.580394 \r\nL 112.220478 149.240535 \r\nL 112.525146 159.470481 \r\nL 112.829814 133.462794 \r\nL 113.134482 149.470714 \r\nL 113.439151 143.736471 \r\nL 113.743819 147.227192 \r\nL 114.048487 186.226817 \r\nL 114.353156 168.615412 \r\nL 114.657824 188.815233 \r\nL 114.962492 167.75645 \r\nL 115.267161 199.641555 \r\nL 115.571829 196.573717 \r\nL 115.876497 190.550503 \r\nL 116.181166 164.344358 \r\nL 116.485834 168.299017 \r\nL 116.790502 178.042036 \r\nL 117.09517 167.03859 \r\nL 117.399839 174.33087 \r\nL 117.704507 173.33586 \r\nL 118.313844 210.77588 \r\nL 118.618512 191.606239 \r\nL 118.92318 115.438349 \r\nL 119.532517 203.536028 \r\nL 119.837185 160.248317 \r\nL 120.141854 174.098962 \r\nL 120.446522 121.576618 \r\nL 120.75119 152.91754 \r\nL 121.055858 196.579424 \r\nL 121.360527 127.62627 \r\nL 121.665195 133.160762 \r\nL 121.969863 136.582949 \r\nL 122.274532 114.621104 \r\nL 122.5792 163.865871 \r\nL 122.883868 163.722373 \r\nL 123.188537 152.238368 \r\nL 123.493205 181.72561 \r\nL 124.102541 146.917154 \r\nL 124.40721 183.280587 \r\nL 124.711878 171.318933 \r\nL 125.016546 196.670483 \r\nL 125.625883 150.147245 \r\nL 125.930551 136.168982 \r\nL 126.23522 146.41624 \r\nL 126.539888 139.103123 \r\nL 126.844556 167.438173 \r\nL 127.149225 164.749518 \r\nL 127.453893 117.668272 \r\nL 127.758561 110.034117 \r\nL 128.063229 140.669759 \r\nL 128.367898 148.113214 \r\nL 128.672566 184.681479 \r\nL 128.977234 127.53444 \r\nL 129.586571 213.294556 \r\nL 129.891239 197.812864 \r\nL 130.195908 170.781441 \r\nL 130.500576 171.160468 \r\nL 130.805244 173.956121 \r\nL 131.109912 145.842386 \r\nL 131.414581 161.367202 \r\nL 131.719249 147.695191 \r\nL 132.328586 142.18344 \r\nL 132.633254 151.253008 \r\nL 132.937922 183.732522 \r\nL 133.242591 158.696309 \r\nL 133.547259 150.026313 \r\nL 133.851927 168.099758 \r\nL 134.156596 170.232019 \r\nL 134.461264 153.800683 \r\nL 134.765932 125.081506 \r\nL 135.0706 73.083925 \r\nL 135.375269 170.568095 \r\nL 135.679937 155.678158 \r\nL 135.984605 131.136545 \r\nL 136.289274 197.285297 \r\nL 136.593942 199.088072 \r\nL 136.89861 195.68441 \r\nL 137.203279 194.261107 \r\nL 137.507947 207.968604 \r\nL 137.812615 120.503659 \r\nL 138.117283 173.920969 \r\nL 138.421952 152.047641 \r\nL 138.72662 152.897975 \r\nL 139.031288 185.058411 \r\nL 139.335957 177.703415 \r\nL 139.945293 146.415322 \r\nL 140.249962 149.656607 \r\nL 140.55463 154.548796 \r\nL 140.859298 150.64904 \r\nL 141.163967 199.645351 \r\nL 141.468635 141.881524 \r\nL 141.773303 159.197053 \r\nL 142.077971 148.953566 \r\nL 142.38264 168.004699 \r\nL 142.687308 106.432012 \r\nL 142.991976 179.595608 \r\nL 143.296645 153.7044 \r\nL 143.601313 187.940143 \r\nL 143.905981 148.284059 \r\nL 144.21065 203.012753 \r\nL 144.515318 129.473603 \r\nL 144.819986 116.906475 \r\nL 145.124654 144.75209 \r\nL 145.429323 152.403885 \r\nL 145.733991 86.435013 \r\nL 146.038659 172.336996 \r\nL 146.343328 180.109883 \r\nL 146.647996 141.884472 \r\nL 146.952664 131.807318 \r\nL 147.257333 134.390479 \r\nL 147.562001 127.164098 \r\nL 147.866669 183.126576 \r\nL 148.171338 169.118765 \r\nL 148.476006 188.370265 \r\nL 148.780674 181.422046 \r\nL 149.085342 181.173452 \r\nL 149.390011 171.025708 \r\nL 149.694679 194.223102 \r\nL 149.999347 187.898082 \r\nL 150.304016 195.322778 \r\nL 150.608684 178.32233 \r\nL 150.913352 103.978155 \r\nL 151.218021 163.011374 \r\nL 151.522689 147.988056 \r\nL 151.827357 113.298337 \r\nL 152.132025 179.590757 \r\nL 152.436694 177.459291 \r\nL 152.741362 188.129121 \r\nL 153.04603 189.674162 \r\nL 153.350699 170.408267 \r\nL 153.655367 179.807999 \r\nL 153.960035 200.400672 \r\nL 154.264704 118.116055 \r\nL 154.569372 125.854543 \r\nL 154.87404 178.260249 \r\nL 155.178709 197.051827 \r\nL 155.788045 170.479769 \r\nL 156.092713 138.204249 \r\nL 156.70205 185.139892 \r\nL 157.006718 143.7677 \r\nL 157.311387 164.643076 \r\nL 157.616055 156.478639 \r\nL 158.225392 181.581927 \r\nL 158.53006 181.682659 \r\nL 158.834728 162.707768 \r\nL 159.139396 170.901053 \r\nL 159.444065 189.965351 \r\nL 159.748733 187.08713 \r\nL 160.053401 189.192621 \r\nL 160.35807 164.3436 \r\nL 160.662738 191.697483 \r\nL 160.967406 171.050107 \r\nL 161.272075 179.353403 \r\nL 161.576743 101.881301 \r\nL 161.881411 153.657334 \r\nL 162.18608 152.064803 \r\nL 162.490748 153.247041 \r\nL 162.795416 131.925658 \r\nL 163.100084 214.997718 \r\nL 163.404753 129.341139 \r\nL 163.709421 128.948889 \r\nL 164.014089 132.271908 \r\nL 164.318758 126.42055 \r\nL 164.623426 136.53011 \r\nL 164.928094 113.46168 \r\nL 165.232763 118.434269 \r\nL 165.537431 110.247595 \r\nL 165.842099 159.473588 \r\nL 166.146768 139.396216 \r\nL 166.451436 148.500343 \r\nL 166.756104 118.255656 \r\nL 167.060772 150.36953 \r\nL 167.365441 158.695047 \r\nL 167.670109 179.365715 \r\nL 167.974777 180.634343 \r\nL 168.279446 147.796665 \r\nL 168.584114 159.687803 \r\nL 168.888782 156.293303 \r\nL 169.193451 134.403808 \r\nL 169.498119 127.913457 \r\nL 169.802787 118.563086 \r\nL 170.107455 101.528466 \r\nL 170.716792 150.253438 \r\nL 171.02146 127.99411 \r\nL 171.326129 154.924738 \r\nL 171.630797 133.352863 \r\nL 171.935465 135.416474 \r\nL 172.240134 191.197714 \r\nL 172.84947 158.345838 \r\nL 173.154139 162.096286 \r\nL 173.458807 103.791112 \r\nL 173.763475 141.798379 \r\nL 174.068143 157.081345 \r\nL 174.372812 145.087598 \r\nL 174.67748 145.113355 \r\nL 174.982148 226.937021 \r\nL 175.286817 149.450933 \r\nL 175.591485 159.635234 \r\nL 175.896153 193.762383 \r\nL 176.200822 175.410462 \r\nL 176.50549 100.394003 \r\nL 176.810158 67.437032 \r\nL 177.114826 203.446165 \r\nL 177.419495 132.465854 \r\nL 177.724163 151.677621 \r\nL 178.028831 150.265347 \r\nL 178.3335 127.104801 \r\nL 178.638168 121.085582 \r\nL 179.247505 163.914358 \r\nL 179.552173 173.657244 \r\nL 179.856841 142.197108 \r\nL 180.16151 85.555766 \r\nL 180.466178 110.225507 \r\nL 180.770846 119.143439 \r\nL 181.075514 118.149893 \r\nL 181.380183 163.598185 \r\nL 181.684851 179.928126 \r\nL 181.989519 158.301842 \r\nL 182.294188 170.762502 \r\nL 182.598856 165.055707 \r\nL 182.903524 183.719866 \r\nL 183.208193 101.132833 \r\nL 183.512861 141.229213 \r\nL 183.817529 157.051633 \r\nL 184.122197 154.689065 \r\nL 184.426866 158.113469 \r\nL 184.731534 149.134055 \r\nL 185.036202 134.780428 \r\nL 185.340871 44.081415 \r\nL 185.645539 102.968883 \r\nL 185.950207 96.641906 \r\nL 186.559544 176.599142 \r\nL 186.864212 126.205979 \r\nL 187.168881 170.110318 \r\nL 187.473549 157.735014 \r\nL 187.778217 133.07416 \r\nL 188.082885 92.422261 \r\nL 188.387554 147.766503 \r\nL 188.692222 143.449645 \r\nL 188.99689 167.882489 \r\nL 189.301559 134.817585 \r\nL 189.606227 130.647491 \r\nL 189.910895 148.979556 \r\nL 190.215564 137.577174 \r\nL 190.520232 112.201201 \r\nL 190.8249 160.334251 \r\nL 191.129568 146.48754 \r\nL 191.434237 156.633011 \r\nL 191.738905 155.408871 \r\nL 192.043573 134.889734 \r\nL 192.348242 229.581212 \r\nL 192.65291 178.69036 \r\nL 192.957578 186.379651 \r\nL 193.262247 183.584215 \r\nL 193.566915 193.646309 \r\nL 193.871583 148.445637 \r\nL 194.176252 178.990632 \r\nL 194.48092 168.430118 \r\nL 194.785588 166.313281 \r\nL 195.090256 122.591383 \r\nL 195.394925 143.691659 \r\nL 195.699593 179.857329 \r\nL 196.004261 172.315449 \r\nL 196.30893 118.345843 \r\nL 196.613598 155.618691 \r\nL 196.918266 147.704565 \r\nL 197.527603 197.997314 \r\nL 197.832271 186.30466 \r\nL 198.136939 85.969659 \r\nL 198.441608 134.398241 \r\nL 198.746276 109.286369 \r\nL 199.050944 157.625481 \r\nL 199.355613 157.713552 \r\nL 199.660281 140.145448 \r\nL 199.964949 148.708243 \r\nL 200.269618 131.539558 \r\nL 200.574286 103.173115 \r\nL 200.878954 169.00956 \r\nL 201.183623 173.834797 \r\nL 201.488291 171.993094 \r\nL 201.792959 122.440659 \r\nL 202.097627 144.485035 \r\nL 202.402296 88.581759 \r\nL 202.706964 141.046846 \r\nL 203.011632 150.235979 \r\nL 203.316301 88.290871 \r\nL 203.620969 164.014682 \r\nL 203.925637 163.110972 \r\nL 204.230306 173.432526 \r\nL 204.534974 93.907766 \r\nL 204.839642 124.13415 \r\nL 205.144311 136.03322 \r\nL 205.448979 181.693432 \r\nL 205.753647 203.133357 \r\nL 206.362984 106.554923 \r\nL 206.667652 150.967211 \r\nL 206.97232 170.858452 \r\nL 207.276989 111.903453 \r\nL 207.581657 102.205209 \r\nL 207.886325 157.00645 \r\nL 208.190994 92.35255 \r\nL 208.495662 80.043532 \r\nL 208.80033 108.967191 \r\nL 209.104998 150.205779 \r\nL 209.409667 131.450389 \r\nL 209.714335 189.257992 \r\nL 210.019003 181.824661 \r\nL 210.323672 190.301361 \r\nL 210.62834 161.700447 \r\nL 210.933008 149.268237 \r\nL 211.237677 142.596484 \r\nL 211.542345 160.506793 \r\nL 211.847013 186.464812 \r\nL 212.151682 171.62984 \r\nL 212.761018 171.026323 \r\nL 213.065686 161.111605 \r\nL 213.370355 144.556256 \r\nL 213.675023 156.682934 \r\nL 213.979691 134.554671 \r\nL 214.28436 158.192192 \r\nL 214.589028 147.855957 \r\nL 214.893696 145.335815 \r\nL 215.198365 219.878174 \r\nL 215.503033 193.245313 \r\nL 215.807701 193.501441 \r\nL 216.112369 175.632466 \r\nL 216.417038 171.569965 \r\nL 216.721706 181.080403 \r\nL 217.026374 204.722429 \r\nL 217.635711 187.436222 \r\nL 217.940379 168.142969 \r\nL 218.245048 205.863113 \r\nL 218.549716 188.801031 \r\nL 218.854384 194.989759 \r\nL 219.159053 179.983813 \r\nL 219.463721 193.485196 \r\nL 219.768389 190.871665 \r\nL 220.073057 143.254289 \r\nL 220.377726 160.027156 \r\nL 220.682394 170.058183 \r\nL 220.987062 156.418541 \r\nL 221.291731 169.363541 \r\nL 221.596399 143.168461 \r\nL 222.205736 199.792926 \r\nL 222.510404 118.277416 \r\nL 222.815072 123.578939 \r\nL 223.11974 138.969026 \r\nL 223.424409 137.701855 \r\nL 223.729077 179.873855 \r\nL 224.033745 194.369787 \r\nL 224.338414 162.048558 \r\nL 224.643082 206.032704 \r\nL 224.94775 170.898938 \r\nL 225.252419 114.838707 \r\nL 225.557087 169.289652 \r\nL 225.861755 182.856509 \r\nL 226.166424 201.868809 \r\nL 226.471092 165.288342 \r\nL 226.77576 146.106859 \r\nL 227.080428 163.161998 \r\nL 227.689765 142.870214 \r\nL 227.994433 180.227433 \r\nL 228.299102 168.996284 \r\nL 228.60377 150.412015 \r\nL 228.908438 102.426396 \r\nL 229.213107 135.459865 \r\nL 229.517775 147.050286 \r\nL 229.822443 173.091524 \r\nL 230.127111 129.001909 \r\nL 230.43178 184.368116 \r\nL 230.736448 198.904359 \r\nL 231.041116 226.582285 \r\nL 231.345785 169.587528 \r\nL 231.650453 190.19108 \r\nL 231.955121 164.979115 \r\nL 232.564458 140.507615 \r\nL 232.869126 144.150926 \r\nL 233.173795 158.548209 \r\nL 233.478463 165.441743 \r\nL 233.783131 158.357916 \r\nL 234.087799 168.956874 \r\nL 234.392468 167.189633 \r\nL 234.697136 148.061997 \r\nL 235.001804 174.160698 \r\nL 235.306473 169.155359 \r\nL 235.611141 185.705421 \r\nL 236.220478 75.759849 \r\nL 236.525146 151.085943 \r\nL 236.829814 151.788157 \r\nL 237.134482 150.64536 \r\nL 237.439151 169.174207 \r\nL 237.743819 167.000246 \r\nL 238.048487 198.705787 \r\nL 238.353156 189.430547 \r\nL 238.657824 223.012694 \r\nL 238.962492 130.332777 \r\nL 239.267161 185.017268 \r\nL 239.571829 182.589879 \r\nL 239.876497 156.563131 \r\nL 240.181166 153.668908 \r\nL 240.485834 164.564952 \r\nL 240.790502 142.632347 \r\nL 241.09517 159.962329 \r\nL 241.399839 130.449017 \r\nL 241.704507 147.852002 \r\nL 242.009175 136.011408 \r\nL 242.313844 205.394241 \r\nL 242.618512 145.653976 \r\nL 242.92318 160.563408 \r\nL 243.227849 147.13191 \r\nL 243.532517 190.009217 \r\nL 243.837185 103.638331 \r\nL 244.141854 179.427594 \r\nL 244.446522 154.474966 \r\nL 244.75119 184.584368 \r\nL 245.055858 167.20276 \r\nL 245.360527 188.019418 \r\nL 245.665195 139.757429 \r\nL 245.969863 119.109422 \r\nL 246.274532 127.981386 \r\nL 246.5792 131.124064 \r\nL 246.883868 84.044854 \r\nL 247.188537 178.188561 \r\nL 247.493205 203.049655 \r\nL 247.797873 148.973851 \r\nL 248.102541 155.131334 \r\nL 248.40721 136.768465 \r\nL 248.711878 145.912288 \r\nL 249.016546 168.887175 \r\nL 249.625883 177.829512 \r\nL 249.930551 166.828346 \r\nL 250.23522 189.316597 \r\nL 250.539888 177.868375 \r\nL 250.844556 195.576565 \r\nL 251.149225 201.03697 \r\nL 251.453893 172.092496 \r\nL 251.758561 183.371203 \r\nL 252.063229 155.237172 \r\nL 252.367898 168.380195 \r\nL 252.977234 116.056485 \r\nL 253.281903 182.779116 \r\nL 253.586571 195.011065 \r\nL 253.891239 189.955723 \r\nL 254.195908 193.90684 \r\nL 254.500576 162.95828 \r\nL 254.805244 177.211774 \r\nL 255.109912 186.25013 \r\nL 255.414581 137.58552 \r\nL 255.719249 129.871569 \r\nL 256.023917 174.401519 \r\nL 256.328586 183.001857 \r\nL 256.633254 178.901798 \r\nL 256.937922 168.731132 \r\nL 257.242591 140.052144 \r\nL 257.851927 164.944371 \r\nL 258.156596 127.512776 \r\nL 258.765932 171.909153 \r\nL 259.0706 174.931419 \r\nL 259.375269 181.315508 \r\nL 259.679937 193.702568 \r\nL 259.984605 167.846727 \r\nL 260.289274 189.336861 \r\nL 260.593942 168.365429 \r\nL 260.89861 159.549332 \r\nL 261.203279 137.339773 \r\nL 261.507947 168.327796 \r\nL 261.812615 176.004839 \r\nL 262.117283 156.264216 \r\nL 262.421952 174.647737 \r\nL 262.72662 97.446279 \r\nL 263.031288 156.23092 \r\nL 263.335957 131.268203 \r\nL 263.640625 155.091898 \r\nL 263.945293 145.355014 \r\nL 264.249962 167.349402 \r\nL 264.55463 131.250113 \r\nL 264.859298 150.334437 \r\nL 265.163967 154.377278 \r\nL 265.468635 89.604381 \r\nL 265.773303 165.208059 \r\nL 266.077971 108.904479 \r\nL 266.38264 119.940155 \r\nL 266.687308 112.831361 \r\nL 266.991976 159.179207 \r\nL 267.296645 143.939547 \r\nL 267.601313 136.011853 \r\nL 267.905981 121.299071 \r\nL 268.21065 167.209589 \r\nL 268.515318 183.861598 \r\nL 268.819986 169.412621 \r\nL 269.124654 190.09218 \r\nL 269.429323 135.422666 \r\nL 269.733991 181.484042 \r\nL 270.038659 130.456355 \r\nL 270.343328 124.165824 \r\nL 270.647996 133.342206 \r\nL 270.952664 113.754826 \r\nL 271.257333 111.339334 \r\nL 271.562001 140.697998 \r\nL 271.866669 129.924451 \r\nL 272.171338 131.744776 \r\nL 272.476006 152.967368 \r\nL 272.780674 127.010457 \r\nL 273.085342 128.901521 \r\nL 273.390011 193.612745 \r\nL 273.694679 158.037423 \r\nL 273.999347 147.444127 \r\nL 274.304016 141.684493 \r\nL 274.608684 116.813478 \r\nL 274.913352 144.688312 \r\nL 275.522689 160.20211 \r\nL 275.827357 152.31364 \r\nL 276.132025 229.874489 \r\nL 276.436694 139.259934 \r\nL 277.04603 191.733105 \r\nL 277.350699 168.836387 \r\nL 277.655367 98.966639 \r\nL 277.960035 68.569309 \r\nL 278.264704 186.760661 \r\nL 278.569372 135.222347 \r\nL 278.87404 156.971372 \r\nL 279.178709 150.702043 \r\nL 279.483377 141.593309 \r\nL 279.788045 117.963507 \r\nL 280.092713 171.10757 \r\nL 280.397382 153.901628 \r\nL 280.70205 159.709096 \r\nL 281.006718 136.402343 \r\nL 281.311387 104.36469 \r\nL 281.616055 128.851068 \r\nL 282.225392 100.966266 \r\nL 282.53006 173.358399 \r\nL 282.834728 186.456498 \r\nL 283.139396 153.319318 \r\nL 283.444065 139.121139 \r\nL 283.748733 173.312176 \r\nL 284.053401 189.937089 \r\nL 284.35807 104.67863 \r\nL 284.662738 146.989515 \r\nL 284.967406 144.0934 \r\nL 285.272075 151.375335 \r\nL 285.576743 164.604144 \r\nL 286.18608 144.959344 \r\nL 286.490748 60.338979 \r\nL 286.795416 104.371243 \r\nL 287.100084 118.244204 \r\nL 287.404753 146.663168 \r\nL 287.709421 189.381143 \r\nL 288.014089 139.184848 \r\nL 288.318758 178.433077 \r\nL 288.623426 140.166009 \r\nL 288.928094 125.378925 \r\nL 289.232763 79.377722 \r\nL 289.537431 115.258231 \r\nL 290.146768 165.128074 \r\nL 290.451436 145.463995 \r\nL 290.756104 157.267969 \r\nL 291.060772 141.355527 \r\nL 291.365441 149.306327 \r\nL 291.670109 148.302908 \r\nL 291.974777 170.687203 \r\nL 292.279446 154.051334 \r\nL 292.584114 173.734695 \r\nL 292.888782 169.002795 \r\nL 293.193451 142.251315 \r\nL 293.498119 227.634659 \r\nL 293.802787 156.946617 \r\nL 294.107455 179.438267 \r\nL 294.412124 192.388749 \r\nL 295.02146 146.672012 \r\nL 295.326129 172.321763 \r\nL 295.630797 178.910943 \r\nL 295.935465 168.118002 \r\nL 296.240134 134.793683 \r\nL 296.544802 177.327055 \r\nL 296.84947 202.249206 \r\nL 297.154139 189.68813 \r\nL 297.458807 104.820574 \r\nL 297.763475 156.162106 \r\nL 298.068143 150.070065 \r\nL 298.372812 159.502786 \r\nL 298.67748 212.926009 \r\nL 298.982148 199.668203 \r\nL 299.286817 101.472392 \r\nL 299.591485 134.230243 \r\nL 299.896153 108.854089 \r\nL 300.200822 185.108939 \r\nL 300.810158 153.250758 \r\nL 301.114826 148.677057 \r\nL 301.419495 132.305692 \r\nL 301.724163 85.822837 \r\nL 302.028831 167.789768 \r\nL 302.3335 169.885975 \r\nL 302.638168 165.682824 \r\nL 302.942836 133.741349 \r\nL 303.247505 148.468515 \r\nL 303.552173 74.814095 \r\nL 303.856841 138.014481 \r\nL 304.16151 160.833569 \r\nL 304.466178 82.978649 \r\nL 304.770846 182.657023 \r\nL 305.075514 171.326266 \r\nL 305.380183 174.743702 \r\nL 305.684851 107.234889 \r\nL 305.989519 110.263724 \r\nL 306.903524 197.358077 \r\nL 307.208193 177.387301 \r\nL 307.512861 123.465583 \r\nL 307.817529 172.607714 \r\nL 308.122197 190.614609 \r\nL 308.426866 125.610685 \r\nL 308.731534 113.300394 \r\nL 309.036202 127.291387 \r\nL 309.645539 61.078126 \r\nL 309.950207 68.483969 \r\nL 310.254876 134.097196 \r\nL 310.559544 138.314148 \r\nL 310.864212 197.977614 \r\nL 311.168881 195.764051 \r\nL 311.473549 197.739378 \r\nL 311.778217 112.234412 \r\nL 312.082885 125.85642 \r\nL 312.692222 170.150464 \r\nL 312.99689 185.404068 \r\nL 313.301559 143.796182 \r\nL 313.606227 152.602905 \r\nL 313.910895 174.314015 \r\nL 314.215564 169.792183 \r\nL 314.520232 157.253362 \r\nL 314.8249 154.023021 \r\nL 315.129568 124.516655 \r\nL 315.434237 154.885901 \r\nL 315.738905 122.462662 \r\nL 316.043573 134.471674 \r\nL 316.348242 202.703132 \r\nL 316.65291 199.141913 \r\nL 317.566915 152.599215 \r\nL 318.176252 197.061344 \r\nL 318.48092 177.908076 \r\nL 318.785588 166.594004 \r\nL 319.090256 186.477398 \r\nL 319.394925 179.008192 \r\nL 319.699593 190.649504 \r\nL 320.004261 182.282831 \r\nL 320.30893 186.716008 \r\nL 320.613598 203.813075 \r\nL 320.918266 200.762296 \r\nL 321.222935 151.106801 \r\nL 321.527603 164.703678 \r\nL 321.832271 192.04639 \r\nL 322.136939 174.815412 \r\nL 322.441608 178.359252 \r\nL 322.746276 116.648693 \r\nL 323.355613 195.311278 \r\nL 323.660281 107.091598 \r\nL 324.269618 166.834369 \r\nL 324.878954 151.466444 \r\nL 325.183623 192.419696 \r\nL 325.488291 166.877596 \r\nL 325.792959 194.430394 \r\nL 326.097627 163.186286 \r\nL 326.402296 112.928747 \r\nL 326.706964 183.762679 \r\nL 327.011632 169.993939 \r\nL 327.316301 193.513479 \r\nL 327.620969 154.985848 \r\nL 327.925637 131.058383 \r\nL 328.230306 159.425902 \r\nL 328.534974 155.609386 \r\nL 328.839642 148.316921 \r\nL 329.144311 185.933735 \r\nL 329.448979 185.424449 \r\nL 330.058315 102.9223 \r\nL 330.362984 157.866572 \r\nL 330.667652 162.485138 \r\nL 330.97232 184.358345 \r\nL 331.276989 119.445863 \r\nL 331.581657 183.181759 \r\nL 331.886325 201.804697 \r\nL 332.190994 227.964799 \r\nL 332.495662 180.345131 \r\nL 332.80033 182.372482 \r\nL 333.409667 143.51381 \r\nL 333.714335 156.657697 \r\nL 334.019003 158.012413 \r\nL 334.323672 172.914385 \r\nL 334.62834 171.287837 \r\nL 334.933008 159.178406 \r\nL 335.237677 198.675226 \r\nL 335.542345 172.093911 \r\nL 335.847013 163.938222 \r\nL 336.151682 179.693885 \r\nL 336.45635 186.15796 \r\nL 336.761018 147.192262 \r\nL 337.065686 125.959151 \r\nL 337.370355 61.099981 \r\nL 337.675023 156.2835 \r\nL 337.979691 162.407056 \r\nL 338.28436 139.693254 \r\nL 338.589028 183.156081 \r\nL 338.893696 180.364743 \r\nL 339.198365 205.503804 \r\nL 339.503033 188.408545 \r\nL 339.807701 222.029904 \r\nL 340.112369 141.237558 \r\nL 340.721706 209.978692 \r\nL 341.026374 173.968162 \r\nL 341.331043 199.701441 \r\nL 341.635711 185.532217 \r\nL 341.940379 125.122681 \r\nL 342.245048 142.580833 \r\nL 342.549716 141.480643 \r\nL 342.854384 158.477953 \r\nL 343.159053 145.511305 \r\nL 343.463721 177.908198 \r\nL 343.768389 143.212594 \r\nL 344.073057 162.155053 \r\nL 344.377726 120.510361 \r\nL 344.682394 192.347669 \r\nL 344.987062 127.412241 \r\nL 345.291731 182.905234 \r\nL 345.596399 152.933069 \r\nL 345.901067 187.687536 \r\nL 346.205736 166.170335 \r\nL 346.510404 201.846506 \r\nL 346.815072 132.233002 \r\nL 347.11974 103.94962 \r\nL 347.424409 142.130685 \r\nL 347.729077 148.868607 \r\nL 348.033745 108.554369 \r\nL 348.338414 192.049831 \r\nL 348.643082 189.734384 \r\nL 348.94775 148.087166 \r\nL 349.252419 173.631551 \r\nL 349.557087 142.11303 \r\nL 349.861755 142.958768 \r\nL 350.166424 186.884876 \r\nL 350.471092 173.89165 \r\nL 350.77576 199.646099 \r\nL 351.080428 147.732231 \r\nL 351.385097 184.776283 \r\nL 351.689765 165.839958 \r\nL 351.994433 177.812556 \r\nL 352.299102 178.418725 \r\nL 352.60377 193.134216 \r\nL 352.908438 157.411265 \r\nL 353.213107 153.885218 \r\nL 353.517775 185.269663 \r\nL 353.822443 138.419403 \r\nL 353.822443 138.419403 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 34.240625 239.758125 \r\nL 34.240625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 369.040625 239.758125 \r\nL 369.040625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 34.240625 239.758125 \r\nL 369.040625 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 34.240625 22.318125 \r\nL 369.040625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_14\">\r\n    <!-- model loss -->\r\n    <g transform=\"translate(169.460938 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-109\"/>\r\n     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"158.59375\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"222.070312\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"283.59375\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"311.376953\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"343.164062\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"370.947266\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"432.128906\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"484.228516\" xlink:href=\"#DejaVuSans-115\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 41.240625 59.674375 \r\nL 96.515625 59.674375 \r\nQ 98.515625 59.674375 98.515625 57.674375 \r\nL 98.515625 29.318125 \r\nQ 98.515625 27.318125 96.515625 27.318125 \r\nL 41.240625 27.318125 \r\nQ 39.240625 27.318125 39.240625 29.318125 \r\nL 39.240625 57.674375 \r\nQ 39.240625 59.674375 41.240625 59.674375 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_14\">\r\n     <path d=\"M 43.240625 35.416562 \r\nL 63.240625 35.416562 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\"/>\r\n    <g id=\"text_15\">\r\n     <!-- train -->\r\n     <g transform=\"translate(71.240625 38.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"141.601562\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"169.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_16\">\r\n     <path d=\"M 43.240625 50.094687 \r\nL 63.240625 50.094687 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_17\"/>\r\n    <g id=\"text_16\">\r\n     <!-- test -->\r\n     <g transform=\"translate(71.240625 53.594687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"100.732422\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"152.832031\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p4a85c7a2d8\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"34.240625\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'],'r')\n",
    "plt.plot(history.history['val_accuracy'],'b')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# Loss \n",
    "plt.plot(history.history['loss'],'r')\n",
    "plt.plot(history.history['val_loss'],'b')\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}